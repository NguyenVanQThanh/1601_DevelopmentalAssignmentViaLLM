{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d461df2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d461df2",
    "outputId": "6dd982cb-7742-4e72-8a09-0ed9f7c4cb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/jupyter_venv/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: trl in /opt/jupyter_venv/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: peft in /opt/jupyter_venv/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: datasets in /opt/jupyter_venv/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /opt/jupyter_venv/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: rouge_score in /opt/jupyter_venv/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: underthesea in /opt/jupyter_venv/lib/python3.10/site-packages (6.8.4)\n",
      "Requirement already satisfied: bitsandbytes in /opt/jupyter_venv/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: thefuzz in /opt/jupyter_venv/lib/python3.10/site-packages (0.22.1)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: requests in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: rich in /opt/jupyter_venv/lib/python3.10/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: psutil in /opt/jupyter_venv/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from peft) (2.7.0)\n",
      "Requirement already satisfied: xxhash in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: absl-py in /opt/jupyter_venv/lib/python3.10/site-packages (from rouge_score) (2.2.2)\n",
      "Requirement already satisfied: nltk in /opt/jupyter_venv/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: joblib in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (1.5.0)\n",
      "Requirement already satisfied: Click>=6.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (8.2.0)\n",
      "Requirement already satisfied: underthesea-core==1.0.4 in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (1.0.4)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (0.9.11)\n",
      "Requirement already satisfied: scikit-learn in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (1.6.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from thefuzz) (3.13.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
      "Collecting Pillow\n",
      "  Using cached pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Requirement already satisfied: scipy in /opt/jupyter_venv/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/jupyter_venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: networkx in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: jinja2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from triton==3.3.0->torch>=1.13.0->peft) (59.6.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/jupyter_venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.6.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Installing collected packages: Pillow, sentence-transformers\n",
      "Successfully installed Pillow-11.2.1 sentence-transformers-4.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U transformers trl peft datasets evaluate rouge_score underthesea bitsandbytes thefuzz sentence-transformers\n",
    "# Tải tài nguyên NLTK cho METEOR\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1LRADR_xiIjl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LRADR_xiIjl",
    "outputId": "6b563c1a-fda4-4f07-e200-3cef8da70ce4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ddc4b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14ddc4b6",
    "outputId": "d490c573-2ce6-404d-c9c2-338e596f6b1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from underthesea import word_tokenize\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0bf13f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc0bf13f",
    "outputId": "d33edc49-fd95-4125-9f46-769b9af0efcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "def load_model_and_tokenizer(quantization=\"int8\"):  # Thêm tham số quantization\n",
    "    model_name = \"vilm/vinallama-7b-chat\"\n",
    "\n",
    "    # Cấu hình quantization với bitsandbytes\n",
    "    if quantization == \"int8\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,  # Quantize thành INT8\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
    "            bnb_8bit_use_double_quant=True,  # Double quantization để tăng hiệu quả\n",
    "        )\n",
    "    elif quantization == \"int4\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,  # Quantize thành INT4\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
    "            bnb_4bit_use_double_quant=True,  # Double quantization\n",
    "            bnb_4bit_quant_type=\"nf4\",  # Dùng NF4 (Normalized Float 4-bit) để tối ưu\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None  # Không quantize\n",
    "\n",
    "    # Tải tokenizer và mô hình\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,  # Áp dụng quantization\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Cấu hình LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.5,  # Tăng dropout để giảm overfitting\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model, tokenizer, peft_config\n",
    "\n",
    "# Tải mô hình với INT8 quantization\n",
    "model, tokenizer, peft_config = load_model_and_tokenizer(quantization=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fadab1",
   "metadata": {
    "id": "e0fadab1"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    if not all(k in example for k in ['input', 'output']):\n",
    "        print('Thiếu key trong example:', example)\n",
    "        return ''  # hoặc raise lỗi nếu bạn muốn dừng\n",
    "    return f\"<s>[INST] {example['input']} [/INST] {example['output']} </s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6ee4ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc6ee4ec",
    "outputId": "2b63813f-cece-4a50-86b9-5f3e801bb8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giá trị duy nhất trong 'input' (exact): 1599\n",
      "Số lượng giá trị duy nhất trong 'output' (exact): 1659\n",
      "Tổng số hàng: 1700\n",
      "Số cặp input tương tự (>90%): 320\n",
      "Số cặp output tương tự (>90%): 41\n",
      "Số hàng sau khi xóa record tương tự: 1471\n",
      "Số lượng giá trị duy nhất trong 'input' (sau xử lý): 1471\n",
      "Số lượng giá trị duy nhất trong 'output' (sau xử lý): 1471\n",
      "                                                  input  \\\n",
      "998   trẻ không biết tôn trọng lượt nói trong thảo l...   \n",
      "254                             trẻ nói ít phải làm sao   \n",
      "1073  trẻ không biết chuẩn bị nội dung khi báo cáo nhóm   \n",
      "643   trẻ không biết nói lời chia tay khi bạn chuyển...   \n",
      "1450  tự kỷ có học được kỹ năng kết thúc tương tác x...   \n",
      "\n",
      "                                                 output  \n",
      "998   nói chen hoặc ngắt lời bạn là thiếu kỹ năng xã...  \n",
      "254   cần đánh giá xem trẻ có hiểu lời tương tác bằn...  \n",
      "1073  nội dung rời rạc là thiếu chuẩn bị – nên luyện...  \n",
      "643   không thể hiện chia tay là thiếu kỹ năng chia ...  \n",
      "1450  có trẻ có thể học nói lời chào cảm ơn xin phép...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from underthesea import word_tokenize\n",
    "from thefuzz import fuzz\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "def extract_json_from_folder(folder_path):\n",
    "    dataset = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    for item in json_data:\n",
    "                        if all(k in item for k in ['input', 'output']):\n",
    "                            item['input'] = preprocess_text(item['input'])\n",
    "                            item['output'] = preprocess_text(item['output'])\n",
    "                            dataset.append(item)\n",
    "                        else:\n",
    "                            print(f\"Thiếu trường trong {filename}: {item}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Không thể parse JSON từ {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi đọc {filename}: {e}\")\n",
    "    return dataset\n",
    "\n",
    "folder_path = \"/workspace/data_finetune\"\n",
    "dataset = extract_json_from_folder(folder_path)\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "print(\"Số lượng giá trị duy nhất trong 'input' (exact):\", df['input'].nunique())\n",
    "print(\"Số lượng giá trị duy nhất trong 'output' (exact):\", df['output'].nunique())\n",
    "print(\"Tổng số hàng:\", len(df))\n",
    "\n",
    "# Fuzzy matching để tìm các record tương tự\n",
    "similarity_threshold = 90  # Ngưỡng độ tương đồng (90%)\n",
    "input_pairs = []\n",
    "output_pairs = []\n",
    "\n",
    "# Tìm các cặp input và output tương tự\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        input_sim = fuzz.ratio(df['input'].iloc[i], df['input'].iloc[j])\n",
    "        if input_sim >= similarity_threshold:\n",
    "            input_pairs.append((i, j, input_sim))\n",
    "        output_sim = fuzz.ratio(df['output'].iloc[i], df['output'].iloc[j])\n",
    "        if output_sim >= similarity_threshold:\n",
    "            output_pairs.append((i, j, output_sim))\n",
    "\n",
    "print(f\"Số cặp input tương tự (>{similarity_threshold}%):\", len(input_pairs))\n",
    "print(f\"Số cặp output tương tự (>{similarity_threshold}%):\", len(output_pairs))\n",
    "\n",
    "# Loại bỏ các record có input hoặc output tương tự (giữ record đầu tiên)\n",
    "indices_to_keep = set(range(len(df)))\n",
    "for i, j, _ in input_pairs:\n",
    "    if j in indices_to_keep:\n",
    "        indices_to_keep.remove(j)\n",
    "for i, j, _ in output_pairs:\n",
    "    if j in indices_to_keep:\n",
    "        indices_to_keep.remove(j)\n",
    "\n",
    "df = df.iloc[list(indices_to_keep)].reset_index(drop=True)\n",
    "print(\"Số hàng sau khi xóa record tương tự:\", len(df))\n",
    "\n",
    "# Kiểm tra lại độ unique\n",
    "print(\"Số lượng giá trị duy nhất trong 'input' (sau xử lý):\", df['input'].nunique())\n",
    "print(\"Số lượng giá trị duy nhất trong 'output' (sau xử lý):\", df['output'].nunique())\n",
    "\n",
    "# Chia train/validation\n",
    "full_dataset = Dataset.from_pandas(df[['input', 'output']])\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df[['input', 'output']])\n",
    "eval_dataset = Dataset.from_pandas(eval_df[['input', 'output']])\n",
    "print(train_df[['input', 'output']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3d58ec",
   "metadata": {
    "id": "3d3d58ec"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "def evaluate_metrics(predictions, references):\n",
    "    # Tải các độ đo\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "    # Tính các độ đo\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "    meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "    # Tải mô hình nhúng câu để tính Cosine Similarity\n",
    "    embedder = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "    # Tính embeddings cho dự đoán và tham chiếu\n",
    "    pred_embeddings = embedder.encode(predictions, convert_to_tensor=True)\n",
    "    ref_embeddings = embedder.encode(references, convert_to_tensor=True)\n",
    "\n",
    "    # Tính Cosine Similarity giữa từng cặp dự đoán-tham chiếu\n",
    "    cosine_scores = util.cos_sim(pred_embeddings, ref_embeddings)\n",
    "    \n",
    "    # Xử lý trường hợp predictions rỗng để tránh lỗi\n",
    "    if len(predictions) > 0 and cosine_scores.ndim == 2 and cosine_scores.shape[0] == len(predictions):\n",
    "        avg_cosine_similarity = np.mean([cosine_scores[i][i].item() for i in range(len(predictions))])\n",
    "    elif len(predictions) == 0:\n",
    "        avg_cosine_similarity = 0.0 # Hoặc np.nan\n",
    "        print(\"Cảnh báo: Danh sách predictions rỗng, cosine_similarity được đặt là 0.0\")\n",
    "    else:\n",
    "        # Trường hợp này ít xảy ra nếu predictions và references cùng độ dài và không rỗng\n",
    "        print(f\"Cảnh báo: Kích thước cosine_scores ({cosine_scores.shape}) không khớp với len(predictions) ({len(predictions)}).\")\n",
    "        avg_cosine_similarity = np.nan # Hoặc một giá trị mặc định\n",
    "    # Gộp kết quả\n",
    "    metrics = {\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rouge2\": rouge_results[\"rouge2\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"],\n",
    "        \"meteor\": meteor_results[\"meteor\"],\n",
    "        \"cosine_similarity\": avg_cosine_similarity,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def generate_predictions(model, tokenizer, inputs, max_length=200):\n",
    "    \"\"\"Tạo dự đoán từ mô hình cho các đầu vào.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Đảm bảo pad_token được thiết lập\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for input_text in inputs:\n",
    "        # Tiền xử lý input để đồng bộ với huấn luyện\n",
    "        input_text = preprocess_text(input_text)\n",
    "        prompt = f\"<s>[INST] {input_text} [/INST]\"\n",
    "        inputs_encoded = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs_encoded,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = generated_text.split(\"[/INST]\")[-1].strip()\n",
    "        # Tiền xử lý dự đoán để đồng bộ với tham chiếu\n",
    "        pred = preprocess_text(pred)\n",
    "        predictions.append(pred)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b126a89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2ae2a398a2a945d18ec5409bda99644a",
      "2d03ea85ed9e4262bfa21065ad73eae4",
      "5f8edfd1f93f4001b51bde9e815d66af",
      "f1a26a9520da4b888f8c54322ae29b82",
      "ca3c86d9cf48410199515ce86fd68eaf",
      "3fd2597c60874a4f9c920f8420b366f0",
      "cd056a6bb14048bdbc7860ce2c06303f",
      "d98a8f43fc76406897ffbb66f7e136c0",
      "b8f7022d52464b0eb74f965319229cef",
      "8983e348f59749bd86146b513c5acacf",
      "551f5413b209400fa23bedb10c4cde9f",
      "046403be22d543dab3c5d9c3d3836b9e",
      "788d9b117db34ff6b06e1fd6a942c39c",
      "916b489caeac4041839f1ee2365360a3",
      "c8d0620a47874f34b94681c71f35c3c8",
      "57e0a819a11b42e0a35b75e072b2dbb1",
      "ba961b0dab1a4ac6864646de1d46d9e6",
      "442e0cfbeb554fd58e2bf0c2d77829c0",
      "a0da77700eab4ab0bb06e1fe7b5184e9",
      "69fe2be7c9814f2ab93168f3d6ac45ae",
      "841bc431816441a4bc79dde9600f9176",
      "5b9cb618b32044518965997a96a7d930",
      "9c6f2000b243485db70d65e993621d3c",
      "c778736f27e349edb304160691210f4b",
      "076e2be9ac414d6d92b90f203da87c38",
      "0ac46097afa5445591d9a3fbad639ae5",
      "4375f815d6724658a2cfaa6c608e478f",
      "79f8dacb371b4fd987b8a4330520cc04",
      "d990e34729bf43eca69270c1fd3dc8cb",
      "2d402147806d413590c8bbbcd89646de",
      "edf820a16c044fe0a2d2f85b11b64683",
      "6f4b1cc17a48461f8e2da6dc9b03c82a",
      "4d0cb99e715847d0b05a031736ecd688",
      "b3fbfe3f3f604b698e7d869437c0d5fe",
      "246d63e4a14841c594db092689f2a64c",
      "c6909db59ff04666b867097216de70ba",
      "1ffc5523dd084452a20ae6c913d39e47",
      "35fcfd38ee344fd0924e883972074784",
      "bb370caba2b844c486d30748e6caf9e0",
      "73e04b4d775848a19e267669303d8266",
      "4526ca6058b74e11ba4329d843d6a9d4",
      "ba9f2bb92edf477d94d6cfa1e4950711",
      "9e320bdd7a7c484183ed8b93e6705af6",
      "cc5fad4a005e4aa9bcaecccb8217df1e",
      "220a0b50b8d04e3dac9bbaf89de8a7c9",
      "1201dfd96b7d4fcabf5b10b3fa769759",
      "98ae3ea018e24dabb90618b485a2b2cb",
      "85f4e94e397f43a588a527fe41bc015a",
      "686120f3641f43298071d6a5e32fe644",
      "efc1ff78f5f546638b298419a4151695",
      "0926abb88e2e4381b4b38839e6f652f5",
      "58850dfce0cc4811ba06ab1ea71d306d",
      "634e691c8c8a4fb68c5af7704b9ce0fe",
      "5c5ac7432fae43f2a17870b83e19a17a",
      "d224187c0f7340279949cddca91d3ecb",
      "079359b31bb04296a232a84fc1b7db2d",
      "ff5faacf70a04f19833d41589b7a78e5",
      "e0d2e5d548e04e2ebcf6b5a79083f0dc",
      "7ac0a7531ecc4823b33549eeff34a131",
      "970af49c4f1940758130a5e334901a14",
      "83a4b7f740ec41389a6df6317af60615",
      "d43bfd1e65ae43af97b7fa6b44cf9023",
      "e2283973a0de470a88d5bb8b78160982",
      "65597da15fda4b51b7aa859b380c12db",
      "6d044e9840e449c5b1e718fb06e99515",
      "d00e51ae50d144e392bf12aa8b54b303",
      "4adcb8502fce4dd1af5b8a5b648fa263",
      "9b47975658384af1a549c9f1acf1e5a5",
      "56eff52cba7c4e06a6378805f2aa2707",
      "4a251249b63e4688ba357a8d412a3b69",
      "55fdfa966042462bb35ee44cfbfb841e",
      "23b95f393dd940039de764bd67d7709a",
      "c68779b0088e4f1c867ef1be26e7112b",
      "9ea9a076bce947bab5096a028f7b3237",
      "c034029a3d5e48ebaf7ea25fa7cb96fd",
      "c1267d7977354e5aa3db8a28f511f2b8",
      "9e526d55e8f84c66ab040536bc502d2b",
      "90fadb3fb3e14b1baaca9e77f571a80e",
      "e2e6f823b0224c52a44e1e798e5c1985",
      "4eaab0584d164139b520cd15eaa3793f",
      "943be389e1a947ad99ec7d4d76de6db0",
      "2c01848e500843e39f326eeb3b0a8cce",
      "8cd9559471a9418d8a134b43c6dc1258",
      "dc6578ccbf9740bfb315a07b6482d8bf",
      "187ce14699ae454c819ceacc050a955f",
      "a08c13a1cfcb4fd295994b99f788d6c3",
      "a81481e6777b4ab78442363553b73381",
      "ad666ee0fe7e48de9c4e934212bd337c",
      "cbbcc5aca91d46ba9b51859494da828b",
      "494b39f1fcf9434b88559980e35c006f",
      "f849b4139b5d4ab3a2ae32ae83807160",
      "1807262027f34e70b1e94883ac402b39",
      "c3d35975670546ab9998a6389e3ec707",
      "6a974c0a2ec043219c538076bfffc87a",
      "79f22a64d0fa4310b9ee63bbfb66b41a",
      "8fcd62139ad44297b2a950d97850468f",
      "a7ee09b72b034234b02194ab94471a2a",
      "a66367d4522a4b1eb6ce7b88394f8e30",
      "a0041fc725964c50a3beff504ddeb9ab",
      "e2627486d9ba4848830b0687cbe8b3d3",
      "55b1ad5ece2c4be18a2acdb322b93c01",
      "e502fdc27ded43fcbbc4d3fe3fac8030",
      "3d7b45cd5bfa4ad9ba035ac29882fb8b",
      "c8ad9a852eec438e972cadc8ea1f0fe2",
      "5fc18b5fb2e349afa2489a0adf1b21fc",
      "2bd1218e781f4784be96419b838c45af",
      "9419d62d8de24bce9697c0f60467ea99",
      "43b4ce75564943ca8b250e54a7ce1834",
      "80acce4d1d5c4459ab794ba6bbcc32cf",
      "fa04a7d89df34dfcbd73d62f4dda799c"
     ]
    },
    "id": "7b126a89",
    "outputId": "b0106b8a-df18-45c5-a8b4-bae598edea1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bắt đầu chạy đơn ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 22664.31 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1176/1176 [00:00<00:00, 35312.11 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 24443.98 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 4540.38 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 370307.92 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 22409.53 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 31731.84 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 21495.43 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 4479.62 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 120879.22 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,176\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 79,953,920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1176, Eval dataset size: 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter_venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/219 08:33 < 02:29, 0.33 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.258800</td>\n",
       "      <td>4.499073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.214600</td>\n",
       "      <td>3.680468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.109500</td>\n",
       "      <td>2.552556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.319500</td>\n",
       "      <td>2.157615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.080900</td>\n",
       "      <td>1.967185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.942400</td>\n",
       "      <td>1.876430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.792700</td>\n",
       "      <td>1.782481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.702200</td>\n",
       "      <td>1.772347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.659500</td>\n",
       "      <td>1.712764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.637100</td>\n",
       "      <td>1.680369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.584300</td>\n",
       "      <td>1.648914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.608600</td>\n",
       "      <td>1.625785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.527600</td>\n",
       "      <td>1.615190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.534200</td>\n",
       "      <td>1.599241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.491800</td>\n",
       "      <td>1.591106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.449400</td>\n",
       "      <td>1.583706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.404900</td>\n",
       "      <td>1.578610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_single/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_single/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_single/checkpoint-100/special_tokens_map.json\n",
      "/opt/jupyter_venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_single/checkpoint-100 (score: 1.5786104202270508).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_vinallama_single/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_vinallama_single/special_tokens_map.json\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "Safetensors PR exists\n",
      "loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Run Metrics: {'rouge1': np.float64(0.6075632250011477), 'rouge2': np.float64(0.26518622641830814), 'rougeL': np.float64(0.3890710278718287), 'meteor': np.float64(0.29583546708812114), 'cosine_similarity': np.float64(0.7726299805156255)}\n",
      "Input: trẻ không biết tôn trọng ranh giới của người khác\n",
      "Prediction: không hiểu khi người khác cần riêng tư – nên luyện phản xạ cá nhân và tình huống qua trò chơi tương tác nhóm\n",
      "Reference: cố ép bạn làm theo ý mình là chưa có kỹ năng đồng thuận – nên luyện hỏi bạn có muốn chơi thế này không trước khi bắt đầu\n",
      "\n",
      "Input: trẻ không dùng tay để chỉ vật\n",
      "Prediction: không dùng tay để chỉ đồ là dấu hiệu chậm phát triển ngôn ngữ hoặc không hiểu về khái niệm vật thể – nên được đánh giá sớm để can thiệp kịp thời\n",
      "Reference: việc trẻ không dùng tay để chỉ khi muốn thể hiện nhu cầu là dấu hiệu quan trọng để nhận biết trẻ có khó khăn trong giao tiếp phi ngôn ngữ\n",
      "\n",
      "Input: tự kỷ và rối loạn ngôn ngữ khác gì nhau\n",
      "Prediction: tự kỷ là một dạng phổ tự kỷ là rối loạn phát triển thần kinh ảnh hưởng đến kỹ năng xã hội giao tiếp và ngôn ngữ rối loạn ngôn ngữ có thể do nhiều nguyên nhân khác nhau như rối loạn phổ tự kỷ ngôn ngữ không rõ nguồn gốc hoặc rối loạn phát triển ngôn ngữ như khó nói chậm nói hoặc nói không rõ ràng nhưng không nhất thiết là tự kỷ\n",
      "Reference: rối loạn ngôn ngữ chủ yếu ảnh hưởng khả năng hiểu và diễn đạt ngôn ngữ trong khi tự kỷ còn liên quan đến hành vi và giao tiếp xã hội một số trẻ tự kỷ có rối loạn ngôn ngữ đi kèm\n",
      "\n",
      "Input: những dấu hiệu của trẻ chậm nói\n",
      "Prediction: dấu hiệu chậm nói ở trẻ bao gồm không nói được từ đơn không bắt chước âm thanh không phản ứng với lời gọi tên không phản ứng với lời khen ngợi không giao tiếp bằng mắt không hiểu ngôn ngữ cơ thể và không chơi tương tác với người khác\n",
      "Reference: những trẻ không có tiến bộ về lời nói ít tương tác bằng mắt và không phát triển vốn từ theo tháng tuổi là dấu hiệu cần được kiểm tra sớm\n",
      "\n",
      "Input: trẻ 22 tháng chưa nói được từ nào\n",
      "Prediction: trẻ 22 tháng chưa nói được từ nào có thể là dấu hiệu chậm phát triển ngôn ngữ – nên đánh giá toàn diện và can thiệp sớm để hỗ trợ phát triển ngôn ngữ\n",
      "Reference: trẻ 22 tháng chưa nói được từ nào là biểu hiện cần lưu ý nên đưa trẻ đi đánh giá ngôn ngữ thính giác hành vi xã hội … càng sớm càng tốt để xác định nguyên nhân và bắt đầu can thiệp\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model, trainer_single\n\u001b[1;32m     81\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Loại bỏ cột không cần thiết để tránh cảnh báo\n",
    "train_dataset = train_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_dataset.column_names else [])\n",
    "eval_dataset = eval_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_dataset.column_names else [])\n",
    "\n",
    "# Đo thời gian chạy đơn\n",
    "print(\"\\n=== Bắt đầu chạy đơn ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Cấu hình huấn luyện cho Single Run\n",
    "training_arguments_single = TrainingArguments(\n",
    "    output_dir=\"./results_single\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.1,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Huấn luyện Single Run\n",
    "trainer_single = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments_single,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.01,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}\")\n",
    "trainer_single.train()\n",
    "\n",
    "# Lưu mô hình Single Run\n",
    "model.save_pretrained(\"./finetuned_vinallama_single\")\n",
    "tokenizer.save_pretrained(\"./finetuned_vinallama_single\")\n",
    "\n",
    "# Đánh giá Single Run\n",
    "test_inputs = eval_df['input'].tolist()\n",
    "test_references = eval_df['output'].tolist()\n",
    "predictions_single = generate_predictions(model, tokenizer, test_inputs)\n",
    "metrics_single = evaluate_metrics(predictions_single, test_references)\n",
    "print(\"Single Run Metrics:\", metrics_single)\n",
    "\n",
    "# Lưu metrics vào file\n",
    "with open(\"single_run_metrics_vinaLLaMA.json\", \"w\") as f:\n",
    "    json.dump(metrics_single, f, indent=4)\n",
    "\n",
    "# Kiểm tra mẫu dự đoán\n",
    "for i in range(5):\n",
    "    print(f\"Input: {test_inputs[i]}\")\n",
    "    print(f\"Prediction: {predictions_single[i]}\")\n",
    "    print(f\"Reference: {test_references[i]}\\n\")\n",
    "\n",
    "del model, trainer_single\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6114ce9-4cc6-4bf4-b7a9-d82f16800442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50809"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18c38bce",
   "metadata": {
    "id": "18c38bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vilm/vinallama-7b-chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_memory\": {\n",
      "    \"cpu\": 329543315456\n",
      "  },\n",
      "  \"no_split_module_classes\": [\n",
      "    \"LlamaDecoderLayer\"\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"special_dtypes\": {},\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "PyTorch: setting up devices\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train size: 1176, Eval size: 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 24078.37 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1176/1176 [00:00<00:00, 35779.06 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 24004.19 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 4491.26 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 400267.91 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 21603.89 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 31038.52 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 21789.94 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 4341.47 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 120997.43 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,176\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 79,953,920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/219 06:12 < 02:19, 0.42 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.379500</td>\n",
       "      <td>4.531033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.161900</td>\n",
       "      <td>3.574407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>2.652926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.432600</td>\n",
       "      <td>2.204682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.084500</td>\n",
       "      <td>1.964558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.911800</td>\n",
       "      <td>1.875689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.796600</td>\n",
       "      <td>1.803997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.719000</td>\n",
       "      <td>1.779074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.646500</td>\n",
       "      <td>1.728266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.691800</td>\n",
       "      <td>1.696275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.585800</td>\n",
       "      <td>1.657658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.596800</td>\n",
       "      <td>1.636317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.532700</td>\n",
       "      <td>1.616935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.513800</td>\n",
       "      <td>1.609249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.513400</td>\n",
       "      <td>1.599591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.423300</td>\n",
       "      <td>1.593062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_1/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_1/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_1/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_1/checkpoint-100 (score: 1.5930616855621338).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_vinallama_fold_1/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_vinallama_fold_1/special_tokens_map.json\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Metrics: {'rouge1': np.float64(0.6055426213230672), 'rouge2': np.float64(0.2677417805359291), 'rougeL': np.float64(0.3881789585562806), 'meteor': np.float64(0.28941821680338164), 'cosine_similarity': np.float64(0.7638248445624012)}\n",
      "Thời gian chạy Fold 1: 1233.10 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.33s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vilm/vinallama-7b-chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_memory\": {\n",
      "    \"cpu\": 329543315456\n",
      "  },\n",
      "  \"no_split_module_classes\": [\n",
      "    \"LlamaDecoderLayer\"\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"special_dtypes\": {},\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "PyTorch: setting up devices\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 22585.61 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 34187.41 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 24050.35 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 4361.05 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 410877.72 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 19903.24 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 29007.21 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21553.61 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 4144.15 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 129843.67 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 79,953,920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/219 06:41 < 01:56, 0.42 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.407100</td>\n",
       "      <td>4.557483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.162900</td>\n",
       "      <td>3.607963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.077200</td>\n",
       "      <td>2.638389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.381300</td>\n",
       "      <td>2.234845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.099100</td>\n",
       "      <td>2.020255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.866800</td>\n",
       "      <td>1.914581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.853600</td>\n",
       "      <td>1.811363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.928400</td>\n",
       "      <td>1.769681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.627300</td>\n",
       "      <td>1.710964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.669200</td>\n",
       "      <td>1.680158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.603600</td>\n",
       "      <td>1.657318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.596100</td>\n",
       "      <td>1.629064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.563000</td>\n",
       "      <td>1.611495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.553900</td>\n",
       "      <td>1.592352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.557000</td>\n",
       "      <td>1.586587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.402000</td>\n",
       "      <td>1.581455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.421900</td>\n",
       "      <td>1.577172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_2/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_2/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_2/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_2/checkpoint-100 (score: 1.5771722793579102).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_vinallama_fold_2/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_vinallama_fold_2/special_tokens_map.json\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Metrics: {'rouge1': np.float64(0.612296227330078), 'rouge2': np.float64(0.27642749006127265), 'rougeL': np.float64(0.39567571816755953), 'meteor': np.float64(0.31775463417489735), 'cosine_similarity': np.float64(0.7789355317751566)}\n",
      "Thời gian chạy Fold 2: 1384.12 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vilm/vinallama-7b-chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_memory\": {\n",
      "    \"cpu\": 329543315456\n",
      "  },\n",
      "  \"no_split_module_classes\": [\n",
      "    \"LlamaDecoderLayer\"\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"special_dtypes\": {},\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "PyTorch: setting up devices\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23277.41 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 34165.64 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 24192.61 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 4309.91 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 418186.85 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21035.92 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 26515.40 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21012.62 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 4236.37 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 134708.91 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 79,953,920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/219 06:50 < 01:59, 0.41 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.358800</td>\n",
       "      <td>4.621956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.245700</td>\n",
       "      <td>3.671950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.122100</td>\n",
       "      <td>2.687490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.364700</td>\n",
       "      <td>2.252066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.056100</td>\n",
       "      <td>2.012088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.876100</td>\n",
       "      <td>1.901807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.812200</td>\n",
       "      <td>1.825553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.870500</td>\n",
       "      <td>1.807350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.632000</td>\n",
       "      <td>1.745516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.662700</td>\n",
       "      <td>1.704270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.570500</td>\n",
       "      <td>1.674341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.575000</td>\n",
       "      <td>1.648396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.569100</td>\n",
       "      <td>1.629644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.560200</td>\n",
       "      <td>1.615251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.646300</td>\n",
       "      <td>1.607693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.362700</td>\n",
       "      <td>1.601709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.397200</td>\n",
       "      <td>1.597535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_3/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_3/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_3/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_3/checkpoint-100 (score: 1.5975353717803955).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_vinallama_fold_3/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_vinallama_fold_3/special_tokens_map.json\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "Attempting to create safetensors variant\n",
      "loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Metrics: {'rouge1': np.float64(0.6062048112639524), 'rouge2': np.float64(0.26864109538770053), 'rougeL': np.float64(0.38330085566116967), 'meteor': np.float64(0.30529588745743913), 'cosine_similarity': np.float64(0.7730792807478483)}\n",
      "Thời gian chạy Fold 3: 1380.63 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.33s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vilm/vinallama-7b-chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_memory\": {\n",
      "    \"cpu\": 329543315456\n",
      "  },\n",
      "  \"no_split_module_classes\": [\n",
      "    \"LlamaDecoderLayer\"\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"special_dtypes\": {},\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "PyTorch: setting up devices\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23136.13 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 34099.56 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23632.54 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 4620.22 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 421147.91 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21237.71 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 30661.79 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 20974.37 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 4430.19 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 133773.64 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 79,953,920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/219 07:05 < 01:33, 0.42 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.341000</td>\n",
       "      <td>4.560609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.208000</td>\n",
       "      <td>3.593446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.051700</td>\n",
       "      <td>2.638216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.411900</td>\n",
       "      <td>2.193009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.076700</td>\n",
       "      <td>1.950299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.915500</td>\n",
       "      <td>1.840190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.827300</td>\n",
       "      <td>1.775663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.904400</td>\n",
       "      <td>1.749025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.634600</td>\n",
       "      <td>1.710365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.671100</td>\n",
       "      <td>1.669446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.609200</td>\n",
       "      <td>1.632012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.568900</td>\n",
       "      <td>1.601398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.570200</td>\n",
       "      <td>1.588018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.552200</td>\n",
       "      <td>1.575146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.682800</td>\n",
       "      <td>1.563682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.409900</td>\n",
       "      <td>1.554413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.420100</td>\n",
       "      <td>1.552172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.398400</td>\n",
       "      <td>1.548026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_4/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_4/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_4/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_4/checkpoint-100 (score: 1.5480263233184814).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_vinallama_fold_4/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_vinallama_fold_4/special_tokens_map.json\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Metrics: {'rouge1': np.float64(0.6122763465912706), 'rouge2': np.float64(0.274433468144246), 'rougeL': np.float64(0.39098477381338026), 'meteor': np.float64(0.31384813958819785), 'cosine_similarity': np.float64(0.7736076315244039)}\n",
      "Thời gian chạy Fold 4: 1437.22 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vilm/vinallama-7b-chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_memory\": {\n",
      "    \"cpu\": 329543315456\n",
      "  },\n",
      "  \"no_split_module_classes\": [\n",
      "    \"LlamaDecoderLayer\"\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"special_dtypes\": {},\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "PyTorch: setting up devices\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 22746.50 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 33881.91 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23574.53 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 4384.48 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 439051.57 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21762.83 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 30476.14 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 22042.14 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 4312.80 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 137871.80 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 79,953,920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/219 06:46 < 01:58, 0.41 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.441400</td>\n",
       "      <td>4.582274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.162300</td>\n",
       "      <td>3.633148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.148700</td>\n",
       "      <td>2.695912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.381600</td>\n",
       "      <td>2.242953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.076000</td>\n",
       "      <td>2.039780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.847700</td>\n",
       "      <td>1.905504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.818100</td>\n",
       "      <td>1.831718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.984100</td>\n",
       "      <td>1.776254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.596500</td>\n",
       "      <td>1.735555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.648800</td>\n",
       "      <td>1.688941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.579900</td>\n",
       "      <td>1.669556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.553600</td>\n",
       "      <td>1.646289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.534400</td>\n",
       "      <td>1.627616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.524600</td>\n",
       "      <td>1.615540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.613000</td>\n",
       "      <td>1.605963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.417800</td>\n",
       "      <td>1.601183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.398300</td>\n",
       "      <td>1.596833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_5/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_5/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_5/checkpoint-100 (score: 1.5968326330184937).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_vinallama_fold_5/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_vinallama_fold_5/special_tokens_map.json\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Metrics: {'rouge1': np.float64(0.6157353286441439), 'rouge2': np.float64(0.2801402317087287), 'rougeL': np.float64(0.39444506773439614), 'meteor': np.float64(0.31114035435930554), 'cosine_similarity': np.float64(0.7833532226734421)}\n",
      "Thời gian chạy Fold 5: 1351.36 giây\n",
      "\n",
      "=== Kết thúc huấn luyện K-Fold ===\n",
      "Tổng thời gian chạy: 6789.07 giây\n",
      "Thời gian trung bình mỗi fold: 1357.29 giây\n",
      "\n",
      "Average Cross-Validation Metrics: {'rouge1': np.float64(0.6104110670305024), 'rouge2': np.float64(0.27347681316757544), 'rougeL': np.float64(0.39051707478655723), 'meteor': np.float64(0.3074914464766443), 'cosine_similarity': np.float64(0.7745601022566504)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_1/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 46303,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 46305\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_1/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_1/checkpoint-100/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 5-Fold Cross-Validation\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Cấu hình KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "fold_models = []\n",
    "fold_times = []\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Lặp qua từng fold\n",
    "for fold, (train_idx, eval_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"\\nTraining Fold {fold + 1}...\")\n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    # Tạo tập train và eval cho fold hiện tại\n",
    "    train_fold = df.iloc[train_idx][['input', 'output']]\n",
    "    eval_fold = df.iloc[eval_idx][['input', 'output']]\n",
    "    train_fold_dataset = Dataset.from_pandas(train_fold)\n",
    "    eval_fold_dataset = Dataset.from_pandas(eval_fold)\n",
    "\n",
    "    # Loại bỏ cột không cần thiết\n",
    "    train_fold_dataset = train_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_fold_dataset.column_names else [])\n",
    "    eval_fold_dataset = eval_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_fold_dataset.column_names else [])\n",
    "\n",
    "    # Tải lại mô hình gốc với INT4 quantization\n",
    "    model, tokenizer, peft_config = load_model_and_tokenizer(quantization=\"int4\")\n",
    "    print(f\"Fold {fold + 1} - Train size: {len(train_fold_dataset)}, Eval size: {len(eval_fold_dataset)}\")\n",
    "    # Cấu hình huấn luyện cho fold\n",
    "    training_arguments_fold = TrainingArguments(\n",
    "        output_dir=f\"./results_fold_{fold + 1}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.1,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.1,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        log_level=\"info\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    # Huấn luyện fold\n",
    "    trainer_fold = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments_fold,\n",
    "        train_dataset=train_fold_dataset,\n",
    "        eval_dataset=eval_fold_dataset,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=formatting_func,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,\n",
    "                early_stopping_threshold=0.01,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    trainer_fold.train()\n",
    "\n",
    "    # Lưu mô hình fold\n",
    "    fold_path = f\"./finetuned_vinallama_fold_{fold + 1}\"\n",
    "    model.save_pretrained(fold_path)\n",
    "    tokenizer.save_pretrained(fold_path)\n",
    "    fold_models.append(fold_path)\n",
    "\n",
    "    # Đánh giá fold\n",
    "    test_inputs_fold = eval_fold['input'].tolist()\n",
    "    test_references_fold = eval_fold['output'].tolist()\n",
    "    predictions_fold = generate_predictions(model, tokenizer, test_inputs_fold)\n",
    "    metrics_fold = evaluate_metrics(predictions_fold, test_references_fold)\n",
    "    print(f\"Fold {fold + 1} Metrics:\", metrics_fold)\n",
    "    fold_metrics.append(metrics_fold)\n",
    "\n",
    "    # Lưu metrics của fold\n",
    "    with open(f\"fold_{fold + 1}_metrics_vinaLLaMA.json\", \"w\") as f:\n",
    "        json.dump(metrics_fold, f, indent=4)\n",
    "    \n",
    "    fold_end_time = time.time()\n",
    "    fold_duration = fold_end_time - fold_start_time\n",
    "    fold_times.append(fold_duration)\n",
    "    print(f\"Thời gian chạy Fold {fold + 1}: {fold_duration:.2f} giây\")\n",
    "    # Dọn dẹp bộ nhớ\n",
    "    del model, trainer_fold\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Tính tổng thời gian và thời gian trung bình\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\n=== Kết thúc huấn luyện K-Fold ===\")\n",
    "print(f\"Tổng thời gian chạy: {total_duration:.2f} giây\")\n",
    "print(f\"Thời gian trung bình mỗi fold: {np.mean(fold_times):.2f} giây\")\n",
    "\n",
    "# Tính trung bình metrics qua các fold\n",
    "avg_metrics = {\n",
    "    \"rouge1\": np.mean([m[\"rouge1\"] for m in fold_metrics]),\n",
    "    \"rouge2\": np.mean([m[\"rouge2\"] for m in fold_metrics]),\n",
    "    \"rougeL\": np.mean([m[\"rougeL\"] for m in fold_metrics]),\n",
    "    \"meteor\": np.mean([m[\"meteor\"] for m in fold_metrics]),\n",
    "    \"cosine_similarity\": np.mean([m[\"cosine_similarity\"] for m in fold_metrics]),\n",
    "}\n",
    "print(\"\\nAverage Cross-Validation Metrics:\", avg_metrics)\n",
    "\n",
    "# Lưu metrics trung bình\n",
    "with open(\"cross_validation_vinallama_metrics.json\", \"w\") as f:\n",
    "    json.dump(avg_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58f93a",
   "metadata": {
    "id": "6b58f93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tìm fold tốt nhất dựa trên metric: 'cosine_similarity' (cao hơn là tốt hơn: True)\n",
      "Fold 1: 'cosine_similarity' = 0.7638\n",
      "Fold 2: 'cosine_similarity' = 0.7789\n",
      "Fold 3: 'cosine_similarity' = 0.7731\n",
      "Fold 4: 'cosine_similarity' = 0.7736\n",
      "Fold 5: 'cosine_similarity' = 0.7834\n",
      "\n",
      "=> Fold tốt nhất được chọn: Fold 5 với cosine_similarity = 0.7834\n",
      "\n",
      "--- Bắt đầu quá trình merge model cho Fold 5 ---\n",
      "Đã giải phóng bộ nhớ GPU (nếu có)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vilm--vinallama-7b-chat/snapshots/9ed2afd8c26e4372a13003736bf4c474d661ebb3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer#, BitsAndBytesConfig # Không cần BitsAndBytesConfig cho bước này\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# --- Phần 1: Tìm fold có metric tốt nhất ---\n",
    "\n",
    "def load_metrics_from_file(file_path):\n",
    "    \"\"\"Đọc metrics từ file JSON.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Cảnh báo: Không tìm thấy file metric: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Cảnh báo: File metric không phải JSON hợp lệ: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def find_best_fold_by_metric(metrics_base_path, num_folds, metric_name_to_optimize, higher_is_better=True):\n",
    "    \"\"\"\n",
    "    Tìm fold có giá trị metric được chỉ định cao nhất (hoặc thấp nhất).\n",
    "\n",
    "    Args:\n",
    "        metrics_base_path (str): Đường dẫn cơ sở đến thư mục chứa các file metrics.\n",
    "                                 Ví dụ: \"./\" nếu các file ở thư mục hiện tại.\n",
    "        num_folds (int): Tổng số lượng folds.\n",
    "        metric_name_to_optimize (str): Tên của metric dùng để so sánh (ví dụ: 'rougeL', 'cosine_similarity').\n",
    "        higher_is_better (bool): True nếu giá trị metric cao hơn là tốt hơn, False nếu thấp hơn là tốt hơn.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_fold_number, best_metric_value, all_metrics_of_best_fold) hoặc (None, None, None) nếu lỗi.\n",
    "    \"\"\"\n",
    "    best_fold_so_far = None\n",
    "    best_metric_val = -float('inf') if higher_is_better else float('inf')\n",
    "    all_metrics_best_fold = None\n",
    "\n",
    "    print(f\"Đang tìm fold tốt nhất dựa trên metric: '{metric_name_to_optimize}' (cao hơn là tốt hơn: {higher_is_better})\")\n",
    "\n",
    "    for i in range(1, num_folds + 1):\n",
    "        # Giả sử tên file là \"fold_X_metrics\" hoặc \"fold_X_metrics.json\"\n",
    "        # Sửa lại mẫu tên file nếu cần\n",
    "        metric_file_candidate_1 = os.path.join(metrics_base_path, f\"fold_{i}_metrics_vinaLLaMA.json\")\n",
    "        metric_file_candidate_2 = os.path.join(metrics_base_path, f\"fold_{i}_metrics_vinaLLaMA\") # Không có đuôi .json\n",
    "\n",
    "        metrics_data = None\n",
    "        if os.path.exists(metric_file_candidate_1):\n",
    "            metrics_data = load_metrics_from_file(metric_file_candidate_1)\n",
    "        elif os.path.exists(metric_file_candidate_2):\n",
    "            metrics_data = load_metrics_from_file(metric_file_candidate_2)\n",
    "        else:\n",
    "            print(f\"Không tìm thấy file metric cho fold {i} tại: {metric_file_candidate_1} hoặc {metric_file_candidate_2}\")\n",
    "            continue\n",
    "\n",
    "        if metrics_data is None:\n",
    "            continue # Bỏ qua nếu không đọc được file\n",
    "\n",
    "        if metric_name_to_optimize not in metrics_data:\n",
    "            print(f\"Cảnh báo: Metric '{metric_name_to_optimize}' không có trong file của fold {i}. Bỏ qua fold này để so sánh.\")\n",
    "            continue\n",
    "\n",
    "        current_metric_val = metrics_data[metric_name_to_optimize]\n",
    "        print(f\"Fold {i}: '{metric_name_to_optimize}' = {current_metric_val:.4f}\")\n",
    "\n",
    "        if higher_is_better:\n",
    "            if current_metric_val > best_metric_val:\n",
    "                best_metric_val = current_metric_val\n",
    "                best_fold_so_far = i\n",
    "                all_metrics_best_fold = metrics_data\n",
    "        else: # lower is better\n",
    "            if current_metric_val < best_metric_val:\n",
    "                best_metric_val = current_metric_val\n",
    "                best_fold_so_far = i\n",
    "                all_metrics_best_fold = metrics_data\n",
    "\n",
    "    if best_fold_so_far is not None:\n",
    "        print(f\"\\n=> Fold tốt nhất được chọn: Fold {best_fold_so_far} với {metric_name_to_optimize} = {best_metric_val:.4f}\")\n",
    "        # print(f\"Toàn bộ metrics của fold {best_fold_so_far}: {all_metrics_best_fold}\")\n",
    "        return best_fold_so_far, best_metric_val, all_metrics_best_fold\n",
    "    else:\n",
    "        print(f\"\\n=> Không thể xác định fold tốt nhất dựa trên metric '{metric_name_to_optimize}'.\")\n",
    "        return None, None, None\n",
    "\n",
    "# --- Cấu hình cho việc tìm fold ---\n",
    "# Đặt đường dẫn đến thư mục chứa các file fold_X_metrics của bạn\n",
    "# Ví dụ: nếu các file fold_1_metrics, fold_2_metrics,... nằm cùng thư mục với script này:\n",
    "METRICS_FILES_DIRECTORY = \"./\" \n",
    "NUM_TOTAL_FOLDS = 5\n",
    "# Chọn metric bạn muốn sử dụng để quyết định fold nào tốt nhất\n",
    "# Ví dụ: 'rougeL', 'cosine_similarity', 'meteor', 'rouge1', 'rouge2'\n",
    "METRIC_TO_OPTIMIZE_FOR = \"cosine_similarity\" # THAY ĐỔI TÊN METRIC NÀY NẾU CẦN\n",
    "\n",
    "best_fold_id, _, _ = find_best_fold_by_metric(\n",
    "    METRICS_FILES_DIRECTORY,\n",
    "    NUM_TOTAL_FOLDS,\n",
    "    METRIC_TO_OPTIMIZE_FOR,\n",
    "    higher_is_better=True # Hầu hết các metric này, cao hơn là tốt hơn\n",
    ")\n",
    "\n",
    "if best_fold_id is None:\n",
    "    print(\"Không thể xác định fold tốt nhất. Sẽ sử dụng một fold mặc định hoặc dừng chương trình.\")\n",
    "    # Gán một fold mặc định nếu muốn tiếp tục\n",
    "    default_fold_if_not_found = 4 # Ví dụ, bạn có thể muốn mặc định là fold 4\n",
    "    print(f\"Sử dụng fold mặc định: {default_fold_if_not_found}\")\n",
    "    best_fold_id = default_fold_if_not_found\n",
    "    # Hoặc bạn có thể dừng chương trình:\n",
    "    # exit(\"Dừng chương trình do không tìm được fold tốt nhất.\")\n",
    "\n",
    "# --- Phần 2: Merge model sử dụng adapter từ fold tốt nhất ---\n",
    "# Sửa đổi để merge vào base model ở định dạng full/half precision\n",
    "\n",
    "print(f\"\\n--- Bắt đầu quá trình merge model cho Fold {best_fold_id} ---\")\n",
    "\n",
    "try:\n",
    "    # Giải phóng bộ nhớ trước khi tải model lớn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Đã giải phóng bộ nhớ GPU (nếu có).\")\n",
    "\n",
    "    # Cấu hình tải mô hình gốc\n",
    "    base_model_name = \"vilm/vinallama-7b-chat\" # Thay bằng model base của bạn nếu khác\n",
    "\n",
    "    print(f\"Đang tải tokenizer cho model: {base_model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    print(\"Tokenizer đã được tải.\")\n",
    "\n",
    "    # Tải base model ở định dạng Bfloat16 (hoặc Float16) - KHÔNG DÙNG BitsAndBytes\n",
    "    print(f\"Đang tải base model '{base_model_name}' ở định dạng BF16 (không lượng tử hóa BitsAndBytes)...\")\n",
    "    # Bỏ hoàn toàn quantization_config khi tải base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        # quantization_config=quantization_config, # <-- BỎ DÒNG NÀY!\n",
    "        torch_dtype=torch.bfloat16, # Tải ở bfloat16 (kích thước lớn)\n",
    "        device_map=\"cuda\" # Vẫn dùng auto device_map để phân bổ lên GPU nếu có đủ VRAM\n",
    "    )\n",
    "    print(\"Base model đã được tải thành công ở định dạng BF16.\")\n",
    "\n",
    "    # Đường dẫn tới adapter LoRA của fold tốt nhất\n",
    "    # Cần điều chỉnh đường dẫn này cho phù hợp với cấu trúc thư mục của bạn\n",
    "    # Đảm bảo bạn sử dụng tên thư mục fine-tuned adapter đúng với fold tốt nhất tìm được ở Phần 1\n",
    "    fine_tuned_adapters_base_dir = \"/home/thanhnguyenvq2403/model/KLTN/\" # Giả định thư mục chứa fine-tuned models\n",
    "    adapter_path_for_best_fold = os.path.join(fine_tuned_adapters_base_dir, f\"finetuned_vinallama_fold_{best_fold_id}\") # <--- SỬA TÊN THƯ MỤC ADAPTER NẾU CẦN (ví dụ: vinallama thay vì seaLLM)\n",
    "\n",
    "    print(f\"Đang tải adapter LoRA từ: {adapter_path_for_best_fold}\")\n",
    "    if not os.path.isdir(adapter_path_for_best_fold):\n",
    "        raise FileNotFoundError(f\"Lỗi: Thư mục adapter LoRA không tồn tại: {adapter_path_for_best_fold}\")\n",
    "\n",
    "    lora_model = PeftModel.from_pretrained(base_model, adapter_path_for_best_fold, is_trainable=False)\n",
    "    print(\"Adapter LoRA đã được tải.\")\n",
    "\n",
    "    # Hợp nhất adapter vào base model (ở định dạng BF16)\n",
    "    # Kết quả merged_model sẽ là mô hình dense ở định dạng BF16\n",
    "    print(\"Đang hợp nhất adapter LoRA vào base model (BF16)...\")\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    print(\"Hợp nhất adapter thành công. Mô hình đã hợp nhất ở định dạng BF16.\")\n",
    "\n",
    "    # Lưu mô hình đã hợp nhất\n",
    "    # Đặt tên cho thư mục lưu model đã merge.\n",
    "    # Tên này nên phản ánh rằng nó đã merge và ở định dạng không lượng tử hóa BitsAndBytes\n",
    "    output_merged_model_dir = f\"/home/thanhnguyenvq2403/model/KLTN/merged_vinallama_fold_{best_fold_id}_bf16\" # Đổi tên cho rõ định dạng\n",
    "    print(f\"Đang lưu mô hình đã hợp nhất vào: {output_merged_model_dir}\")\n",
    "\n",
    "    # Đảm bảo thư mục output tồn tại\n",
    "    os.makedirs(output_merged_model_dir, exist_ok=True)\n",
    "\n",
    "    merged_model.save_pretrained(output_merged_model_dir, safe_serialization=True) # Nên dùng safe_serialization\n",
    "    tokenizer.save_pretrained(output_merged_model_dir) # Lưu cả tokenizer\n",
    "\n",
    "    print(f\"Mô hình đã hợp nhất và tokenizer đã được lưu vào: {output_merged_model_dir}\")\n",
    "\n",
    "    # Dọn dẹp bộ nhớ\n",
    "    del base_model\n",
    "    del lora_model\n",
    "    del merged_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Đã dọn dẹp bộ nhớ.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     print(f\"LỖI FILE: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"ĐÃ CÓ LỖI XẢY RA TRONG QUÁ TRÌNH MERGE MODEL: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "046403be22d543dab3c5d9c3d3836b9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_788d9b117db34ff6b06e1fd6a942c39c",
       "IPY_MODEL_916b489caeac4041839f1ee2365360a3",
       "IPY_MODEL_c8d0620a47874f34b94681c71f35c3c8"
      ],
      "layout": "IPY_MODEL_57e0a819a11b42e0a35b75e072b2dbb1"
     }
    },
    "076e2be9ac414d6d92b90f203da87c38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d402147806d413590c8bbbcd89646de",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_edf820a16c044fe0a2d2f85b11b64683",
      "value": 1176
     }
    },
    "079359b31bb04296a232a84fc1b7db2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff5faacf70a04f19833d41589b7a78e5",
       "IPY_MODEL_e0d2e5d548e04e2ebcf6b5a79083f0dc",
       "IPY_MODEL_7ac0a7531ecc4823b33549eeff34a131"
      ],
      "layout": "IPY_MODEL_970af49c4f1940758130a5e334901a14"
     }
    },
    "0926abb88e2e4381b4b38839e6f652f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ac46097afa5445591d9a3fbad639ae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f4b1cc17a48461f8e2da6dc9b03c82a",
      "placeholder": "​",
      "style": "IPY_MODEL_4d0cb99e715847d0b05a031736ecd688",
      "value": " 1176/1176 [00:00&lt;00:00, 18224.04 examples/s]"
     }
    },
    "1201dfd96b7d4fcabf5b10b3fa769759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efc1ff78f5f546638b298419a4151695",
      "placeholder": "​",
      "style": "IPY_MODEL_0926abb88e2e4381b4b38839e6f652f5",
      "value": "Truncating train dataset: 100%"
     }
    },
    "1807262027f34e70b1e94883ac402b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a66367d4522a4b1eb6ce7b88394f8e30",
      "placeholder": "​",
      "style": "IPY_MODEL_a0041fc725964c50a3beff504ddeb9ab",
      "value": " 294/294 [00:00&lt;00:00, 2586.64 examples/s]"
     }
    },
    "187ce14699ae454c819ceacc050a955f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ffc5523dd084452a20ae6c913d39e47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e320bdd7a7c484183ed8b93e6705af6",
      "placeholder": "​",
      "style": "IPY_MODEL_cc5fad4a005e4aa9bcaecccb8217df1e",
      "value": " 1176/1176 [00:00&lt;00:00, 2971.18 examples/s]"
     }
    },
    "220a0b50b8d04e3dac9bbaf89de8a7c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1201dfd96b7d4fcabf5b10b3fa769759",
       "IPY_MODEL_98ae3ea018e24dabb90618b485a2b2cb",
       "IPY_MODEL_85f4e94e397f43a588a527fe41bc015a"
      ],
      "layout": "IPY_MODEL_686120f3641f43298071d6a5e32fe644"
     }
    },
    "23b95f393dd940039de764bd67d7709a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "246d63e4a14841c594db092689f2a64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb370caba2b844c486d30748e6caf9e0",
      "placeholder": "​",
      "style": "IPY_MODEL_73e04b4d775848a19e267669303d8266",
      "value": "Tokenizing train dataset: 100%"
     }
    },
    "2ae2a398a2a945d18ec5409bda99644a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d03ea85ed9e4262bfa21065ad73eae4",
       "IPY_MODEL_5f8edfd1f93f4001b51bde9e815d66af",
       "IPY_MODEL_f1a26a9520da4b888f8c54322ae29b82"
      ],
      "layout": "IPY_MODEL_ca3c86d9cf48410199515ce86fd68eaf"
     }
    },
    "2bd1218e781f4784be96419b838c45af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c01848e500843e39f326eeb3b0a8cce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d03ea85ed9e4262bfa21065ad73eae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fd2597c60874a4f9c920f8420b366f0",
      "placeholder": "​",
      "style": "IPY_MODEL_cd056a6bb14048bdbc7860ce2c06303f",
      "value": "Applying formatting function to train dataset: 100%"
     }
    },
    "2d402147806d413590c8bbbcd89646de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35fcfd38ee344fd0924e883972074784": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d7b45cd5bfa4ad9ba035ac29882fb8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80acce4d1d5c4459ab794ba6bbcc32cf",
      "placeholder": "​",
      "style": "IPY_MODEL_fa04a7d89df34dfcbd73d62f4dda799c",
      "value": " 294/294 [00:00&lt;00:00, 15282.07 examples/s]"
     }
    },
    "3fd2597c60874a4f9c920f8420b366f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4375f815d6724658a2cfaa6c608e478f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43b4ce75564943ca8b250e54a7ce1834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "442e0cfbeb554fd58e2bf0c2d77829c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4526ca6058b74e11ba4329d843d6a9d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "494b39f1fcf9434b88559980e35c006f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a974c0a2ec043219c538076bfffc87a",
      "placeholder": "​",
      "style": "IPY_MODEL_79f22a64d0fa4310b9ee63bbfb66b41a",
      "value": "Tokenizing eval dataset: 100%"
     }
    },
    "4a251249b63e4688ba357a8d412a3b69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1267d7977354e5aa3db8a28f511f2b8",
      "placeholder": "​",
      "style": "IPY_MODEL_9e526d55e8f84c66ab040536bc502d2b",
      "value": " 294/294 [00:00&lt;00:00, 7943.86 examples/s]"
     }
    },
    "4adcb8502fce4dd1af5b8a5b648fa263": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b47975658384af1a549c9f1acf1e5a5",
       "IPY_MODEL_56eff52cba7c4e06a6378805f2aa2707",
       "IPY_MODEL_4a251249b63e4688ba357a8d412a3b69"
      ],
      "layout": "IPY_MODEL_55fdfa966042462bb35ee44cfbfb841e"
     }
    },
    "4d0cb99e715847d0b05a031736ecd688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4eaab0584d164139b520cd15eaa3793f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_187ce14699ae454c819ceacc050a955f",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a08c13a1cfcb4fd295994b99f788d6c3",
      "value": 294
     }
    },
    "551f5413b209400fa23bedb10c4cde9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55b1ad5ece2c4be18a2acdb322b93c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fc18b5fb2e349afa2489a0adf1b21fc",
      "placeholder": "​",
      "style": "IPY_MODEL_2bd1218e781f4784be96419b838c45af",
      "value": "Truncating eval dataset: 100%"
     }
    },
    "55fdfa966042462bb35ee44cfbfb841e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56eff52cba7c4e06a6378805f2aa2707": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ea9a076bce947bab5096a028f7b3237",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c034029a3d5e48ebaf7ea25fa7cb96fd",
      "value": 294
     }
    },
    "57e0a819a11b42e0a35b75e072b2dbb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58850dfce0cc4811ba06ab1ea71d306d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b9cb618b32044518965997a96a7d930": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c5ac7432fae43f2a17870b83e19a17a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f8edfd1f93f4001b51bde9e815d66af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d98a8f43fc76406897ffbb66f7e136c0",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8f7022d52464b0eb74f965319229cef",
      "value": 1176
     }
    },
    "5fc18b5fb2e349afa2489a0adf1b21fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "634e691c8c8a4fb68c5af7704b9ce0fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65597da15fda4b51b7aa859b380c12db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "686120f3641f43298071d6a5e32fe644": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69fe2be7c9814f2ab93168f3d6ac45ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a974c0a2ec043219c538076bfffc87a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d044e9840e449c5b1e718fb06e99515": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f4b1cc17a48461f8e2da6dc9b03c82a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e04b4d775848a19e267669303d8266": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "788d9b117db34ff6b06e1fd6a942c39c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba961b0dab1a4ac6864646de1d46d9e6",
      "placeholder": "​",
      "style": "IPY_MODEL_442e0cfbeb554fd58e2bf0c2d77829c0",
      "value": "Converting train dataset to ChatML: 100%"
     }
    },
    "79f22a64d0fa4310b9ee63bbfb66b41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79f8dacb371b4fd987b8a4330520cc04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ac0a7531ecc4823b33549eeff34a131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d044e9840e449c5b1e718fb06e99515",
      "placeholder": "​",
      "style": "IPY_MODEL_d00e51ae50d144e392bf12aa8b54b303",
      "value": " 294/294 [00:00&lt;00:00, 5948.68 examples/s]"
     }
    },
    "80acce4d1d5c4459ab794ba6bbcc32cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83a4b7f740ec41389a6df6317af60615": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "841bc431816441a4bc79dde9600f9176": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85f4e94e397f43a588a527fe41bc015a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c5ac7432fae43f2a17870b83e19a17a",
      "placeholder": "​",
      "style": "IPY_MODEL_d224187c0f7340279949cddca91d3ecb",
      "value": " 1176/1176 [00:00&lt;00:00, 56603.68 examples/s]"
     }
    },
    "8983e348f59749bd86146b513c5acacf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cd9559471a9418d8a134b43c6dc1258": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fcd62139ad44297b2a950d97850468f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90fadb3fb3e14b1baaca9e77f571a80e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e2e6f823b0224c52a44e1e798e5c1985",
       "IPY_MODEL_4eaab0584d164139b520cd15eaa3793f",
       "IPY_MODEL_943be389e1a947ad99ec7d4d76de6db0"
      ],
      "layout": "IPY_MODEL_2c01848e500843e39f326eeb3b0a8cce"
     }
    },
    "916b489caeac4041839f1ee2365360a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0da77700eab4ab0bb06e1fe7b5184e9",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69fe2be7c9814f2ab93168f3d6ac45ae",
      "value": 1176
     }
    },
    "9419d62d8de24bce9697c0f60467ea99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "943be389e1a947ad99ec7d4d76de6db0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a81481e6777b4ab78442363553b73381",
      "placeholder": "​",
      "style": "IPY_MODEL_ad666ee0fe7e48de9c4e934212bd337c",
      "value": " 294/294 [00:00&lt;00:00, 8462.00 examples/s]"
     }
    },
    "970af49c4f1940758130a5e334901a14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98ae3ea018e24dabb90618b485a2b2cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58850dfce0cc4811ba06ab1ea71d306d",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_634e691c8c8a4fb68c5af7704b9ce0fe",
      "value": 1176
     }
    },
    "9b47975658384af1a549c9f1acf1e5a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b95f393dd940039de764bd67d7709a",
      "placeholder": "​",
      "style": "IPY_MODEL_c68779b0088e4f1c867ef1be26e7112b",
      "value": "Converting eval dataset to ChatML: 100%"
     }
    },
    "9c6f2000b243485db70d65e993621d3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c778736f27e349edb304160691210f4b",
       "IPY_MODEL_076e2be9ac414d6d92b90f203da87c38",
       "IPY_MODEL_0ac46097afa5445591d9a3fbad639ae5"
      ],
      "layout": "IPY_MODEL_4375f815d6724658a2cfaa6c608e478f"
     }
    },
    "9e320bdd7a7c484183ed8b93e6705af6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e526d55e8f84c66ab040536bc502d2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ea9a076bce947bab5096a028f7b3237": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0041fc725964c50a3beff504ddeb9ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a08c13a1cfcb4fd295994b99f788d6c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a0da77700eab4ab0bb06e1fe7b5184e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a66367d4522a4b1eb6ce7b88394f8e30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7ee09b72b034234b02194ab94471a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a81481e6777b4ab78442363553b73381": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad666ee0fe7e48de9c4e934212bd337c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3fbfe3f3f604b698e7d869437c0d5fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_246d63e4a14841c594db092689f2a64c",
       "IPY_MODEL_c6909db59ff04666b867097216de70ba",
       "IPY_MODEL_1ffc5523dd084452a20ae6c913d39e47"
      ],
      "layout": "IPY_MODEL_35fcfd38ee344fd0924e883972074784"
     }
    },
    "b8f7022d52464b0eb74f965319229cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba961b0dab1a4ac6864646de1d46d9e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba9f2bb92edf477d94d6cfa1e4950711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bb370caba2b844c486d30748e6caf9e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c034029a3d5e48ebaf7ea25fa7cb96fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1267d7977354e5aa3db8a28f511f2b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3d35975670546ab9998a6389e3ec707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c68779b0088e4f1c867ef1be26e7112b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6909db59ff04666b867097216de70ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4526ca6058b74e11ba4329d843d6a9d4",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba9f2bb92edf477d94d6cfa1e4950711",
      "value": 1176
     }
    },
    "c778736f27e349edb304160691210f4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79f8dacb371b4fd987b8a4330520cc04",
      "placeholder": "​",
      "style": "IPY_MODEL_d990e34729bf43eca69270c1fd3dc8cb",
      "value": "Applying chat template to train dataset: 100%"
     }
    },
    "c8ad9a852eec438e972cadc8ea1f0fe2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8d0620a47874f34b94681c71f35c3c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_841bc431816441a4bc79dde9600f9176",
      "placeholder": "​",
      "style": "IPY_MODEL_5b9cb618b32044518965997a96a7d930",
      "value": " 1176/1176 [00:00&lt;00:00, 18682.87 examples/s]"
     }
    },
    "ca3c86d9cf48410199515ce86fd68eaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbbcc5aca91d46ba9b51859494da828b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_494b39f1fcf9434b88559980e35c006f",
       "IPY_MODEL_f849b4139b5d4ab3a2ae32ae83807160",
       "IPY_MODEL_1807262027f34e70b1e94883ac402b39"
      ],
      "layout": "IPY_MODEL_c3d35975670546ab9998a6389e3ec707"
     }
    },
    "cc5fad4a005e4aa9bcaecccb8217df1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd056a6bb14048bdbc7860ce2c06303f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d00e51ae50d144e392bf12aa8b54b303": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d224187c0f7340279949cddca91d3ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d43bfd1e65ae43af97b7fa6b44cf9023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d98a8f43fc76406897ffbb66f7e136c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d990e34729bf43eca69270c1fd3dc8cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc6578ccbf9740bfb315a07b6482d8bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0d2e5d548e04e2ebcf6b5a79083f0dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2283973a0de470a88d5bb8b78160982",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65597da15fda4b51b7aa859b380c12db",
      "value": 294
     }
    },
    "e2283973a0de470a88d5bb8b78160982": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2627486d9ba4848830b0687cbe8b3d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55b1ad5ece2c4be18a2acdb322b93c01",
       "IPY_MODEL_e502fdc27ded43fcbbc4d3fe3fac8030",
       "IPY_MODEL_3d7b45cd5bfa4ad9ba035ac29882fb8b"
      ],
      "layout": "IPY_MODEL_c8ad9a852eec438e972cadc8ea1f0fe2"
     }
    },
    "e2e6f823b0224c52a44e1e798e5c1985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cd9559471a9418d8a134b43c6dc1258",
      "placeholder": "​",
      "style": "IPY_MODEL_dc6578ccbf9740bfb315a07b6482d8bf",
      "value": "Applying chat template to eval dataset: 100%"
     }
    },
    "e502fdc27ded43fcbbc4d3fe3fac8030": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9419d62d8de24bce9697c0f60467ea99",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_43b4ce75564943ca8b250e54a7ce1834",
      "value": 294
     }
    },
    "edf820a16c044fe0a2d2f85b11b64683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "efc1ff78f5f546638b298419a4151695": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1a26a9520da4b888f8c54322ae29b82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8983e348f59749bd86146b513c5acacf",
      "placeholder": "​",
      "style": "IPY_MODEL_551f5413b209400fa23bedb10c4cde9f",
      "value": " 1176/1176 [00:00&lt;00:00, 11431.75 examples/s]"
     }
    },
    "f849b4139b5d4ab3a2ae32ae83807160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fcd62139ad44297b2a950d97850468f",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a7ee09b72b034234b02194ab94471a2a",
      "value": 294
     }
    },
    "fa04a7d89df34dfcbd73d62f4dda799c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff5faacf70a04f19833d41589b7a78e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83a4b7f740ec41389a6df6317af60615",
      "placeholder": "​",
      "style": "IPY_MODEL_d43bfd1e65ae43af97b7fa6b44cf9023",
      "value": "Applying formatting function to eval dataset: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
