{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1d461df2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d461df2",
        "outputId": "6dd982cb-7742-4e72-8a09-0ed9f7c4cb39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./venv310/lib/python3.10/site-packages (4.51.3)\n",
            "Requirement already satisfied: trl in ./venv310/lib/python3.10/site-packages (0.17.0)\n",
            "Requirement already satisfied: peft in ./venv310/lib/python3.10/site-packages (0.15.2)\n",
            "Requirement already satisfied: datasets in ./venv310/lib/python3.10/site-packages (3.6.0)\n",
            "Requirement already satisfied: evaluate in ./venv310/lib/python3.10/site-packages (0.4.3)\n",
            "Requirement already satisfied: rouge_score in ./venv310/lib/python3.10/site-packages (0.1.2)\n",
            "Requirement already satisfied: underthesea in ./venv310/lib/python3.10/site-packages (6.8.4)\n",
            "Requirement already satisfied: bitsandbytes in ./venv310/lib/python3.10/site-packages (0.45.5)\n",
            "Requirement already satisfied: thefuzz in ./venv310/lib/python3.10/site-packages (0.22.1)\n",
            "Requirement already satisfied: sentence-transformers in ./venv310/lib/python3.10/site-packages (4.1.0)\n",
            "Requirement already satisfied: numpy in ./venv310/lib/python3.10/site-packages (2.2.5)\n",
            "Requirement already satisfied: dotenv in ./venv310/lib/python3.10/site-packages (0.9.9)\n",
            "Requirement already satisfied: filelock in ./venv310/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv310/lib/python3.10/site-packages (from transformers) (0.31.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./venv310/lib/python3.10/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./venv310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./venv310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./venv310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv310/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./venv310/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./venv310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./venv310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in ./venv310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.1)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in ./venv310/lib/python3.10/site-packages (from trl) (1.6.0)\n",
            "Requirement already satisfied: rich in ./venv310/lib/python3.10/site-packages (from trl) (14.0.0)\n",
            "Requirement already satisfied: psutil in ./venv310/lib/python3.10/site-packages (from peft) (6.0.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in ./venv310/lib/python3.10/site-packages (from peft) (2.8.0.dev20250510+cu128)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./venv310/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv310/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./venv310/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in ./venv310/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./venv310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv310/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
            "Requirement already satisfied: absl-py in ./venv310/lib/python3.10/site-packages (from rouge_score) (2.2.2)\n",
            "Requirement already satisfied: nltk in ./venv310/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in ./venv310/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: Click>=6.0 in ./venv310/lib/python3.10/site-packages (from underthesea) (8.1.8)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in ./venv310/lib/python3.10/site-packages (from underthesea) (0.9.11)\n",
            "Requirement already satisfied: joblib in ./venv310/lib/python3.10/site-packages (from underthesea) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn in ./venv310/lib/python3.10/site-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in ./venv310/lib/python3.10/site-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.8.0.87)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.3.14)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.3.3.41)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.9.55)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.2.55)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.5.7.53)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.55)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0.11)\n",
            "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in ./venv310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3.0+git96316ce5)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in ./venv310/lib/python3.10/site-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.13.0->peft) (65.5.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in ./venv310/lib/python3.10/site-packages (from thefuzz) (3.13.0)\n",
            "Requirement already satisfied: scipy in ./venv310/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in ./venv310/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: python-dotenv in ./venv310/lib/python3.10/site-packages (from dotenv) (1.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in ./venv310/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv310/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv310/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv310/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv310/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv310/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./venv310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./venv310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv310/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv310/lib/python3.10/site-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./venv310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv310/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -U transformers trl peft datasets evaluate rouge_score underthesea bitsandbytes thefuzz sentence-transformers numpy dotenv\n",
        "# Tải tài nguyên NLTK cho METEOR\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1LRADR_xiIjl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LRADR_xiIjl",
        "outputId": "6b563c1a-fda4-4f07-e200-3cef8da70ce4"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14ddc4b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ddc4b6",
        "outputId": "d490c573-2ce6-404d-c9c2-338e596f6b1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from underthesea import word_tokenize\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, EarlyStoppingCallback\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fc0bf13f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0bf13f",
        "outputId": "d33edc49-fd95-4125-9f46-769b9af0efcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(38369, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Tải biến môi trường từ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# Đọc access token từ biến môi trường\n",
        "hf_token = os.getenv(\"HF_VISTRAL\")\n",
        "if not hf_token:\n",
        "    raise ValueError(\"Không tìm thấy HF_VISTRAL trong file .env. Vui lòng thêm token vào file .env với định dạng: HF_VISTRAL=your_token\")\n",
        "\n",
        "def load_model_and_tokenizer(quantization=\"int8\"):  # Thêm tham số quantization\n",
        "    model_name = \"Viet-Mistral/Vistral-7B-Chat\"\n",
        "\n",
        "    # Cấu hình quantization với bitsandbytes\n",
        "    if quantization == \"int8\":\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,  # Quantize thành INT8\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
        "            bnb_8bit_use_double_quant=True,  # Double quantization để tăng hiệu quả\n",
        "        )\n",
        "    elif quantization == \"int4\":\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,  # Quantize thành INT4\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
        "            bnb_4bit_use_double_quant=True,  # Double quantization\n",
        "            bnb_4bit_quant_type=\"nf4\",  # Dùng NF4 (Normalized Float 4-bit) để tối ưu\n",
        "        )\n",
        "    else:\n",
        "        quantization_config = None  # Không quantize\n",
        "\n",
        "    # Tải tokenizer và mô hình với token xác thực\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        token=hf_token  # Sử dụng token từ biến môi trường\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token  # Sử dụng token từ biến môi trường\n",
        "    )\n",
        "    print(model)\n",
        "    # Cấu hình LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.5,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    return model, tokenizer, peft_config\n",
        "\n",
        "# Tải mô hình với INT8 quantization\n",
        "model, tokenizer, peft_config = load_model_and_tokenizer(quantization=\"int8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e0fadab1",
      "metadata": {
        "id": "e0fadab1"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa formatting_func với apply_chat_template\n",
        "def formatting_func(example):\n",
        "    if not all(k in example for k in ['input', 'output']):\n",
        "        print('Thiếu key trong example:', example)\n",
        "        return ''\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example['input']},\n",
        "        {\"role\": \"assistant\", \"content\": example['output']}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cc6ee4ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc6ee4ec",
        "outputId": "2b63813f-cece-4a50-86b9-5f3e801bb8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng giá trị duy nhất trong 'input' (exact): 1599\n",
            "Số lượng giá trị duy nhất trong 'output' (exact): 1659\n",
            "Tổng số hàng: 1700\n",
            "Số cặp input tương tự (>90%): 320\n",
            "Số cặp output tương tự (>90%): 41\n",
            "Số hàng sau khi xóa record tương tự: 1471\n",
            "Số lượng giá trị duy nhất trong 'input' (sau xử lý): 1471\n",
            "Số lượng giá trị duy nhất trong 'output' (sau xử lý): 1471\n",
            "                                                  input  \\\n",
            "998       trẻ tự kỷ có cần học trường chuyên biệt không   \n",
            "254                 trẻ nói chuyện không đúng hoàn cảnh   \n",
            "1073                      tự kỷ có bị coi là bệnh không   \n",
            "643   trẻ không biết hoàn thành công việc đúng thời hạn   \n",
            "1450                     trẻ không chỉ vật để gây chú ý   \n",
            "\n",
            "                                                 output  \n",
            "998   không phải trẻ tự kỷ nào cũng cần học trường c...  \n",
            "254   trẻ nói nội dung không phù hợp tình huống có t...  \n",
            "1073  tự kỷ không phải là bệnh mà là một rối loạn ph...  \n",
            "643   hay quên hoặc trễ deadline là thiếu tổ chức – ...  \n",
            "1450  không sử dụng hành vi chỉ vật để thu hút sự ch...  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from underthesea import word_tokenize\n",
        "from thefuzz import fuzz\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    text = \" \".join(tokens)\n",
        "    return text\n",
        "\n",
        "def extract_json_from_folder(folder_path):\n",
        "    dataset = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".json\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    json_data = json.load(f)\n",
        "                    for item in json_data:\n",
        "                        if all(k in item for k in ['input', 'output']):\n",
        "                            item['input'] = preprocess_text(item['input'])\n",
        "                            item['output'] = preprocess_text(item['output'])\n",
        "                            dataset.append(item)\n",
        "                        else:\n",
        "                            print(f\"Thiếu trường trong {filename}: {item}\")\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Không thể parse JSON từ {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Lỗi khi đọc {filename}: {e}\")\n",
        "    return dataset\n",
        "\n",
        "folder_path = \"/home/thanhnguyenvq2403/model/KLTN/data_finetune\"\n",
        "dataset = extract_json_from_folder(folder_path)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "print(\"Số lượng giá trị duy nhất trong 'input' (exact):\", df['input'].nunique())\n",
        "print(\"Số lượng giá trị duy nhất trong 'output' (exact):\", df['output'].nunique())\n",
        "print(\"Tổng số hàng:\", len(df))\n",
        "\n",
        "# Fuzzy matching để tìm các record tương tự\n",
        "similarity_threshold = 90  # Ngưỡng độ tương đồng (90%)\n",
        "input_pairs = []\n",
        "output_pairs = []\n",
        "\n",
        "# Tìm các cặp input và output tương tự\n",
        "for i in range(len(df)):\n",
        "    for j in range(i + 1, len(df)):\n",
        "        input_sim = fuzz.ratio(df['input'].iloc[i], df['input'].iloc[j])\n",
        "        if input_sim >= similarity_threshold:\n",
        "            input_pairs.append((i, j, input_sim))\n",
        "        output_sim = fuzz.ratio(df['output'].iloc[i], df['output'].iloc[j])\n",
        "        if output_sim >= similarity_threshold:\n",
        "            output_pairs.append((i, j, output_sim))\n",
        "\n",
        "print(f\"Số cặp input tương tự (>{similarity_threshold}%):\", len(input_pairs))\n",
        "print(f\"Số cặp output tương tự (>{similarity_threshold}%):\", len(output_pairs))\n",
        "\n",
        "# Loại bỏ các record có input hoặc output tương tự (giữ record đầu tiên)\n",
        "indices_to_keep = set(range(len(df)))\n",
        "for i, j, _ in input_pairs:\n",
        "    if j in indices_to_keep:\n",
        "        indices_to_keep.remove(j)\n",
        "for i, j, _ in output_pairs:\n",
        "    if j in indices_to_keep:\n",
        "        indices_to_keep.remove(j)\n",
        "\n",
        "df = df.iloc[list(indices_to_keep)].reset_index(drop=True)\n",
        "print(\"Số hàng sau khi xóa record tương tự:\", len(df))\n",
        "\n",
        "# Kiểm tra lại độ unique\n",
        "print(\"Số lượng giá trị duy nhất trong 'input' (sau xử lý):\", df['input'].nunique())\n",
        "print(\"Số lượng giá trị duy nhất trong 'output' (sau xử lý):\", df['output'].nunique())\n",
        "\n",
        "# Chia train/validation\n",
        "full_dataset = Dataset.from_pandas(df[['input', 'output']])\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(train_df[['input', 'output']])\n",
        "eval_dataset = Dataset.from_pandas(eval_df[['input', 'output']])\n",
        "print(train_df[['input', 'output']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d3d58ec",
      "metadata": {
        "id": "3d3d58ec"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "def evaluate_metrics(predictions, references):\n",
        "    # Tải các độ đo\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "    # Tính các độ đo\n",
        "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
        "    meteor_results = meteor.compute(predictions=predictions, references=references)\n",
        "\n",
        "    # Tải mô hình nhúng câu để tính Cosine Similarity\n",
        "    embedder = SentenceTransformer('BAAI/bge-m3')\n",
        "\n",
        "    # Tính embeddings cho dự đoán và tham chiếu\n",
        "    pred_embeddings = embedder.encode(predictions, convert_to_tensor=True)\n",
        "    ref_embeddings = embedder.encode(references, convert_to_tensor=True)\n",
        "\n",
        "    # Tính Cosine Similarity giữa từng cặp dự đoán-tham chiếu\n",
        "    cosine_scores = util.cos_sim(pred_embeddings, ref_embeddings)\n",
        "    \n",
        "    # Xử lý trường hợp predictions rỗng để tránh lỗi\n",
        "    if len(predictions) > 0 and cosine_scores.ndim == 2 and cosine_scores.shape[0] == len(predictions):\n",
        "        avg_cosine_similarity = np.mean([cosine_scores[i][i].item() for i in range(len(predictions))])\n",
        "    elif len(predictions) == 0:\n",
        "        avg_cosine_similarity = 0.0 # Hoặc np.nan\n",
        "        print(\"Cảnh báo: Danh sách predictions rỗng, cosine_similarity được đặt là 0.0\")\n",
        "    else:\n",
        "        # Trường hợp này ít xảy ra nếu predictions và references cùng độ dài và không rỗng\n",
        "        print(f\"Cảnh báo: Kích thước cosine_scores ({cosine_scores.shape}) không khớp với len(predictions) ({len(predictions)}).\")\n",
        "        avg_cosine_similarity = np.nan # Hoặc một giá trị mặc định\n",
        "    # Gộp kết quả\n",
        "    metrics = {\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"meteor\": meteor_results[\"meteor\"],\n",
        "        \"cosine_similarity\": avg_cosine_similarity,\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def generate_predictions(model, tokenizer, inputs, max_length=200):\n",
        "    \"\"\"Tạo dự đoán từ mô hình cho các đầu vào.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    # Đảm bảo pad_token được thiết lập\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    for input_text in inputs:\n",
        "        # Tiền xử lý input để đồng bộ với huấn luyện\n",
        "        input_text = preprocess_text(input_text)\n",
        "        prompt = f\"<s>[INST] {input_text} [/INST]\"\n",
        "        inputs_encoded = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs_encoded,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        pred = generated_text.split(\"[/INST]\")[-1].strip()\n",
        "        # Tiền xử lý dự đoán để đồng bộ với tham chiếu\n",
        "        pred = preprocess_text(pred)\n",
        "        predictions.append(pred)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7b126a89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2ae2a398a2a945d18ec5409bda99644a",
            "2d03ea85ed9e4262bfa21065ad73eae4",
            "5f8edfd1f93f4001b51bde9e815d66af",
            "f1a26a9520da4b888f8c54322ae29b82",
            "ca3c86d9cf48410199515ce86fd68eaf",
            "3fd2597c60874a4f9c920f8420b366f0",
            "cd056a6bb14048bdbc7860ce2c06303f",
            "d98a8f43fc76406897ffbb66f7e136c0",
            "b8f7022d52464b0eb74f965319229cef",
            "8983e348f59749bd86146b513c5acacf",
            "551f5413b209400fa23bedb10c4cde9f",
            "046403be22d543dab3c5d9c3d3836b9e",
            "788d9b117db34ff6b06e1fd6a942c39c",
            "916b489caeac4041839f1ee2365360a3",
            "c8d0620a47874f34b94681c71f35c3c8",
            "57e0a819a11b42e0a35b75e072b2dbb1",
            "ba961b0dab1a4ac6864646de1d46d9e6",
            "442e0cfbeb554fd58e2bf0c2d77829c0",
            "a0da77700eab4ab0bb06e1fe7b5184e9",
            "69fe2be7c9814f2ab93168f3d6ac45ae",
            "841bc431816441a4bc79dde9600f9176",
            "5b9cb618b32044518965997a96a7d930",
            "9c6f2000b243485db70d65e993621d3c",
            "c778736f27e349edb304160691210f4b",
            "076e2be9ac414d6d92b90f203da87c38",
            "0ac46097afa5445591d9a3fbad639ae5",
            "4375f815d6724658a2cfaa6c608e478f",
            "79f8dacb371b4fd987b8a4330520cc04",
            "d990e34729bf43eca69270c1fd3dc8cb",
            "2d402147806d413590c8bbbcd89646de",
            "edf820a16c044fe0a2d2f85b11b64683",
            "6f4b1cc17a48461f8e2da6dc9b03c82a",
            "4d0cb99e715847d0b05a031736ecd688",
            "b3fbfe3f3f604b698e7d869437c0d5fe",
            "246d63e4a14841c594db092689f2a64c",
            "c6909db59ff04666b867097216de70ba",
            "1ffc5523dd084452a20ae6c913d39e47",
            "35fcfd38ee344fd0924e883972074784",
            "bb370caba2b844c486d30748e6caf9e0",
            "73e04b4d775848a19e267669303d8266",
            "4526ca6058b74e11ba4329d843d6a9d4",
            "ba9f2bb92edf477d94d6cfa1e4950711",
            "9e320bdd7a7c484183ed8b93e6705af6",
            "cc5fad4a005e4aa9bcaecccb8217df1e",
            "220a0b50b8d04e3dac9bbaf89de8a7c9",
            "1201dfd96b7d4fcabf5b10b3fa769759",
            "98ae3ea018e24dabb90618b485a2b2cb",
            "85f4e94e397f43a588a527fe41bc015a",
            "686120f3641f43298071d6a5e32fe644",
            "efc1ff78f5f546638b298419a4151695",
            "0926abb88e2e4381b4b38839e6f652f5",
            "58850dfce0cc4811ba06ab1ea71d306d",
            "634e691c8c8a4fb68c5af7704b9ce0fe",
            "5c5ac7432fae43f2a17870b83e19a17a",
            "d224187c0f7340279949cddca91d3ecb",
            "079359b31bb04296a232a84fc1b7db2d",
            "ff5faacf70a04f19833d41589b7a78e5",
            "e0d2e5d548e04e2ebcf6b5a79083f0dc",
            "7ac0a7531ecc4823b33549eeff34a131",
            "970af49c4f1940758130a5e334901a14",
            "83a4b7f740ec41389a6df6317af60615",
            "d43bfd1e65ae43af97b7fa6b44cf9023",
            "e2283973a0de470a88d5bb8b78160982",
            "65597da15fda4b51b7aa859b380c12db",
            "6d044e9840e449c5b1e718fb06e99515",
            "d00e51ae50d144e392bf12aa8b54b303",
            "4adcb8502fce4dd1af5b8a5b648fa263",
            "9b47975658384af1a549c9f1acf1e5a5",
            "56eff52cba7c4e06a6378805f2aa2707",
            "4a251249b63e4688ba357a8d412a3b69",
            "55fdfa966042462bb35ee44cfbfb841e",
            "23b95f393dd940039de764bd67d7709a",
            "c68779b0088e4f1c867ef1be26e7112b",
            "9ea9a076bce947bab5096a028f7b3237",
            "c034029a3d5e48ebaf7ea25fa7cb96fd",
            "c1267d7977354e5aa3db8a28f511f2b8",
            "9e526d55e8f84c66ab040536bc502d2b",
            "90fadb3fb3e14b1baaca9e77f571a80e",
            "e2e6f823b0224c52a44e1e798e5c1985",
            "4eaab0584d164139b520cd15eaa3793f",
            "943be389e1a947ad99ec7d4d76de6db0",
            "2c01848e500843e39f326eeb3b0a8cce",
            "8cd9559471a9418d8a134b43c6dc1258",
            "dc6578ccbf9740bfb315a07b6482d8bf",
            "187ce14699ae454c819ceacc050a955f",
            "a08c13a1cfcb4fd295994b99f788d6c3",
            "a81481e6777b4ab78442363553b73381",
            "ad666ee0fe7e48de9c4e934212bd337c",
            "cbbcc5aca91d46ba9b51859494da828b",
            "494b39f1fcf9434b88559980e35c006f",
            "f849b4139b5d4ab3a2ae32ae83807160",
            "1807262027f34e70b1e94883ac402b39",
            "c3d35975670546ab9998a6389e3ec707",
            "6a974c0a2ec043219c538076bfffc87a",
            "79f22a64d0fa4310b9ee63bbfb66b41a",
            "8fcd62139ad44297b2a950d97850468f",
            "a7ee09b72b034234b02194ab94471a2a",
            "a66367d4522a4b1eb6ce7b88394f8e30",
            "a0041fc725964c50a3beff504ddeb9ab",
            "e2627486d9ba4848830b0687cbe8b3d3",
            "55b1ad5ece2c4be18a2acdb322b93c01",
            "e502fdc27ded43fcbbc4d3fe3fac8030",
            "3d7b45cd5bfa4ad9ba035ac29882fb8b",
            "c8ad9a852eec438e972cadc8ea1f0fe2",
            "5fc18b5fb2e349afa2489a0adf1b21fc",
            "2bd1218e781f4784be96419b838c45af",
            "9419d62d8de24bce9697c0f60467ea99",
            "43b4ce75564943ca8b250e54a7ce1834",
            "80acce4d1d5c4459ab794ba6bbcc32cf",
            "fa04a7d89df34dfcbd73d62f4dda799c"
          ]
        },
        "id": "7b126a89",
        "outputId": "b0106b8a-df18-45c5-a8b4-bae598edea1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Bắt đầu chạy đơn ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Applying formatting function to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 14963.16 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1176/1176 [00:00<00:00, 61320.54 examples/s]\n",
            "Adding EOS to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 39026.35 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 8909.38 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 507667.92 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 14285.28 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 52687.77 examples/s]\n",
            "Adding EOS to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 35267.35 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 8808.61 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 253393.34 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,176\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 1176, Eval dataset size: 295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [180/219 1:16:13 < 16:42, 0.04 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.001600</td>\n",
              "      <td>3.725602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.237200</td>\n",
              "      <td>2.575135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.287800</td>\n",
              "      <td>2.068729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.095800</td>\n",
              "      <td>1.946580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.928100</td>\n",
              "      <td>1.796626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.737500</td>\n",
              "      <td>1.741616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.687600</td>\n",
              "      <td>1.641230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.469600</td>\n",
              "      <td>1.587687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.413100</td>\n",
              "      <td>1.507579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.293800</td>\n",
              "      <td>1.449912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.293100</td>\n",
              "      <td>1.443382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.234900</td>\n",
              "      <td>1.387826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.248700</td>\n",
              "      <td>1.366052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.196300</td>\n",
              "      <td>1.356827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.150500</td>\n",
              "      <td>1.328845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.016000</td>\n",
              "      <td>1.330598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.980700</td>\n",
              "      <td>1.342360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.992900</td>\n",
              "      <td>1.333192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_single_Vistral-7B-Chat/checkpoint-100\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825b3ac-6e092298517b8ad366680206;5d00ef6c-b95c-4e93-b058-f526e9b22ec3)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_single_Vistral-7B-Chat/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_single_Vistral-7B-Chat/checkpoint-100/special_tokens_map.json\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_single_Vistral-7B-Chat/checkpoint-100 (score: 1.3288447856903076).\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825bbc8-61a70d0d4252c2df60bd234e;c86e2c18-97ab-4464-95fa-252c760b78df)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_Vistral-7B-Chat_single/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_Vistral-7B-Chat_single/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 8194,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
            "\n",
            "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
            "loading file sentencepiece.bpe.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Safetensors PR exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Single Run Metrics for Viet-Mistral/Vistral-7B-Chat: {'rouge1': np.float64(0.6409198006255284), 'rouge2': np.float64(0.32474052246808505), 'rougeL': np.float64(0.43748978823827767), 'meteor': np.float64(0.3567473610495855), 'cosine_similarity': np.float64(0.7992728334362224)}\n",
            "Đã lưu metrics chạy đơn vào: single_run_metrics_Vistral-7B-Chat.json\n",
            "Thời gian chạy đơn: 6998.68 giây\n",
            "Input: trẻ không biết liên hệ với người lớn khi cần hỗ trợ hoạt động\n",
            "Prediction: trẻ không biết nhờ cô giáo hoặc người lớn khi gặp khó khăn là thiếu kỹ năng tìm kiếm hỗ trợ – nên luyện mẫu câu như cô ơi con chưa hiểu phần này\n",
            "Reference: tự làm hết mà không hiệu quả – nên luyện viết thư gọi điện hoặc nhờ giáo viên liên hệ hỗ trợ đúng người\n",
            "\n",
            "Input: trẻ không có giao tiếp mắt\n",
            "Prediction: trẻ không nhìn vào người khác khi nói chuyện có thể là dấu hiệu của rối loạn phổ tự kỷ hoặc rối loạn phát triển\n",
            "Reference: giao tiếp bằng mắt là một dấu hiệu quan trọng của sự phát triển xã hội nếu trẻ ít hoặc không có giao tiếp mắt nên đánh giá nguy cơ rối loạn phổ tự kỷ hoặc chậm phát triển giao tiếp\n",
            "\n",
            "Input: người tự kỷ có nên sống tập thể không\n",
            "Prediction: người tự kỷ có thể sống tập thể nếu được hỗ trợ phù hợp về môi trường xã hội và kỹ năng sống nhóm tuy nhiên cần đánh giá kỹ từng trường hợp cụ thể để đảm bảo an toàn và hòa nhập\n",
            "Reference: tùy từng cá nhân người tự kỷ chức năng cao có thể sống tập thể nếu môi trường bao dung có hướng dẫn rõ ràng và hỗ trợ xã hội người có khó khăn nhiều hơn nên được hỗ trợ tại cộng đồng nhỏ hoặc gia đình\n",
            "\n",
            "Input: trẻ không bắt chước hành động\n",
            "Prediction: trẻ không bắt chước hành động có thể là dấu hiệu chậm phát triển kỹ năng xã hội hoặc rối loạn phổ tự kỷ\n",
            "Reference: thiếu bắt chước là dấu hiệu cảnh báo rối loạn phát triển trẻ nên có khả năng bắt chước cử chỉ hành động đơn giản từ 12 18 tháng cần đánh giá nếu trẻ không có kỹ năng này\n",
            "\n",
            "Input: cảm ơn bạn nhiều nhé\n",
            "Prediction: không có gì bạn cứ chia sẻ nếu cần hỗ trợ thêm\n",
            "Reference: rất vui được hỗ trợ bạn nếu cần thêm thông tin cứ nhắn mình bất kỳ lúc nào\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "48933"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import gc\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Loại bỏ cột không cần thiết để tránh cảnh báo\n",
        "train_dataset = train_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_dataset.column_names else [])\n",
        "eval_dataset = eval_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_dataset.column_names else [])\n",
        "\n",
        "model_name = \"Viet-Mistral/Vistral-7B-Chat\"\n",
        "\n",
        "# Đo thời gian chạy đơn\n",
        "print(\"\\n=== Bắt đầu chạy đơn ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Cấu hình huấn luyện cho Single Run\n",
        "training_arguments_single = TrainingArguments(\n",
        "    output_dir=f\"./results_single_{model_name.split('/')[-1]}\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.1,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.1,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    log_level=\"info\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Huấn luyện Single Run\n",
        "trainer_single = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments_single,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_func,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.01,\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "print(f\"Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}\")\n",
        "trainer_single.train()\n",
        "\n",
        "# Lưu mô hình Single Run\n",
        "model.save_pretrained(f\"./finetuned_{model_name.split('/')[-1]}_single\")\n",
        "tokenizer.save_pretrained(f\"./finetuned_{model_name.split('/')[-1]}_single\")\n",
        "\n",
        "# Đánh giá Single Run\n",
        "test_inputs = eval_df['input'].tolist()\n",
        "test_references = eval_df['output'].tolist()\n",
        "predictions_single = generate_predictions(model, tokenizer, test_inputs)\n",
        "metrics_single = evaluate_metrics(predictions_single, test_references)\n",
        "print(f\"Single Run Metrics for {model_name}:\", metrics_single)\n",
        "\n",
        "# Lưu metrics vào file\n",
        "metrics_file = f\"single_run_metrics_{model_name.split('/')[-1]}.json\"\n",
        "try:\n",
        "    with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics_single, f, indent=4, ensure_ascii=False)\n",
        "    print(f\"Đã lưu metrics chạy đơn vào: {metrics_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi lưu metrics chạy đơn: {e}\")\n",
        "\n",
        "# In thời gian chạy\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "print(f\"Thời gian chạy đơn: {duration:.2f} giây\")\n",
        "\n",
        "# Kiểm tra mẫu dự đoán\n",
        "for i in range(5):\n",
        "    print(f\"Input: {test_inputs[i]}\")\n",
        "    print(f\"Prediction: {predictions_single[i]}\")\n",
        "    print(f\"Reference: {test_references[i]}\\n\")\n",
        "del model, trainer_single\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ef0e7636",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17299"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18c38bce",
      "metadata": {
        "id": "18c38bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/config.json\n",
            "Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 38369\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/model.safetensors.index.json\n",
            "Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.24s/it]\n",
            "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "All the weights of MistralForCausalLM were initialized from the model checkpoint at Viet-Mistral/Vistral-7B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(38369, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 - Train size: 1176, Eval size: 295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 15853.40 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1176/1176 [00:00<00:00, 50817.53 examples/s]\n",
            "Adding EOS to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 41141.21 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 8623.30 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 463058.72 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 15967.68 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 47785.88 examples/s]\n",
            "Adding EOS to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 38509.79 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 8003.93 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 184454.34 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,176\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [170/219 09:39 < 02:49, 0.29 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.052900</td>\n",
              "      <td>3.548330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.257500</td>\n",
              "      <td>2.602690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.268300</td>\n",
              "      <td>2.068326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.033100</td>\n",
              "      <td>1.919198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.874100</td>\n",
              "      <td>1.760837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.714000</td>\n",
              "      <td>1.724056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.649800</td>\n",
              "      <td>1.598928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.441700</td>\n",
              "      <td>1.607735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.388800</td>\n",
              "      <td>1.525305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.262500</td>\n",
              "      <td>1.510203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.342000</td>\n",
              "      <td>1.421071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.234300</td>\n",
              "      <td>1.401683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.266100</td>\n",
              "      <td>1.350429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.171800</td>\n",
              "      <td>1.321016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.148300</td>\n",
              "      <td>1.314417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.984900</td>\n",
              "      <td>1.337606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.974900</td>\n",
              "      <td>1.343389</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_Vistral-7B-Chatfold_1/checkpoint-100\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825d804-4d8aa39f4dd278685743b91d;53461ca7-9f39-44f2-bd5f-08eddd4d3005)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_Vistral-7B-Chatfold_1/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_Vistral-7B-Chatfold_1/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_Vistral-7B-Chatfold_1/checkpoint-100 (score: 1.314416766166687).\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825d8f3-372cd40f7915e2f0252170b8;afefbd33-f88f-43ff-b73f-2fab52c1317e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_1/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_1/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 8194,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
            "\n",
            "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
            "Safetensors PR exists\n",
            "loading file sentencepiece.bpe.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Metrics: {'rouge1': np.float64(0.6388628518218955), 'rouge2': np.float64(0.30762221948409285), 'rougeL': np.float64(0.42255159322495506), 'meteor': np.float64(0.35230586075016956), 'cosine_similarity': np.float64(0.7959634940503008)}\n",
            "Thời gian chạy Fold 1: 2220.55 giây\n",
            "\n",
            "Training Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/config.json\n",
            "Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 38369\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/model.safetensors.index.json\n",
            "Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.64s/it]\n",
            "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "All the weights of MistralForCausalLM were initialized from the model checkpoint at Viet-Mistral/Vistral-7B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(38369, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 - Train size: 1177, Eval size: 294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 16266.47 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 62944.78 examples/s]\n",
            "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 49149.23 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 9044.52 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 470744.33 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 15676.45 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 51727.23 examples/s]\n",
            "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 39661.81 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 8726.14 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 232052.20 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,177\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [180/219 10:16 < 02:15, 0.29 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.003900</td>\n",
              "      <td>3.663294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.253700</td>\n",
              "      <td>2.645182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.292400</td>\n",
              "      <td>2.107560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.093800</td>\n",
              "      <td>1.941300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.859700</td>\n",
              "      <td>1.792064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.741700</td>\n",
              "      <td>1.735420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.764000</td>\n",
              "      <td>1.631298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.682800</td>\n",
              "      <td>1.633458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.404900</td>\n",
              "      <td>1.557241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.374200</td>\n",
              "      <td>1.514990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.317200</td>\n",
              "      <td>1.476365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.298200</td>\n",
              "      <td>1.425632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.227800</td>\n",
              "      <td>1.386910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.203900</td>\n",
              "      <td>1.355263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.261100</td>\n",
              "      <td>1.331684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.045200</td>\n",
              "      <td>1.346881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.989900</td>\n",
              "      <td>1.369853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.001400</td>\n",
              "      <td>1.360986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_Vistral-7B-Chatfold_2/checkpoint-100\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825e0b2-245cd8a405a2ead86b6c1647;266b3e9a-0548-424e-ac94-63205a0507cc)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_Vistral-7B-Chatfold_2/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_Vistral-7B-Chatfold_2/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_Vistral-7B-Chatfold_2/checkpoint-100 (score: 1.331683874130249).\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825e1c7-2241e84a50f4da037725e2b9;c69f63de-1921-49f2-b61c-d13e0bdc18e1)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_2/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_2/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 8194,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
            "\n",
            "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
            "loading file sentencepiece.bpe.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Safetensors PR exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Metrics: {'rouge1': np.float64(0.6367582884338493), 'rouge2': np.float64(0.3181013289168212), 'rougeL': np.float64(0.42950613509864766), 'meteor': np.float64(0.3538016252219771), 'cosine_similarity': np.float64(0.7999923233272267)}\n",
            "Thời gian chạy Fold 2: 2197.71 giây\n",
            "\n",
            "Training Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/config.json\n",
            "Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 38369\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/model.safetensors.index.json\n",
            "Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.26s/it]\n",
            "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "All the weights of MistralForCausalLM were initialized from the model checkpoint at Viet-Mistral/Vistral-7B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(38369, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 - Train size: 1177, Eval size: 294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 14281.98 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 49804.24 examples/s]\n",
            "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 40176.57 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 8135.94 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 620733.79 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 13472.36 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 45656.09 examples/s]\n",
            "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 36043.65 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 7901.71 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 208017.10 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,177\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [180/219 10:34 < 02:18, 0.28 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.013600</td>\n",
              "      <td>3.546726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.248000</td>\n",
              "      <td>2.578102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.284400</td>\n",
              "      <td>2.131077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.076100</td>\n",
              "      <td>1.975454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.877800</td>\n",
              "      <td>1.844475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.699600</td>\n",
              "      <td>1.771116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.638500</td>\n",
              "      <td>1.651551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.573100</td>\n",
              "      <td>1.639525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.380100</td>\n",
              "      <td>1.542265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.302800</td>\n",
              "      <td>1.537493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.276900</td>\n",
              "      <td>1.507904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.226400</td>\n",
              "      <td>1.442427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.221600</td>\n",
              "      <td>1.419058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.165400</td>\n",
              "      <td>1.384094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.271200</td>\n",
              "      <td>1.368582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.993300</td>\n",
              "      <td>1.386151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.966400</td>\n",
              "      <td>1.404340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.000300</td>\n",
              "      <td>1.395716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_Vistral-7B-Chatfold_3/checkpoint-100\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825e951-73d4586b540aa25b16998217;44d938cb-c579-42cb-b226-776382dc2f97)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_Vistral-7B-Chatfold_3/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_Vistral-7B-Chatfold_3/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_Vistral-7B-Chatfold_3/checkpoint-100 (score: 1.3685821294784546).\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825ea6c-12b0b1df0ca89bd62647ad68;03f2dd28-8e9d-4cf8-977e-69037fedd8ba)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_3/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_3/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 8194,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
            "\n",
            "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
            "Safetensors PR exists\n",
            "loading file sentencepiece.bpe.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Metrics: {'rouge1': np.float64(0.6454578240278577), 'rouge2': np.float64(0.3188491578713486), 'rougeL': np.float64(0.4273567540240792), 'meteor': np.float64(0.3586067768206585), 'cosine_similarity': np.float64(0.7995221779054525)}\n",
            "Thời gian chạy Fold 3: 2310.81 giây\n",
            "\n",
            "Training Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/config.json\n",
            "Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 38369\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/model.safetensors.index.json\n",
            "Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.27s/it]\n",
            "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "All the weights of MistralForCausalLM were initialized from the model checkpoint at Viet-Mistral/Vistral-7B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(38369, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 - Train size: 1177, Eval size: 294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 14838.06 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 52396.53 examples/s]\n",
            "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 39374.18 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 7956.43 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 605432.40 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 13276.97 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 39280.27 examples/s]\n",
            "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 34213.57 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 6860.64 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 175508.88 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,177\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [180/219 10:41 < 02:20, 0.28 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.969100</td>\n",
              "      <td>3.682665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.305500</td>\n",
              "      <td>2.679393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.287800</td>\n",
              "      <td>2.100338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.038900</td>\n",
              "      <td>1.930973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.900400</td>\n",
              "      <td>1.784392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.731800</td>\n",
              "      <td>1.731735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.650600</td>\n",
              "      <td>1.618359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.632600</td>\n",
              "      <td>1.578088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.417400</td>\n",
              "      <td>1.508415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.331000</td>\n",
              "      <td>1.477577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.302000</td>\n",
              "      <td>1.488923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.266400</td>\n",
              "      <td>1.413946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.212400</td>\n",
              "      <td>1.408464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.187600</td>\n",
              "      <td>1.373526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.210900</td>\n",
              "      <td>1.341626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.002800</td>\n",
              "      <td>1.360528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.958300</td>\n",
              "      <td>1.377494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.982800</td>\n",
              "      <td>1.363647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_Vistral-7B-Chatfold_4/checkpoint-100\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825f25d-79ad07a26793a15b1c83290f;10db8bb4-d270-4764-bda6-a19d3cc7e599)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_Vistral-7B-Chatfold_4/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_Vistral-7B-Chatfold_4/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_Vistral-7B-Chatfold_4/checkpoint-100 (score: 1.3416255712509155).\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825f37b-7164980b6c5033946658bccc;4db83218-719c-4b4e-8063-e52c66764442)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_4/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_4/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 8194,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
            "\n",
            "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
            "loading file sentencepiece.bpe.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Safetensors PR exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Metrics: {'rouge1': np.float64(0.6429769766389173), 'rouge2': np.float64(0.32457812602579594), 'rougeL': np.float64(0.43064962619100455), 'meteor': np.float64(0.35790174550434856), 'cosine_similarity': np.float64(0.7966724995042191)}\n",
            "Thời gian chạy Fold 4: 2220.53 giây\n",
            "\n",
            "Training Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/config.json\n",
            "Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 38369\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/model.safetensors.index.json\n",
            "Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.08s/it]\n",
            "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "All the weights of MistralForCausalLM were initialized from the model checkpoint at Viet-Mistral/Vistral-7B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(38369, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 - Train size: 1177, Eval size: 294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file tokenizer.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer.json\n",
            "loading file added_tokens.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--Viet-Mistral--Vistral-7B-Chat/snapshots/d331b64e61b935cc43c2b3010ae9fb4fde599b45/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 14116.58 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 51954.83 examples/s]\n",
            "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 34926.57 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 8041.66 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 687945.35 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 13653.15 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 42304.21 examples/s]\n",
            "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 30369.55 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 7496.89 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 192705.95 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,177\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [170/219 10:01 < 02:55, 0.28 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.048300</td>\n",
              "      <td>3.675917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.256400</td>\n",
              "      <td>2.612996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.309700</td>\n",
              "      <td>2.164761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.088500</td>\n",
              "      <td>1.970531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.939200</td>\n",
              "      <td>1.821433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.708000</td>\n",
              "      <td>1.716155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.662100</td>\n",
              "      <td>1.633589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.664900</td>\n",
              "      <td>1.594562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.395300</td>\n",
              "      <td>1.567374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.388800</td>\n",
              "      <td>1.524768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.277800</td>\n",
              "      <td>1.476997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.250600</td>\n",
              "      <td>1.420824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.230600</td>\n",
              "      <td>1.402987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.192000</td>\n",
              "      <td>1.357471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.250700</td>\n",
              "      <td>1.356660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.959400</td>\n",
              "      <td>1.384271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.959200</td>\n",
              "      <td>1.412592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_Vistral-7B-Chatfold_5/checkpoint-100\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825fb07-3b5d480d08d1075d2ddaaae4;2dea0dc0-8001-4aec-a228-f5c9d504355d)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_Vistral-7B-Chatfold_5/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_Vistral-7B-Chatfold_5/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: output, input, text. If output, input, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_Vistral-7B-Chatfold_5/checkpoint-100 (score: 1.3566595315933228).\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6825fc00-3b283e5606face2b56cb2c26;91988683-2531-4fc7-9e50-852c7e0b2256)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/Viet-Mistral/Vistral-7B-Chat/resolve/main/config.json.\n",
            "Access to model Viet-Mistral/Vistral-7B-Chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in Viet-Mistral/Vistral-7B-Chat.\n",
            "  warnings.warn(\n",
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Viet-Mistral/Vistral-7B-Chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_5/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_5/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/thanhnguyenvq2403/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 8194,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "Safetensors PR exists\n",
            "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
            "\n",
            "All the weights of XLMRobertaModel were initialized from the model checkpoint at BAAI/bge-m3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
            "loading file sentencepiece.bpe.model from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/thanhnguyenvq2403/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Metrics: {'rouge1': np.float64(0.6354878517582643), 'rouge2': np.float64(0.3096272185338643), 'rougeL': np.float64(0.41933638699349385), 'meteor': np.float64(0.35169169794992045), 'cosine_similarity': np.float64(0.7936978731431118)}\n",
            "Thời gian chạy Fold 5: 2442.62 giây\n",
            "\n",
            "=== Kết thúc huấn luyện K-Fold ===\n",
            "Tổng thời gian chạy: 11394.03 giây\n",
            "Thời gian trung bình mỗi fold: 2278.44 giây\n",
            "\n",
            "Average Cross-Validation Metrics: {'rouge1': np.float64(0.6399087585361569), 'rouge2': np.float64(0.3157556101663846), 'rougeL': np.float64(0.42588009910643604), 'meteor': np.float64(0.3548615412494148), 'cosine_similarity': np.float64(0.7971696735860623)}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Cấu hình KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_metrics = []\n",
        "fold_models = []\n",
        "fold_times = []\n",
        "total_start_time = time.time()\n",
        "\n",
        "# Lặp qua từng fold\n",
        "for fold, (train_idx, eval_idx) in enumerate(kf.split(df)):\n",
        "    print(f\"\\nTraining Fold {fold + 1}...\")\n",
        "    fold_start_time = time.time()\n",
        "\n",
        "    # Tạo tập train và eval cho fold hiện tại\n",
        "    train_fold = df.iloc[train_idx][['input', 'output']]\n",
        "    eval_fold = df.iloc[eval_idx][['input', 'output']]\n",
        "    train_fold_dataset = Dataset.from_pandas(train_fold)\n",
        "    eval_fold_dataset = Dataset.from_pandas(eval_fold)\n",
        "\n",
        "    # Loại bỏ cột không cần thiết\n",
        "    train_fold_dataset = train_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_fold_dataset.column_names else [])\n",
        "    eval_fold_dataset = eval_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_fold_dataset.column_names else [])\n",
        "\n",
        "    # Tải lại mô hình gốc với INT4 quantization\n",
        "    model, tokenizer, peft_config = load_model_and_tokenizer(quantization=\"int4\")\n",
        "    print(f\"Fold {fold + 1} - Train size: {len(train_fold_dataset)}, Eval size: {len(eval_fold_dataset)}\")\n",
        "\n",
        "    # Cấu hình huấn luyện cho fold\n",
        "    training_arguments_fold = TrainingArguments(\n",
        "        output_dir=f\"./results_{model_name.split('/')[-1]}fold_{fold + 1}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        save_steps=100,\n",
        "        logging_steps=10,\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.1,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        max_grad_norm=0.3,\n",
        "        warmup_ratio=0.1,\n",
        "        group_by_length=True,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        logging_strategy=\"steps\",\n",
        "        log_level=\"info\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "    )\n",
        "\n",
        "    # Huấn luyện fold\n",
        "    trainer_fold = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_arguments_fold,\n",
        "        train_dataset=train_fold_dataset,\n",
        "        eval_dataset=eval_fold_dataset,\n",
        "        peft_config=peft_config,\n",
        "        formatting_func=formatting_func,\n",
        "        callbacks=[\n",
        "            EarlyStoppingCallback(\n",
        "                early_stopping_patience=3,\n",
        "                early_stopping_threshold=0.01,\n",
        "            )\n",
        "        ],\n",
        "    )\n",
        "    trainer_fold.train()\n",
        "\n",
        "    # Lưu mô hình fold\n",
        "    fold_path = f\"./finetuned_vistral_fold_{fold + 1}\"\n",
        "#pragma warning disable format\n",
        "    model.save_pretrained(fold_path)\n",
        "    tokenizer.save_pretrained(fold_path)\n",
        "    fold_models.append(fold_path)\n",
        "\n",
        "    # Đánh giá fold\n",
        "    test_inputs_fold = eval_fold['input'].tolist()\n",
        "    test_references_fold = eval_fold['output'].tolist()\n",
        "    predictions_fold = generate_predictions(model, tokenizer, test_inputs_fold)\n",
        "    metrics_fold = evaluate_metrics(predictions_fold, test_references_fold)\n",
        "    print(f\"Fold {fold + 1} Metrics:\", metrics_fold)\n",
        "    fold_metrics.append(metrics_fold)\n",
        "\n",
        "    # Lưu metrics của fold\n",
        "    with open(f\"fold_{fold + 1}_metrics_vistral.json\", \"w\") as f:\n",
        "        json.dump(metrics_fold, f, indent=4)\n",
        "\n",
        "    # Đo thời gian chạy của fold\n",
        "    fold_end_time = time.time()\n",
        "    fold_duration = fold_end_time - fold_start_time\n",
        "    fold_times.append(fold_duration)\n",
        "    print(f\"Thời gian chạy Fold {fold + 1}: {fold_duration:.2f} giây\")\n",
        "\n",
        "    # Dọn dẹp bộ nhớ\n",
        "    del model, trainer_fold\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Tính tổng thời gian và thời gian trung bình\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "print(f\"\\n=== Kết thúc huấn luyện K-Fold ===\")\n",
        "print(f\"Tổng thời gian chạy: {total_duration:.2f} giây\")\n",
        "print(f\"Thời gian trung bình mỗi fold: {np.mean(fold_times):.2f} giây\")\n",
        "\n",
        "# Tính trung bình metrics qua các fold\n",
        "avg_metrics = {\n",
        "    \"rouge1\": np.mean([m[\"rouge1\"] for m in fold_metrics]),\n",
        "    \"rouge2\": np.mean([m[\"rouge2\"] for m in fold_metrics]),\n",
        "    \"rougeL\": np.mean([m[\"rougeL\"] for m in fold_metrics]),\n",
        "    \"meteor\": np.mean([m[\"meteor\"] for m in fold_metrics]),\n",
        "    \"cosine_similarity\": np.mean([m[\"cosine_similarity\"] for m in fold_metrics]),\n",
        "}\n",
        "print(\"\\nAverage Cross-Validation Metrics:\", avg_metrics)\n",
        "\n",
        "# Lưu metrics trung bình\n",
        "with open(\"cross_validation_metrics_vistral.json\", \"w\") as f:\n",
        "    json.dump(avg_metrics, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6b58f93a",
      "metadata": {
        "id": "6b58f93a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/thanhnguyenvq2403/model/KLTN/venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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\n",
            "Đang tìm fold tốt nhất dựa trên metric: 'cosine_similarity' (cao hơn là tốt hơn: True)\n",
            "Fold 1: 'cosine_similarity' = 0.7960\n",
            "Fold 2: 'cosine_similarity' = 0.8000\n",
            "Fold 3: 'cosine_similarity' = 0.7995\n",
            "Fold 4: 'cosine_similarity' = 0.7967\n",
            "Fold 5: 'cosine_similarity' = 0.7937\n",
            "\n",
            "=> Fold tốt nhất được chọn: Fold 2 với cosine_similarity = 0.8000\n",
            "\n",
            "--- Bắt đầu quá trình merge model cho Fold 2 ---\n",
            "Đã giải phóng bộ nhớ GPU (nếu có).\n",
            "Đang tải tokenizer cho model: Viet-Mistral/Vistral-7B-Chat...\n",
            "Tokenizer đã được tải.\n",
            "Đang tải base model 'Viet-Mistral/Vistral-7B-Chat' ở định dạng BF16 (không lượng tử hóa BitsAndBytes)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model đã được tải thành công ở định dạng BF16.\n",
            "Đang tải adapter LoRA từ: /home/thanhnguyenvq2403/model/KLTN/finetuned_vistral_fold_2\n",
            "Adapter LoRA đã được tải.\n",
            "Đang hợp nhất adapter LoRA vào base model (BF16)...\n",
            "Hợp nhất adapter thành công. Mô hình đã hợp nhất ở định dạng BF16.\n",
            "Đang lưu mô hình đã hợp nhất vào: /home/thanhnguyenvq2403/model/KLTN/merged_dama_2_fold_2_bf16\n",
            "Mô hình đã hợp nhất và tokenizer đã được lưu vào: /home/thanhnguyenvq2403/model/KLTN/merged_dama_2_fold_2_bf16\n",
            "Đã dọn dẹp bộ nhớ.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "from dotenv import load_dotenv\n",
        "# Tải biến môi trường từ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# Đọc access token từ biến môi trường\n",
        "hf_token = os.getenv(\"HF_VISTRAL\")\n",
        "print(hf_token)\n",
        "\n",
        "# --- Phần 1: Tìm fold có metric tốt nhất ---\n",
        "\n",
        "def load_metrics_from_file(file_path):\n",
        "    \"\"\"Đọc metrics từ file JSON.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Cảnh báo: Không tìm thấy file metric: {file_path}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Cảnh báo: File metric không phải JSON hợp lệ: {file_path}\")\n",
        "        return None\n",
        "\n",
        "def find_best_fold_by_metric(metrics_base_path, num_folds, metric_name_to_optimize, higher_is_better=True):\n",
        "    \"\"\"\n",
        "    Tìm fold có giá trị metric được chỉ định cao nhất (hoặc thấp nhất).\n",
        "\n",
        "    Args:\n",
        "        metrics_base_path (str): Đường dẫn cơ sở đến thư mục chứa các file metrics.\n",
        "                                 Ví dụ: \"./\" nếu các file ở thư mục hiện tại.\n",
        "        num_folds (int): Tổng số lượng folds.\n",
        "        metric_name_to_optimize (str): Tên của metric dùng để so sánh (ví dụ: 'rougeL', 'cosine_similarity').\n",
        "        higher_is_better (bool): True nếu giá trị metric cao hơn là tốt hơn, False nếu thấp hơn là tốt hơn.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_fold_number, best_metric_value, all_metrics_of_best_fold) hoặc (None, None, None) nếu lỗi.\n",
        "    \"\"\"\n",
        "    best_fold_so_far = None\n",
        "    best_metric_val = -float('inf') if higher_is_better else float('inf')\n",
        "    all_metrics_best_fold = None\n",
        "\n",
        "    print(f\"Đang tìm fold tốt nhất dựa trên metric: '{metric_name_to_optimize}' (cao hơn là tốt hơn: {higher_is_better})\")\n",
        "\n",
        "    for i in range(1, num_folds + 1):\n",
        "        # Giả sử tên file là \"fold_X_metrics\" hoặc \"fold_X_metrics.json\"\n",
        "        # Sửa lại mẫu tên file nếu cần\n",
        "        metric_file_candidate_1 = os.path.join(metrics_base_path, f\"fold_{i}_metrics_vistral.json\")\n",
        "        metric_file_candidate_2 = os.path.join(metrics_base_path, f\"fold_{i}_metrics_vistral\") # Không có đuôi .json\n",
        "\n",
        "        metrics_data = None\n",
        "        if os.path.exists(metric_file_candidate_1):\n",
        "            metrics_data = load_metrics_from_file(metric_file_candidate_1)\n",
        "        elif os.path.exists(metric_file_candidate_2):\n",
        "            metrics_data = load_metrics_from_file(metric_file_candidate_2)\n",
        "        else:\n",
        "            print(f\"Không tìm thấy file metric cho fold {i} tại: {metric_file_candidate_1} hoặc {metric_file_candidate_2}\")\n",
        "            continue\n",
        "\n",
        "        if metrics_data is None:\n",
        "            continue # Bỏ qua nếu không đọc được file\n",
        "\n",
        "        if metric_name_to_optimize not in metrics_data:\n",
        "            print(f\"Cảnh báo: Metric '{metric_name_to_optimize}' không có trong file của fold {i}. Bỏ qua fold này để so sánh.\")\n",
        "            continue\n",
        "\n",
        "        current_metric_val = metrics_data[metric_name_to_optimize]\n",
        "        print(f\"Fold {i}: '{metric_name_to_optimize}' = {current_metric_val:.4f}\")\n",
        "\n",
        "        if higher_is_better:\n",
        "            if current_metric_val > best_metric_val:\n",
        "                best_metric_val = current_metric_val\n",
        "                best_fold_so_far = i\n",
        "                all_metrics_best_fold = metrics_data\n",
        "        else: # lower is better\n",
        "            if current_metric_val < best_metric_val:\n",
        "                best_metric_val = current_metric_val\n",
        "                best_fold_so_far = i\n",
        "                all_metrics_best_fold = metrics_data\n",
        "\n",
        "    if best_fold_so_far is not None:\n",
        "        print(f\"\\n=> Fold tốt nhất được chọn: Fold {best_fold_so_far} với {metric_name_to_optimize} = {best_metric_val:.4f}\")\n",
        "        # print(f\"Toàn bộ metrics của fold {best_fold_so_far}: {all_metrics_best_fold}\")\n",
        "        return best_fold_so_far, best_metric_val, all_metrics_best_fold\n",
        "    else:\n",
        "        print(f\"\\n=> Không thể xác định fold tốt nhất dựa trên metric '{metric_name_to_optimize}'.\")\n",
        "        return None, None, None\n",
        "\n",
        "# --- Cấu hình cho việc tìm fold ---\n",
        "# Đặt đường dẫn đến thư mục chứa các file fold_X_metrics của bạn\n",
        "# Ví dụ: nếu các file fold_1_metrics, fold_2_metrics,... nằm cùng thư mục với script này:\n",
        "METRICS_FILES_DIRECTORY = \"./\" \n",
        "NUM_TOTAL_FOLDS = 5\n",
        "# Chọn metric bạn muốn sử dụng để quyết định fold nào tốt nhất\n",
        "# Ví dụ: 'rougeL', 'cosine_similarity', 'meteor', 'rouge1', 'rouge2'\n",
        "METRIC_TO_OPTIMIZE_FOR = \"cosine_similarity\" # THAY ĐỔI TÊN METRIC NÀY NẾU CẦN\n",
        "\n",
        "best_fold_id, _, _ = find_best_fold_by_metric(\n",
        "    METRICS_FILES_DIRECTORY,\n",
        "    NUM_TOTAL_FOLDS,\n",
        "    METRIC_TO_OPTIMIZE_FOR,\n",
        "    higher_is_better=True # Hầu hết các metric này, cao hơn là tốt hơn\n",
        ")\n",
        "\n",
        "if best_fold_id is None:\n",
        "    print(\"Không thể xác định fold tốt nhất. Sẽ sử dụng một fold mặc định hoặc dừng chương trình.\")\n",
        "    # Gán một fold mặc định nếu muốn tiếp tục\n",
        "    default_fold_if_not_found = 4 # Ví dụ, bạn có thể muốn mặc định là fold 4\n",
        "    print(f\"Sử dụng fold mặc định: {default_fold_if_not_found}\")\n",
        "    best_fold_id = default_fold_if_not_found\n",
        "    # Hoặc bạn có thể dừng chương trình:\n",
        "    # exit(\"Dừng chương trình do không tìm được fold tốt nhất.\")\n",
        "\n",
        "# --- Phần 2: Merge model sử dụng adapter từ fold tốt nhất ---\n",
        "# Sửa đổi để merge vào base model ở định dạng full/half precision\n",
        "\n",
        "print(f\"\\n--- Bắt đầu quá trình merge model cho Fold {best_fold_id} ---\")\n",
        "\n",
        "try:\n",
        "    # Giải phóng bộ nhớ trước khi tải model lớn\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Đã giải phóng bộ nhớ GPU (nếu có).\")\n",
        "\n",
        "    # Cấu hình tải mô hình gốc\n",
        "    base_model_name = \"Viet-Mistral/Vistral-7B-Chat\" # Thay bằng model base của bạn nếu khác\n",
        "    # Dựa trên lỗi trước đó, model Vinallama có tên khác (Viet-Mistral/Vinallama-7B-Chat?)\n",
        "    # Bạn cần chắc chắn base_model_name ở đây là model gốc bạn đã dùng để fine-tune\n",
        "    # Ví dụ: base_model_name = \"Viet-Mistral/Vinallama-7B-Chat\" # <-- KHẢ NĂNG CAO BẠN CẦN THAY ĐỔI Ở ĐÂY\n",
        "\n",
        "    print(f\"Đang tải tokenizer cho model: {base_model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model_name,\n",
        "        token=hf_token  # Sử dụng token từ biến môi trường\n",
        "    )\n",
        "    print(\"Tokenizer đã được tải.\")\n",
        "\n",
        "    # Tải base model ở định dạng Bfloat16 (hoặc Float16) - KHÔNG DÙNG BitsAndBytes\n",
        "    print(f\"Đang tải base model '{base_model_name}' ở định dạng BF16 (không lượng tử hóa BitsAndBytes)...\")\n",
        "    # Bỏ hoàn toàn quantization_config khi tải base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        # quantization_config=quantization_config, # <-- BỎ DÒNG NÀY!\n",
        "        torch_dtype=torch.bfloat16, # Tải ở bfloat16 (kích thước lớn)\n",
        "        device_map=\"cuda\", # Vẫn dùng auto device_map để phân bổ lên GPU nếu có đủ VRAM\n",
        "        token=hf_token\n",
        "    )\n",
        "    print(\"Base model đã được tải thành công ở định dạng BF16.\")\n",
        "\n",
        "    # Đường dẫn tới adapter LoRA của fold tốt nhất\n",
        "    # Cần điều chỉnh đường dẫn này cho phù hợp với cấu trúc thư mục của bạn\n",
        "    # Đảm bảo bạn sử dụng tên thư mục fine-tuned adapter đúng với fold tốt nhất tìm được ở Phần 1\n",
        "    fine_tuned_adapters_base_dir = \"/home/thanhnguyenvq2403/model/KLTN/\" # Giả định thư mục chứa fine-tuned models\n",
        "    adapter_path_for_best_fold = os.path.join(fine_tuned_adapters_base_dir, f\"finetuned_vistral_fold_{best_fold_id}\") # <--- SỬA TÊN THƯ MỤC ADAPTER NẾU CẦN (ví dụ: vinallama thay vì seaLLM)\n",
        "\n",
        "    print(f\"Đang tải adapter LoRA từ: {adapter_path_for_best_fold}\")\n",
        "    if not os.path.isdir(adapter_path_for_best_fold):\n",
        "        raise FileNotFoundError(f\"Lỗi: Thư mục adapter LoRA không tồn tại: {adapter_path_for_best_fold}\")\n",
        "\n",
        "    lora_model = PeftModel.from_pretrained(base_model, adapter_path_for_best_fold, is_trainable=False)\n",
        "    print(\"Adapter LoRA đã được tải.\")\n",
        "\n",
        "    # Hợp nhất adapter vào base model (ở định dạng BF16)\n",
        "    # Kết quả merged_model sẽ là mô hình dense ở định dạng BF16\n",
        "    print(\"Đang hợp nhất adapter LoRA vào base model (BF16)...\")\n",
        "    merged_model = lora_model.merge_and_unload()\n",
        "    print(\"Hợp nhất adapter thành công. Mô hình đã hợp nhất ở định dạng BF16.\")\n",
        "\n",
        "    # Lưu mô hình đã hợp nhất\n",
        "    # Đặt tên cho thư mục lưu model đã merge.\n",
        "    # Tên này nên phản ánh rằng nó đã merge và ở định dạng không lượng tử hóa BitsAndBytes\n",
        "    output_merged_model_dir = f\"/home/thanhnguyenvq2403/model/KLTN/merged_dama_2_fold_{best_fold_id}_bf16\" # Đổi tên cho rõ định dạng\n",
        "    print(f\"Đang lưu mô hình đã hợp nhất vào: {output_merged_model_dir}\")\n",
        "\n",
        "    # Đảm bảo thư mục output tồn tại\n",
        "    os.makedirs(output_merged_model_dir, exist_ok=True)\n",
        "\n",
        "    merged_model.save_pretrained(output_merged_model_dir, safe_serialization=True) # Nên dùng safe_serialization\n",
        "    tokenizer.save_pretrained(output_merged_model_dir) # Lưu cả tokenizer\n",
        "\n",
        "    print(f\"Mô hình đã hợp nhất và tokenizer đã được lưu vào: {output_merged_model_dir}\")\n",
        "\n",
        "    # Dọn dẹp bộ nhớ\n",
        "    del base_model\n",
        "    del lora_model\n",
        "    del merged_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Đã dọn dẹp bộ nhớ.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "     print(f\"LỖI FILE: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"ĐÃ CÓ LỖI XẢY RA TRONG QUÁ TRÌNH MERGE MODEL: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "046403be22d543dab3c5d9c3d3836b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_788d9b117db34ff6b06e1fd6a942c39c",
              "IPY_MODEL_916b489caeac4041839f1ee2365360a3",
              "IPY_MODEL_c8d0620a47874f34b94681c71f35c3c8"
            ],
            "layout": "IPY_MODEL_57e0a819a11b42e0a35b75e072b2dbb1"
          }
        },
        "076e2be9ac414d6d92b90f203da87c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d402147806d413590c8bbbcd89646de",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edf820a16c044fe0a2d2f85b11b64683",
            "value": 1176
          }
        },
        "079359b31bb04296a232a84fc1b7db2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff5faacf70a04f19833d41589b7a78e5",
              "IPY_MODEL_e0d2e5d548e04e2ebcf6b5a79083f0dc",
              "IPY_MODEL_7ac0a7531ecc4823b33549eeff34a131"
            ],
            "layout": "IPY_MODEL_970af49c4f1940758130a5e334901a14"
          }
        },
        "0926abb88e2e4381b4b38839e6f652f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ac46097afa5445591d9a3fbad639ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4b1cc17a48461f8e2da6dc9b03c82a",
            "placeholder": "​",
            "style": "IPY_MODEL_4d0cb99e715847d0b05a031736ecd688",
            "value": " 1176/1176 [00:00&lt;00:00, 18224.04 examples/s]"
          }
        },
        "1201dfd96b7d4fcabf5b10b3fa769759": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efc1ff78f5f546638b298419a4151695",
            "placeholder": "​",
            "style": "IPY_MODEL_0926abb88e2e4381b4b38839e6f652f5",
            "value": "Truncating train dataset: 100%"
          }
        },
        "1807262027f34e70b1e94883ac402b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a66367d4522a4b1eb6ce7b88394f8e30",
            "placeholder": "​",
            "style": "IPY_MODEL_a0041fc725964c50a3beff504ddeb9ab",
            "value": " 294/294 [00:00&lt;00:00, 2586.64 examples/s]"
          }
        },
        "187ce14699ae454c819ceacc050a955f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffc5523dd084452a20ae6c913d39e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e320bdd7a7c484183ed8b93e6705af6",
            "placeholder": "​",
            "style": "IPY_MODEL_cc5fad4a005e4aa9bcaecccb8217df1e",
            "value": " 1176/1176 [00:00&lt;00:00, 2971.18 examples/s]"
          }
        },
        "220a0b50b8d04e3dac9bbaf89de8a7c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1201dfd96b7d4fcabf5b10b3fa769759",
              "IPY_MODEL_98ae3ea018e24dabb90618b485a2b2cb",
              "IPY_MODEL_85f4e94e397f43a588a527fe41bc015a"
            ],
            "layout": "IPY_MODEL_686120f3641f43298071d6a5e32fe644"
          }
        },
        "23b95f393dd940039de764bd67d7709a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246d63e4a14841c594db092689f2a64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb370caba2b844c486d30748e6caf9e0",
            "placeholder": "​",
            "style": "IPY_MODEL_73e04b4d775848a19e267669303d8266",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "2ae2a398a2a945d18ec5409bda99644a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d03ea85ed9e4262bfa21065ad73eae4",
              "IPY_MODEL_5f8edfd1f93f4001b51bde9e815d66af",
              "IPY_MODEL_f1a26a9520da4b888f8c54322ae29b82"
            ],
            "layout": "IPY_MODEL_ca3c86d9cf48410199515ce86fd68eaf"
          }
        },
        "2bd1218e781f4784be96419b838c45af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c01848e500843e39f326eeb3b0a8cce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d03ea85ed9e4262bfa21065ad73eae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd2597c60874a4f9c920f8420b366f0",
            "placeholder": "​",
            "style": "IPY_MODEL_cd056a6bb14048bdbc7860ce2c06303f",
            "value": "Applying formatting function to train dataset: 100%"
          }
        },
        "2d402147806d413590c8bbbcd89646de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35fcfd38ee344fd0924e883972074784": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d7b45cd5bfa4ad9ba035ac29882fb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80acce4d1d5c4459ab794ba6bbcc32cf",
            "placeholder": "​",
            "style": "IPY_MODEL_fa04a7d89df34dfcbd73d62f4dda799c",
            "value": " 294/294 [00:00&lt;00:00, 15282.07 examples/s]"
          }
        },
        "3fd2597c60874a4f9c920f8420b366f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4375f815d6724658a2cfaa6c608e478f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b4ce75564943ca8b250e54a7ce1834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "442e0cfbeb554fd58e2bf0c2d77829c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4526ca6058b74e11ba4329d843d6a9d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494b39f1fcf9434b88559980e35c006f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a974c0a2ec043219c538076bfffc87a",
            "placeholder": "​",
            "style": "IPY_MODEL_79f22a64d0fa4310b9ee63bbfb66b41a",
            "value": "Tokenizing eval dataset: 100%"
          }
        },
        "4a251249b63e4688ba357a8d412a3b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1267d7977354e5aa3db8a28f511f2b8",
            "placeholder": "​",
            "style": "IPY_MODEL_9e526d55e8f84c66ab040536bc502d2b",
            "value": " 294/294 [00:00&lt;00:00, 7943.86 examples/s]"
          }
        },
        "4adcb8502fce4dd1af5b8a5b648fa263": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b47975658384af1a549c9f1acf1e5a5",
              "IPY_MODEL_56eff52cba7c4e06a6378805f2aa2707",
              "IPY_MODEL_4a251249b63e4688ba357a8d412a3b69"
            ],
            "layout": "IPY_MODEL_55fdfa966042462bb35ee44cfbfb841e"
          }
        },
        "4d0cb99e715847d0b05a031736ecd688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eaab0584d164139b520cd15eaa3793f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187ce14699ae454c819ceacc050a955f",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a08c13a1cfcb4fd295994b99f788d6c3",
            "value": 294
          }
        },
        "551f5413b209400fa23bedb10c4cde9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b1ad5ece2c4be18a2acdb322b93c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc18b5fb2e349afa2489a0adf1b21fc",
            "placeholder": "​",
            "style": "IPY_MODEL_2bd1218e781f4784be96419b838c45af",
            "value": "Truncating eval dataset: 100%"
          }
        },
        "55fdfa966042462bb35ee44cfbfb841e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56eff52cba7c4e06a6378805f2aa2707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea9a076bce947bab5096a028f7b3237",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c034029a3d5e48ebaf7ea25fa7cb96fd",
            "value": 294
          }
        },
        "57e0a819a11b42e0a35b75e072b2dbb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58850dfce0cc4811ba06ab1ea71d306d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9cb618b32044518965997a96a7d930": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c5ac7432fae43f2a17870b83e19a17a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f8edfd1f93f4001b51bde9e815d66af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d98a8f43fc76406897ffbb66f7e136c0",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8f7022d52464b0eb74f965319229cef",
            "value": 1176
          }
        },
        "5fc18b5fb2e349afa2489a0adf1b21fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "634e691c8c8a4fb68c5af7704b9ce0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65597da15fda4b51b7aa859b380c12db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "686120f3641f43298071d6a5e32fe644": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69fe2be7c9814f2ab93168f3d6ac45ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a974c0a2ec043219c538076bfffc87a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d044e9840e449c5b1e718fb06e99515": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4b1cc17a48461f8e2da6dc9b03c82a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73e04b4d775848a19e267669303d8266": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "788d9b117db34ff6b06e1fd6a942c39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba961b0dab1a4ac6864646de1d46d9e6",
            "placeholder": "​",
            "style": "IPY_MODEL_442e0cfbeb554fd58e2bf0c2d77829c0",
            "value": "Converting train dataset to ChatML: 100%"
          }
        },
        "79f22a64d0fa4310b9ee63bbfb66b41a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79f8dacb371b4fd987b8a4330520cc04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac0a7531ecc4823b33549eeff34a131": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d044e9840e449c5b1e718fb06e99515",
            "placeholder": "​",
            "style": "IPY_MODEL_d00e51ae50d144e392bf12aa8b54b303",
            "value": " 294/294 [00:00&lt;00:00, 5948.68 examples/s]"
          }
        },
        "80acce4d1d5c4459ab794ba6bbcc32cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a4b7f740ec41389a6df6317af60615": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841bc431816441a4bc79dde9600f9176": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f4e94e397f43a588a527fe41bc015a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c5ac7432fae43f2a17870b83e19a17a",
            "placeholder": "​",
            "style": "IPY_MODEL_d224187c0f7340279949cddca91d3ecb",
            "value": " 1176/1176 [00:00&lt;00:00, 56603.68 examples/s]"
          }
        },
        "8983e348f59749bd86146b513c5acacf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd9559471a9418d8a134b43c6dc1258": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcd62139ad44297b2a950d97850468f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90fadb3fb3e14b1baaca9e77f571a80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2e6f823b0224c52a44e1e798e5c1985",
              "IPY_MODEL_4eaab0584d164139b520cd15eaa3793f",
              "IPY_MODEL_943be389e1a947ad99ec7d4d76de6db0"
            ],
            "layout": "IPY_MODEL_2c01848e500843e39f326eeb3b0a8cce"
          }
        },
        "916b489caeac4041839f1ee2365360a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0da77700eab4ab0bb06e1fe7b5184e9",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69fe2be7c9814f2ab93168f3d6ac45ae",
            "value": 1176
          }
        },
        "9419d62d8de24bce9697c0f60467ea99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943be389e1a947ad99ec7d4d76de6db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a81481e6777b4ab78442363553b73381",
            "placeholder": "​",
            "style": "IPY_MODEL_ad666ee0fe7e48de9c4e934212bd337c",
            "value": " 294/294 [00:00&lt;00:00, 8462.00 examples/s]"
          }
        },
        "970af49c4f1940758130a5e334901a14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ae3ea018e24dabb90618b485a2b2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58850dfce0cc4811ba06ab1ea71d306d",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_634e691c8c8a4fb68c5af7704b9ce0fe",
            "value": 1176
          }
        },
        "9b47975658384af1a549c9f1acf1e5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23b95f393dd940039de764bd67d7709a",
            "placeholder": "​",
            "style": "IPY_MODEL_c68779b0088e4f1c867ef1be26e7112b",
            "value": "Converting eval dataset to ChatML: 100%"
          }
        },
        "9c6f2000b243485db70d65e993621d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c778736f27e349edb304160691210f4b",
              "IPY_MODEL_076e2be9ac414d6d92b90f203da87c38",
              "IPY_MODEL_0ac46097afa5445591d9a3fbad639ae5"
            ],
            "layout": "IPY_MODEL_4375f815d6724658a2cfaa6c608e478f"
          }
        },
        "9e320bdd7a7c484183ed8b93e6705af6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e526d55e8f84c66ab040536bc502d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ea9a076bce947bab5096a028f7b3237": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0041fc725964c50a3beff504ddeb9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a08c13a1cfcb4fd295994b99f788d6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0da77700eab4ab0bb06e1fe7b5184e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a66367d4522a4b1eb6ce7b88394f8e30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ee09b72b034234b02194ab94471a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a81481e6777b4ab78442363553b73381": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad666ee0fe7e48de9c4e934212bd337c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3fbfe3f3f604b698e7d869437c0d5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_246d63e4a14841c594db092689f2a64c",
              "IPY_MODEL_c6909db59ff04666b867097216de70ba",
              "IPY_MODEL_1ffc5523dd084452a20ae6c913d39e47"
            ],
            "layout": "IPY_MODEL_35fcfd38ee344fd0924e883972074784"
          }
        },
        "b8f7022d52464b0eb74f965319229cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba961b0dab1a4ac6864646de1d46d9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba9f2bb92edf477d94d6cfa1e4950711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb370caba2b844c486d30748e6caf9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c034029a3d5e48ebaf7ea25fa7cb96fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1267d7977354e5aa3db8a28f511f2b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d35975670546ab9998a6389e3ec707": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c68779b0088e4f1c867ef1be26e7112b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6909db59ff04666b867097216de70ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4526ca6058b74e11ba4329d843d6a9d4",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba9f2bb92edf477d94d6cfa1e4950711",
            "value": 1176
          }
        },
        "c778736f27e349edb304160691210f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79f8dacb371b4fd987b8a4330520cc04",
            "placeholder": "​",
            "style": "IPY_MODEL_d990e34729bf43eca69270c1fd3dc8cb",
            "value": "Applying chat template to train dataset: 100%"
          }
        },
        "c8ad9a852eec438e972cadc8ea1f0fe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d0620a47874f34b94681c71f35c3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841bc431816441a4bc79dde9600f9176",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9cb618b32044518965997a96a7d930",
            "value": " 1176/1176 [00:00&lt;00:00, 18682.87 examples/s]"
          }
        },
        "ca3c86d9cf48410199515ce86fd68eaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbcc5aca91d46ba9b51859494da828b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_494b39f1fcf9434b88559980e35c006f",
              "IPY_MODEL_f849b4139b5d4ab3a2ae32ae83807160",
              "IPY_MODEL_1807262027f34e70b1e94883ac402b39"
            ],
            "layout": "IPY_MODEL_c3d35975670546ab9998a6389e3ec707"
          }
        },
        "cc5fad4a005e4aa9bcaecccb8217df1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd056a6bb14048bdbc7860ce2c06303f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d00e51ae50d144e392bf12aa8b54b303": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d224187c0f7340279949cddca91d3ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d43bfd1e65ae43af97b7fa6b44cf9023": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d98a8f43fc76406897ffbb66f7e136c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d990e34729bf43eca69270c1fd3dc8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc6578ccbf9740bfb315a07b6482d8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0d2e5d548e04e2ebcf6b5a79083f0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2283973a0de470a88d5bb8b78160982",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65597da15fda4b51b7aa859b380c12db",
            "value": 294
          }
        },
        "e2283973a0de470a88d5bb8b78160982": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2627486d9ba4848830b0687cbe8b3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55b1ad5ece2c4be18a2acdb322b93c01",
              "IPY_MODEL_e502fdc27ded43fcbbc4d3fe3fac8030",
              "IPY_MODEL_3d7b45cd5bfa4ad9ba035ac29882fb8b"
            ],
            "layout": "IPY_MODEL_c8ad9a852eec438e972cadc8ea1f0fe2"
          }
        },
        "e2e6f823b0224c52a44e1e798e5c1985": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cd9559471a9418d8a134b43c6dc1258",
            "placeholder": "​",
            "style": "IPY_MODEL_dc6578ccbf9740bfb315a07b6482d8bf",
            "value": "Applying chat template to eval dataset: 100%"
          }
        },
        "e502fdc27ded43fcbbc4d3fe3fac8030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9419d62d8de24bce9697c0f60467ea99",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43b4ce75564943ca8b250e54a7ce1834",
            "value": 294
          }
        },
        "edf820a16c044fe0a2d2f85b11b64683": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efc1ff78f5f546638b298419a4151695": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a26a9520da4b888f8c54322ae29b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8983e348f59749bd86146b513c5acacf",
            "placeholder": "​",
            "style": "IPY_MODEL_551f5413b209400fa23bedb10c4cde9f",
            "value": " 1176/1176 [00:00&lt;00:00, 11431.75 examples/s]"
          }
        },
        "f849b4139b5d4ab3a2ae32ae83807160": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fcd62139ad44297b2a950d97850468f",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7ee09b72b034234b02194ab94471a2a",
            "value": 294
          }
        },
        "fa04a7d89df34dfcbd73d62f4dda799c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff5faacf70a04f19833d41589b7a78e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83a4b7f740ec41389a6df6317af60615",
            "placeholder": "​",
            "style": "IPY_MODEL_d43bfd1e65ae43af97b7fa6b44cf9023",
            "value": "Applying formatting function to eval dataset: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
