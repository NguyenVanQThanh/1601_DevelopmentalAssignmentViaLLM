{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d461df2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d461df2",
    "outputId": "6dd982cb-7742-4e72-8a09-0ed9f7c4cb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/jupyter_venv/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: trl in /opt/jupyter_venv/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: peft in /opt/jupyter_venv/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: datasets in /opt/jupyter_venv/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /opt/jupyter_venv/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: rouge_score in /opt/jupyter_venv/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: underthesea in /opt/jupyter_venv/lib/python3.10/site-packages (6.8.4)\n",
      "Requirement already satisfied: bitsandbytes in /opt/jupyter_venv/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: thefuzz in /opt/jupyter_venv/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: bert_score in /opt/jupyter_venv/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: sentence-transformers in /opt/jupyter_venv/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: requests in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/jupyter_venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: rich in /opt/jupyter_venv/lib/python3.10/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from peft) (2.7.0)\n",
      "Requirement already satisfied: psutil in /opt/jupyter_venv/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: xxhash in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/jupyter_venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: absl-py in /opt/jupyter_venv/lib/python3.10/site-packages (from rouge_score) (2.2.2)\n",
      "Requirement already satisfied: nltk in /opt/jupyter_venv/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: joblib in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (1.5.0)\n",
      "Requirement already satisfied: Click>=6.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (8.2.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (0.9.11)\n",
      "Requirement already satisfied: scikit-learn in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (1.6.1)\n",
      "Requirement already satisfied: underthesea-core==1.0.4 in /opt/jupyter_venv/lib/python3.10/site-packages (from underthesea) (1.0.4)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from thefuzz) (3.13.0)\n",
      "Requirement already satisfied: matplotlib in /opt/jupyter_venv/lib/python3.10/site-packages (from bert_score) (3.10.3)\n",
      "Requirement already satisfied: scipy in /opt/jupyter_venv/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /opt/jupyter_venv/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/jupyter_venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/jupyter_venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/jupyter_venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: jinja2 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: networkx in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/jupyter_venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from triton==3.3.0->torch>=1.13.0->peft) (59.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/jupyter_venv/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from matplotlib->bert_score) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from matplotlib->bert_score) (4.58.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/jupyter_venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/jupyter_venv/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U transformers trl peft datasets evaluate rouge_score underthesea bitsandbytes thefuzz bert_score sentence-transformers\n",
    "# Tải tài nguyên NLTK cho METEOR\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c01dc91-a6d3-4e3e-a99b-1a0c88f5f3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Bây giờ mới import các thư viện\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# ... (phần còn lại của hàm load_model_and_tokenizer và code gọi hàm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1LRADR_xiIjl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LRADR_xiIjl",
    "outputId": "6b563c1a-fda4-4f07-e200-3cef8da70ce4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ddc4b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14ddc4b6",
    "outputId": "d490c573-2ce6-404d-c9c2-338e596f6b1c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from underthesea import word_tokenize\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0bf13f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc0bf13f",
    "outputId": "d33edc49-fd95-4125-9f46-769b9af0efcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_model_and_tokenizer(model_name, quantization=\"int8\"):\n",
    "    # Cấu hình quantization với bitsandbytes\n",
    "    if quantization == \"int8\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,  # Quantize thành INT8\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
    "            bnb_8bit_use_double_quant=True,  # Double quantization để tăng hiệu quả\n",
    "        )\n",
    "    elif quantization == \"int4\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,  # Quantize thành INT4\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
    "            bnb_4bit_use_double_quant=True,  # Double quantization\n",
    "            bnb_4bit_quant_type=\"nf4\",  # Dùng NF4 (Normalized Float 4-bit) để tối ưu\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None  # Không quantize\n",
    "\n",
    "    # Tải tokenizer và mô hình\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,  # Áp dụng quantization\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Cấu hình LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.5,  # Tăng dropout để giảm overfitting\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model, tokenizer, peft_config\n",
    "\n",
    "# Chọn mô hình để thử (thay đổi model_name để thử từng mô hình)\n",
    "model_name = \"SeaLLMs/SeaLLMs-v3-7B-Chat\" \n",
    "model, tokenizer, peft_config = load_model_and_tokenizer(model_name, quantization=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0fadab1",
   "metadata": {
    "id": "e0fadab1"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    if not all(k in example for k in ['input', 'output']):\n",
    "        print('Thiếu key trong example:', example)\n",
    "        return ''  # hoặc raise lỗi nếu bạn muốn dừng\n",
    "    return f\"<s>[INST] {example['input']} [/INST] {example['output']} </s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6ee4ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc6ee4ec",
    "outputId": "2b63813f-cece-4a50-86b9-5f3e801bb8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giá trị duy nhất trong 'input' (exact): 1599\n",
      "Số lượng giá trị duy nhất trong 'output' (exact): 1659\n",
      "Tổng số hàng: 1700\n",
      "Số cặp input tương tự (>90%): 320\n",
      "Số cặp output tương tự (>90%): 41\n",
      "Số hàng sau khi xóa record tương tự: 1471\n",
      "Số lượng giá trị duy nhất trong 'input' (sau xử lý): 1471\n",
      "Số lượng giá trị duy nhất trong 'output' (sau xử lý): 1471\n",
      "                                                  input  \\\n",
      "998   trẻ không biết tôn trọng lượt nói trong thảo l...   \n",
      "254                             trẻ nói ít phải làm sao   \n",
      "1073  trẻ không biết chuẩn bị nội dung khi báo cáo nhóm   \n",
      "643   trẻ không biết nói lời chia tay khi bạn chuyển...   \n",
      "1450  tự kỷ có học được kỹ năng kết thúc tương tác x...   \n",
      "\n",
      "                                                 output  \n",
      "998   nói chen hoặc ngắt lời bạn là thiếu kỹ năng xã...  \n",
      "254   cần đánh giá xem trẻ có hiểu lời tương tác bằn...  \n",
      "1073  nội dung rời rạc là thiếu chuẩn bị – nên luyện...  \n",
      "643   không thể hiện chia tay là thiếu kỹ năng chia ...  \n",
      "1450  có trẻ có thể học nói lời chào cảm ơn xin phép...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from underthesea import word_tokenize\n",
    "from thefuzz import fuzz\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "def extract_json_from_folder(folder_path):\n",
    "    dataset = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    for item in json_data:\n",
    "                        if all(k in item for k in ['input', 'output']):\n",
    "                            item['input'] = preprocess_text(item['input'])\n",
    "                            item['output'] = preprocess_text(item['output'])\n",
    "                            dataset.append(item)\n",
    "                        else:\n",
    "                            print(f\"Thiếu trường trong {filename}: {item}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Không thể parse JSON từ {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi đọc {filename}: {e}\")\n",
    "    return dataset\n",
    "\n",
    "folder_path = \"./data_finetune\"\n",
    "dataset = extract_json_from_folder(folder_path)\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "print(\"Số lượng giá trị duy nhất trong 'input' (exact):\", df['input'].nunique())\n",
    "print(\"Số lượng giá trị duy nhất trong 'output' (exact):\", df['output'].nunique())\n",
    "print(\"Tổng số hàng:\", len(df))\n",
    "\n",
    "# Fuzzy matching để tìm các record tương tự\n",
    "similarity_threshold = 90  # Ngưỡng độ tương đồng (90%)\n",
    "input_pairs = []\n",
    "output_pairs = []\n",
    "\n",
    "# Tìm các cặp input và output tương tự\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        input_sim = fuzz.ratio(df['input'].iloc[i], df['input'].iloc[j])\n",
    "        if input_sim >= similarity_threshold:\n",
    "            input_pairs.append((i, j, input_sim))\n",
    "        output_sim = fuzz.ratio(df['output'].iloc[i], df['output'].iloc[j])\n",
    "        if output_sim >= similarity_threshold:\n",
    "            output_pairs.append((i, j, output_sim))\n",
    "\n",
    "print(f\"Số cặp input tương tự (>{similarity_threshold}%):\", len(input_pairs))\n",
    "print(f\"Số cặp output tương tự (>{similarity_threshold}%):\", len(output_pairs))\n",
    "\n",
    "# Loại bỏ các record có input hoặc output tương tự (giữ record đầu tiên)\n",
    "indices_to_keep = set(range(len(df)))\n",
    "for i, j, _ in input_pairs:\n",
    "    if j in indices_to_keep:\n",
    "        indices_to_keep.remove(j)\n",
    "for i, j, _ in output_pairs:\n",
    "    if j in indices_to_keep:\n",
    "        indices_to_keep.remove(j)\n",
    "\n",
    "df = df.iloc[list(indices_to_keep)].reset_index(drop=True)\n",
    "print(\"Số hàng sau khi xóa record tương tự:\", len(df))\n",
    "\n",
    "# Kiểm tra lại độ unique\n",
    "print(\"Số lượng giá trị duy nhất trong 'input' (sau xử lý):\", df['input'].nunique())\n",
    "print(\"Số lượng giá trị duy nhất trong 'output' (sau xử lý):\", df['output'].nunique())\n",
    "\n",
    "# Chia train/validation\n",
    "full_dataset = Dataset.from_pandas(df[['input', 'output']])\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df[['input', 'output']])\n",
    "eval_dataset = Dataset.from_pandas(eval_df[['input', 'output']])\n",
    "print(train_df[['input', 'output']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3d58ec",
   "metadata": {
    "id": "3d3d58ec"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "def evaluate_metrics(predictions, references):\n",
    "    # Tải các độ đo\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "    # Tính các độ đo\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "    meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "    # Tải mô hình nhúng câu để tính Cosine Similarity\n",
    "    embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "    # Tính embeddings cho dự đoán và tham chiếu\n",
    "    pred_embeddings = embedder.encode(predictions, convert_to_tensor=True)\n",
    "    ref_embeddings = embedder.encode(references, convert_to_tensor=True)\n",
    "\n",
    "    # Tính Cosine Similarity giữa từng cặp dự đoán-tham chiếu\n",
    "    cosine_scores = util.cos_sim(pred_embeddings, ref_embeddings)\n",
    "    # Lấy trung bình Cosine Similarity (chỉ lấy đường chéo chính, vì mỗi dự đoán chỉ so với tham chiếu tương ứng)\n",
    "    avg_cosine_similarity = np.mean([cosine_scores[i][i].item() for i in range(len(predictions))])\n",
    "    \n",
    "    # Gộp kết quả\n",
    "    metrics = {\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rouge2\": rouge_results[\"rouge2\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"],\n",
    "        \"meteor\": meteor_results[\"meteor\"],\n",
    "        \"cosine_similarity\": avg_cosine_similarity,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def generate_predictions(model, tokenizer, inputs, max_length=200):\n",
    "    \"\"\"Tạo dự đoán từ mô hình cho các đầu vào.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Đảm bảo pad_token được thiết lập\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for input_text in inputs:\n",
    "        # Tiền xử lý input để đồng bộ với huấn luyện\n",
    "        input_text = preprocess_text(input_text)\n",
    "        prompt = f\"<s>[INST] {input_text} [/INST]\"\n",
    "        inputs_encoded = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs_encoded,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = generated_text.split(\"[/INST]\")[-1].strip()\n",
    "        # Tiền xử lý dự đoán để đồng bộ với tham chiếu\n",
    "        pred = preprocess_text(pred)\n",
    "        predictions.append(pred)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b126a89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2ae2a398a2a945d18ec5409bda99644a",
      "2d03ea85ed9e4262bfa21065ad73eae4",
      "5f8edfd1f93f4001b51bde9e815d66af",
      "f1a26a9520da4b888f8c54322ae29b82",
      "ca3c86d9cf48410199515ce86fd68eaf",
      "3fd2597c60874a4f9c920f8420b366f0",
      "cd056a6bb14048bdbc7860ce2c06303f",
      "d98a8f43fc76406897ffbb66f7e136c0",
      "b8f7022d52464b0eb74f965319229cef",
      "8983e348f59749bd86146b513c5acacf",
      "551f5413b209400fa23bedb10c4cde9f",
      "046403be22d543dab3c5d9c3d3836b9e",
      "788d9b117db34ff6b06e1fd6a942c39c",
      "916b489caeac4041839f1ee2365360a3",
      "c8d0620a47874f34b94681c71f35c3c8",
      "57e0a819a11b42e0a35b75e072b2dbb1",
      "ba961b0dab1a4ac6864646de1d46d9e6",
      "442e0cfbeb554fd58e2bf0c2d77829c0",
      "a0da77700eab4ab0bb06e1fe7b5184e9",
      "69fe2be7c9814f2ab93168f3d6ac45ae",
      "841bc431816441a4bc79dde9600f9176",
      "5b9cb618b32044518965997a96a7d930",
      "9c6f2000b243485db70d65e993621d3c",
      "c778736f27e349edb304160691210f4b",
      "076e2be9ac414d6d92b90f203da87c38",
      "0ac46097afa5445591d9a3fbad639ae5",
      "4375f815d6724658a2cfaa6c608e478f",
      "79f8dacb371b4fd987b8a4330520cc04",
      "d990e34729bf43eca69270c1fd3dc8cb",
      "2d402147806d413590c8bbbcd89646de",
      "edf820a16c044fe0a2d2f85b11b64683",
      "6f4b1cc17a48461f8e2da6dc9b03c82a",
      "4d0cb99e715847d0b05a031736ecd688",
      "b3fbfe3f3f604b698e7d869437c0d5fe",
      "246d63e4a14841c594db092689f2a64c",
      "c6909db59ff04666b867097216de70ba",
      "1ffc5523dd084452a20ae6c913d39e47",
      "35fcfd38ee344fd0924e883972074784",
      "bb370caba2b844c486d30748e6caf9e0",
      "73e04b4d775848a19e267669303d8266",
      "4526ca6058b74e11ba4329d843d6a9d4",
      "ba9f2bb92edf477d94d6cfa1e4950711",
      "9e320bdd7a7c484183ed8b93e6705af6",
      "cc5fad4a005e4aa9bcaecccb8217df1e",
      "220a0b50b8d04e3dac9bbaf89de8a7c9",
      "1201dfd96b7d4fcabf5b10b3fa769759",
      "98ae3ea018e24dabb90618b485a2b2cb",
      "85f4e94e397f43a588a527fe41bc015a",
      "686120f3641f43298071d6a5e32fe644",
      "efc1ff78f5f546638b298419a4151695",
      "0926abb88e2e4381b4b38839e6f652f5",
      "58850dfce0cc4811ba06ab1ea71d306d",
      "634e691c8c8a4fb68c5af7704b9ce0fe",
      "5c5ac7432fae43f2a17870b83e19a17a",
      "d224187c0f7340279949cddca91d3ecb",
      "079359b31bb04296a232a84fc1b7db2d",
      "ff5faacf70a04f19833d41589b7a78e5",
      "e0d2e5d548e04e2ebcf6b5a79083f0dc",
      "7ac0a7531ecc4823b33549eeff34a131",
      "970af49c4f1940758130a5e334901a14",
      "83a4b7f740ec41389a6df6317af60615",
      "d43bfd1e65ae43af97b7fa6b44cf9023",
      "e2283973a0de470a88d5bb8b78160982",
      "65597da15fda4b51b7aa859b380c12db",
      "6d044e9840e449c5b1e718fb06e99515",
      "d00e51ae50d144e392bf12aa8b54b303",
      "4adcb8502fce4dd1af5b8a5b648fa263",
      "9b47975658384af1a549c9f1acf1e5a5",
      "56eff52cba7c4e06a6378805f2aa2707",
      "4a251249b63e4688ba357a8d412a3b69",
      "55fdfa966042462bb35ee44cfbfb841e",
      "23b95f393dd940039de764bd67d7709a",
      "c68779b0088e4f1c867ef1be26e7112b",
      "9ea9a076bce947bab5096a028f7b3237",
      "c034029a3d5e48ebaf7ea25fa7cb96fd",
      "c1267d7977354e5aa3db8a28f511f2b8",
      "9e526d55e8f84c66ab040536bc502d2b",
      "90fadb3fb3e14b1baaca9e77f571a80e",
      "e2e6f823b0224c52a44e1e798e5c1985",
      "4eaab0584d164139b520cd15eaa3793f",
      "943be389e1a947ad99ec7d4d76de6db0",
      "2c01848e500843e39f326eeb3b0a8cce",
      "8cd9559471a9418d8a134b43c6dc1258",
      "dc6578ccbf9740bfb315a07b6482d8bf",
      "187ce14699ae454c819ceacc050a955f",
      "a08c13a1cfcb4fd295994b99f788d6c3",
      "a81481e6777b4ab78442363553b73381",
      "ad666ee0fe7e48de9c4e934212bd337c",
      "cbbcc5aca91d46ba9b51859494da828b",
      "494b39f1fcf9434b88559980e35c006f",
      "f849b4139b5d4ab3a2ae32ae83807160",
      "1807262027f34e70b1e94883ac402b39",
      "c3d35975670546ab9998a6389e3ec707",
      "6a974c0a2ec043219c538076bfffc87a",
      "79f22a64d0fa4310b9ee63bbfb66b41a",
      "8fcd62139ad44297b2a950d97850468f",
      "a7ee09b72b034234b02194ab94471a2a",
      "a66367d4522a4b1eb6ce7b88394f8e30",
      "a0041fc725964c50a3beff504ddeb9ab",
      "e2627486d9ba4848830b0687cbe8b3d3",
      "55b1ad5ece2c4be18a2acdb322b93c01",
      "e502fdc27ded43fcbbc4d3fe3fac8030",
      "3d7b45cd5bfa4ad9ba035ac29882fb8b",
      "c8ad9a852eec438e972cadc8ea1f0fe2",
      "5fc18b5fb2e349afa2489a0adf1b21fc",
      "2bd1218e781f4784be96419b838c45af",
      "9419d62d8de24bce9697c0f60467ea99",
      "43b4ce75564943ca8b250e54a7ce1834",
      "80acce4d1d5c4459ab794ba6bbcc32cf",
      "fa04a7d89df34dfcbd73d62f4dda799c"
     ]
    },
    "id": "7b126a89",
    "outputId": "b0106b8a-df18-45c5-a8b4-bae598edea1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bắt đầu chạy đơn ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 23092.34 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1176/1176 [00:00<00:00, 33982.10 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 23820.22 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 3984.64 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 365127.06 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 21376.59 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 30175.58 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 21191.70 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 4013.73 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 127901.56 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,176\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 80,740,352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1176, Eval dataset size: 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter_venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/219 08:23 < 02:26, 0.33 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.368900</td>\n",
       "      <td>3.509594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.108500</td>\n",
       "      <td>2.584057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.221300</td>\n",
       "      <td>1.980858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.861000</td>\n",
       "      <td>1.800578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.711100</td>\n",
       "      <td>1.675380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.643400</td>\n",
       "      <td>1.639721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.572700</td>\n",
       "      <td>1.562272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.469700</td>\n",
       "      <td>1.562473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.446700</td>\n",
       "      <td>1.493969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.365400</td>\n",
       "      <td>1.478668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.378500</td>\n",
       "      <td>1.454019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.352300</td>\n",
       "      <td>1.429186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.340300</td>\n",
       "      <td>1.422301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.315300</td>\n",
       "      <td>1.412183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.295000</td>\n",
       "      <td>1.405428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>1.399132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.180200</td>\n",
       "      <td>1.397082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_single_seaLLM/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_single_seaLLM/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_single_seaLLM/checkpoint-100/special_tokens_map.json\n",
      "/opt/jupyter_venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_single_seaLLM/checkpoint-100 (score: 1.3970816135406494).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_seaLLM_single/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_seaLLM_single/special_tokens_map.json\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "loading file vocab.txt from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Run Metrics: {'rouge1': np.float64(0.6209654627664118), 'rouge2': np.float64(0.2679127253546244), 'rougeL': np.float64(0.3886934998629489), 'meteor': np.float64(0.2989020973699027), 'cosine_similarity': np.float64(0.7166761577129364)}\n",
      "Thời gian chạy đơn: 2128.68 giây\n",
      "Input: trẻ không biết tôn trọng ranh giới của người khác\n",
      "Prediction: không biết giữ khoảng cách khi nói chuyện hoặc chạm vào người khác là thiếu kỹ năng xã hội và thể hiện thiếu tôn trọng cá nhân s\n",
      "Reference: cố ép bạn làm theo ý mình là chưa có kỹ năng đồng thuận – nên luyện hỏi bạn có muốn chơi thế này không trước khi bắt đầu\n",
      "\n",
      "Input: trẻ không dùng tay để chỉ vật\n",
      "Prediction: trẻ không dùng tay để chỉ vật có thể là dấu hiệu chậm phát triển ngôn ngữ động tác hoặc khả năng xã hội – cần được đánh giá chuyên sâu s\n",
      "Reference: việc trẻ không dùng tay để chỉ khi muốn thể hiện nhu cầu là dấu hiệu quan trọng để nhận biết trẻ có khó khăn trong giao tiếp phi ngôn ngữ\n",
      "\n",
      "Input: tự kỷ và rối loạn ngôn ngữ khác gì nhau\n",
      "Prediction: tự kỷ là hội chứng phức tạp liên quan đến cả phát triển xã hội hành vi và ngôn ngữ trong khi rối loạn ngôn ngữ chỉ tập trung vào khả năng giao tiếp và hiểu lời không có vấn đề về kỹ năng xã hội hoặc học tập chung s\n",
      "Reference: rối loạn ngôn ngữ chủ yếu ảnh hưởng khả năng hiểu và diễn đạt ngôn ngữ trong khi tự kỷ còn liên quan đến hành vi và giao tiếp xã hội một số trẻ tự kỷ có rối loạn ngôn ngữ đi kèm\n",
      "\n",
      "Input: những dấu hiệu của trẻ chậm nói\n",
      "Prediction: trẻ chậm nói thường có các dấu hiệu như ít nói sau 2 tuổi không bắt chước phát âm khó khăn trong giao tiếp không phản ứng với tên gọi hoặc ít dùng ngôn ngữ biểu cảm s\n",
      "Reference: những trẻ không có tiến bộ về lời nói ít tương tác bằng mắt và không phát triển vốn từ theo tháng tuổi là dấu hiệu cần được kiểm tra sớm\n",
      "\n",
      "Input: trẻ 22 tháng chưa nói được từ nào\n",
      "Prediction: trẻ 22 tháng cần theo dõi kỹ năng xã hội và phát triển tư duy nếu không có phản ứng xã hội hay chơi đồ chơi đơn lẻ nên đưa đến chuyên gia đánh giá phát triển sớm s\n",
      "Reference: trẻ 22 tháng chưa nói được từ nào là biểu hiện cần lưu ý nên đưa trẻ đi đánh giá ngôn ngữ thính giác hành vi xã hội … càng sớm càng tốt để xác định nguyên nhân và bắt đầu can thiệp\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40424"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "import time\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Loại bỏ cột không cần thiết để tránh cảnh báo\n",
    "train_dataset = train_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_dataset.column_names else [])\n",
    "eval_dataset = eval_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_dataset.column_names else [])\n",
    "\n",
    "# Đo thời gian chạy đơn\n",
    "print(\"\\n=== Bắt đầu chạy đơn ===\")\n",
    "start_time = time.time()\n",
    "# Cấu hình huấn luyện cho Single Run\n",
    "training_arguments_single = TrainingArguments(\n",
    "    output_dir=\"./results_single_seaLLM\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.1,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Huấn luyện Single Run\n",
    "trainer_single = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments_single,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.01,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}\")\n",
    "trainer_single.train()\n",
    "\n",
    "# Lưu mô hình Single Run\n",
    "model.save_pretrained(\"./finetuned_seaLLM_single\")\n",
    "tokenizer.save_pretrained(\"./finetuned_seaLLM_single\")\n",
    "\n",
    "# Đánh giá Single Run\n",
    "test_inputs = eval_df['input'].tolist()\n",
    "test_references = eval_df['output'].tolist()\n",
    "predictions_single = generate_predictions(model, tokenizer, test_inputs)\n",
    "metrics_single = evaluate_metrics(predictions_single, test_references)\n",
    "print(\"Single Run Metrics:\", metrics_single)\n",
    "\n",
    "# Lưu metrics vào file\n",
    "with open(\"single_run_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics_single, f, indent=4)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Thời gian chạy đơn: {duration:.2f} giây\")\n",
    "# Kiểm tra mẫu dự đoán\n",
    "for i in range(5):\n",
    "    print(f\"Input: {test_inputs[i]}\")\n",
    "    print(f\"Prediction: {predictions_single[i]}\")\n",
    "    print(f\"Reference: {test_references[i]}\\n\")\n",
    "del model, trainer_single\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c38bce",
   "metadata": {
    "id": "18c38bce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/model.safetensors.index.json\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at SeaLLMs/SeaLLMs-v3-7B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train size: 1176, Eval size: 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 24070.96 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1176/1176 [00:00<00:00, 33716.82 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 23800.23 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 3955.05 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1176/1176 [00:00<00:00, 441466.17 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 21371.42 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 29289.83 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 21188.07 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 3845.83 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 133648.70 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,176\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 80,740,352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/219 06:11 < 02:18, 0.43 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.446400</td>\n",
       "      <td>3.562637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.153600</td>\n",
       "      <td>2.595901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.233900</td>\n",
       "      <td>2.009578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.884600</td>\n",
       "      <td>1.788575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.734100</td>\n",
       "      <td>1.700707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.630100</td>\n",
       "      <td>1.638279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.568800</td>\n",
       "      <td>1.573687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.471500</td>\n",
       "      <td>1.568805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.433300</td>\n",
       "      <td>1.510820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.414000</td>\n",
       "      <td>1.500385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.383400</td>\n",
       "      <td>1.465578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.346600</td>\n",
       "      <td>1.451896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.344600</td>\n",
       "      <td>1.437270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.340200</td>\n",
       "      <td>1.428523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.325900</td>\n",
       "      <td>1.420555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.205900</td>\n",
       "      <td>1.416580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_1/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_1/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_1/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 295\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_1/checkpoint-100 (score: 1.4165802001953125).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_seaLLM_fold_1/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_seaLLM_fold_1/special_tokens_map.json\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Metrics: {'rouge1': np.float64(0.6200400707199183), 'rouge2': np.float64(0.2653809954657709), 'rougeL': np.float64(0.3869829849116039), 'meteor': np.float64(0.29977252852576763), 'cosine_similarity': np.float64(0.7107559507933714)}\n",
      "Thời gian chạy Fold 1: 1342.25 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/model.safetensors.index.json\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at SeaLLMs/SeaLLMs-v3-7B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23870.45 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 33017.84 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 22632.31 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 3995.28 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 396362.57 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 22291.98 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 29947.67 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21497.25 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 3899.98 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 125560.06 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 80,740,352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/219 07:08 < 01:33, 0.42 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.469100</td>\n",
       "      <td>3.560424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.155600</td>\n",
       "      <td>2.588985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.227300</td>\n",
       "      <td>1.986628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.859200</td>\n",
       "      <td>1.810577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.746000</td>\n",
       "      <td>1.688296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.589800</td>\n",
       "      <td>1.646391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.605300</td>\n",
       "      <td>1.572036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.641400</td>\n",
       "      <td>1.559412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.424400</td>\n",
       "      <td>1.505609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.447900</td>\n",
       "      <td>1.478844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.397900</td>\n",
       "      <td>1.455244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.361500</td>\n",
       "      <td>1.442785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.360200</td>\n",
       "      <td>1.438339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.359100</td>\n",
       "      <td>1.418505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.385900</td>\n",
       "      <td>1.407999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.238400</td>\n",
       "      <td>1.406046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.171000</td>\n",
       "      <td>1.405430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.217900</td>\n",
       "      <td>1.402470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_2/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_2/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_2/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_2/checkpoint-100 (score: 1.402470350265503).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_seaLLM_fold_2/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_seaLLM_fold_2/special_tokens_map.json\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Metrics: {'rouge1': np.float64(0.6150194476133337), 'rouge2': np.float64(0.26575860542813906), 'rougeL': np.float64(0.38138324962843595), 'meteor': np.float64(0.3053644084335558), 'cosine_similarity': np.float64(0.7007525935262239)}\n",
      "Thời gian chạy Fold 2: 1409.68 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/model.safetensors.index.json\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at SeaLLMs/SeaLLMs-v3-7B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 22954.22 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 32800.44 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 20160.39 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 3945.20 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 453074.14 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21395.05 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 29055.05 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 20713.66 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 3724.33 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 116596.57 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 80,740,352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/219 06:55 < 01:31, 0.43 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.443400</td>\n",
       "      <td>3.588466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.223100</td>\n",
       "      <td>2.607627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.253700</td>\n",
       "      <td>1.995495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.832500</td>\n",
       "      <td>1.834454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.720100</td>\n",
       "      <td>1.721159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.625600</td>\n",
       "      <td>1.693332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.590200</td>\n",
       "      <td>1.594280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.608800</td>\n",
       "      <td>1.578740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.453700</td>\n",
       "      <td>1.506898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.406700</td>\n",
       "      <td>1.497715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.360400</td>\n",
       "      <td>1.479867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.378500</td>\n",
       "      <td>1.454126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.360200</td>\n",
       "      <td>1.447111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.361400</td>\n",
       "      <td>1.428493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.422200</td>\n",
       "      <td>1.418369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.189800</td>\n",
       "      <td>1.415376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.186000</td>\n",
       "      <td>1.417719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.191400</td>\n",
       "      <td>1.414028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_3/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_3/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_3/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_3/checkpoint-100 (score: 1.4140279293060303).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_seaLLM_fold_3/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_seaLLM_fold_3/special_tokens_map.json\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Metrics: {'rouge1': np.float64(0.6140320285136482), 'rouge2': np.float64(0.26326810794388406), 'rougeL': np.float64(0.3838437874256317), 'meteor': np.float64(0.2934598263068808), 'cosine_similarity': np.float64(0.7117251672712314)}\n",
      "Thời gian chạy Fold 3: 1337.43 giây\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/model.safetensors.index.json\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at SeaLLMs/SeaLLMs-v3-7B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23479.90 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 32516.55 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23756.61 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 3899.47 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 442872.15 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21882.16 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 29735.36 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21287.58 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 3860.63 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 116015.18 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 80,740,352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/219 06:37 < 01:56, 0.42 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.404400</td>\n",
       "      <td>3.564743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.202100</td>\n",
       "      <td>2.581878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.258200</td>\n",
       "      <td>1.949601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.844900</td>\n",
       "      <td>1.782102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.763300</td>\n",
       "      <td>1.668871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>1.631508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.620400</td>\n",
       "      <td>1.552141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.644300</td>\n",
       "      <td>1.530426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.445500</td>\n",
       "      <td>1.466146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.419800</td>\n",
       "      <td>1.451501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.391700</td>\n",
       "      <td>1.430066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.345100</td>\n",
       "      <td>1.414586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.365900</td>\n",
       "      <td>1.414746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.351200</td>\n",
       "      <td>1.392182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.457200</td>\n",
       "      <td>1.382551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.208300</td>\n",
       "      <td>1.379029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.209100</td>\n",
       "      <td>1.378473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_4/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_4/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_4/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_4/checkpoint-100 (score: 1.3784734010696411).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_seaLLM_fold_4/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_seaLLM_fold_4/special_tokens_map.json\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Metrics: {'rouge1': np.float64(0.6228208442049195), 'rouge2': np.float64(0.2720542133201901), 'rougeL': np.float64(0.3896761821591251), 'meteor': np.float64(0.29739650798481826), 'cosine_similarity': np.float64(0.7075164061622555)}\n",
      "Thời gian chạy Fold 4: 1320.89 giây\n",
      "\n",
      "Training Fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/model.safetensors.index.json\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at SeaLLMs/SeaLLMs-v3-7B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train size: 1177, Eval size: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23811.85 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 32446.24 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 22953.48 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 3899.86 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 400608.28 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 22304.07 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 29066.01 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21195.00 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 3787.52 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 121132.16 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,177\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 219\n",
      "  Number of trainable parameters = 80,740,352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/219 07:09 < 01:34, 0.41 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.473000</td>\n",
       "      <td>3.527105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.161300</td>\n",
       "      <td>2.558093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.254800</td>\n",
       "      <td>1.979399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.852100</td>\n",
       "      <td>1.800944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>1.695969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.572000</td>\n",
       "      <td>1.626179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.587200</td>\n",
       "      <td>1.578608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.694900</td>\n",
       "      <td>1.542922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.398600</td>\n",
       "      <td>1.501430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.405500</td>\n",
       "      <td>1.472815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.392900</td>\n",
       "      <td>1.462161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.369400</td>\n",
       "      <td>1.441666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.330900</td>\n",
       "      <td>1.433541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.326400</td>\n",
       "      <td>1.422761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.445400</td>\n",
       "      <td>1.412230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.233000</td>\n",
       "      <td>1.406537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.200500</td>\n",
       "      <td>1.400414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.207800</td>\n",
       "      <td>1.399182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_fold_5/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./results_fold_5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./results_fold_5/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, input, output. If text, input, output are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_fold_5/checkpoint-100 (score: 1.3991822004318237).\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./finetuned_seaLLM_fold_5/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_seaLLM_fold_5/special_tokens_map.json\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at None\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Metrics: {'rouge1': np.float64(0.6199962413803413), 'rouge2': np.float64(0.26565730440611623), 'rougeL': np.float64(0.37761908175709924), 'meteor': np.float64(0.29684600750577994), 'cosine_similarity': np.float64(0.7089269719511068)}\n",
      "Thời gian chạy Fold 5: 1371.63 giây\n",
      "\n",
      "=== Kết thúc huấn luyện K-Fold ===\n",
      "Tổng thời gian chạy: 6784.18 giây\n",
      "Thời gian trung bình mỗi fold: 1356.37 giây\n",
      "\n",
      "Average Cross-Validation Metrics: {'rouge1': np.float64(0.6183817264864322), 'rouge2': np.float64(0.2664238453128201), 'rougeL': np.float64(0.3839010571763791), 'meteor': np.float64(0.29856785575136047), 'cosine_similarity': np.float64(0.7079354179408377)}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 5-Fold Cross-Validation\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "# Cấu hình KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "fold_models = []\n",
    "fold_times = []\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Lặp qua từng fold\n",
    "for fold, (train_idx, eval_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"\\nTraining Fold {fold + 1}...\")\n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    # Tạo tập train và eval cho fold hiện tại\n",
    "    train_fold = df.iloc[train_idx][['input', 'output']]\n",
    "    eval_fold = df.iloc[eval_idx][['input', 'output']]\n",
    "    train_fold_dataset = Dataset.from_pandas(train_fold)\n",
    "    eval_fold_dataset = Dataset.from_pandas(eval_fold)\n",
    "\n",
    "    # Loại bỏ cột không cần thiết\n",
    "    train_fold_dataset = train_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_fold_dataset.column_names else [])\n",
    "    eval_fold_dataset = eval_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_fold_dataset.column_names else [])\n",
    "\n",
    "    # Tải lại mô hình gốc với INT4 quantization\n",
    "    model, tokenizer, peft_config = load_model_and_tokenizer(model_name,quantization=\"int4\")\n",
    "    print(f\"Fold {fold + 1} - Train size: {len(train_fold_dataset)}, Eval size: {len(eval_fold_dataset)}\")\n",
    "    # Cấu hình huấn luyện cho fold\n",
    "    training_arguments_fold = TrainingArguments(\n",
    "        output_dir=f\"./results_fold_{fold + 1}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.1,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.1,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        log_level=\"info\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    # Huấn luyện fold\n",
    "    trainer_fold = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments_fold,\n",
    "        train_dataset=train_fold_dataset,\n",
    "        eval_dataset=eval_fold_dataset,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=formatting_func,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,\n",
    "                early_stopping_threshold=0.01,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    trainer_fold.train()\n",
    "\n",
    "    # Lưu mô hình fold\n",
    "    fold_path = f\"./finetuned_seaLLM_fold_{fold + 1}\"\n",
    "    model.save_pretrained(fold_path)\n",
    "    tokenizer.save_pretrained(fold_path)\n",
    "    fold_models.append(fold_path)\n",
    "\n",
    "    # Đánh giá fold\n",
    "    test_inputs_fold = eval_fold['input'].tolist()\n",
    "    test_references_fold = eval_fold['output'].tolist()\n",
    "    predictions_fold = generate_predictions(model, tokenizer, test_inputs_fold)\n",
    "    metrics_fold = evaluate_metrics(predictions_fold, test_references_fold)\n",
    "    print(f\"Fold {fold + 1} Metrics:\", metrics_fold)\n",
    "    fold_metrics.append(metrics_fold)\n",
    "\n",
    "    # Lưu metrics của fold\n",
    "    with open(f\"fold_{fold + 1}_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics_fold, f, indent=4)\n",
    "\n",
    "    fold_end_time = time.time()\n",
    "    fold_duration = fold_end_time - fold_start_time\n",
    "    fold_times.append(fold_duration)\n",
    "    print(f\"Thời gian chạy Fold {fold + 1}: {fold_duration:.2f} giây\")\n",
    "    \n",
    "    # Dọn dẹp bộ nhớ\n",
    "    del model, trainer_fold\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Tính tổng thời gian và thời gian trung bình\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\n=== Kết thúc huấn luyện K-Fold ===\")\n",
    "print(f\"Tổng thời gian chạy: {total_duration:.2f} giây\")\n",
    "print(f\"Thời gian trung bình mỗi fold: {np.mean(fold_times):.2f} giây\")\n",
    "\n",
    "# Tính trung bình metrics qua các fold\n",
    "avg_metrics = {\n",
    "    \"rouge1\": np.mean([m[\"rouge1\"] for m in fold_metrics]),\n",
    "    \"rouge2\": np.mean([m[\"rouge2\"] for m in fold_metrics]),\n",
    "    \"rougeL\": np.mean([m[\"rougeL\"] for m in fold_metrics]),\n",
    "    \"meteor\": np.mean([m[\"meteor\"] for m in fold_metrics]),\n",
    "    \"cosine_similarity\": np.mean([m[\"cosine_similarity\"] for m in fold_metrics]),\n",
    "}\n",
    "print(\"\\nAverage Cross-Validation Metrics:\", avg_metrics)\n",
    "\n",
    "# Lưu metrics trung bình\n",
    "with open(\"cross_validation_seaLLM_metrics.json\", \"w\") as f:\n",
    "    json.dump(avg_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b58f93a",
   "metadata": {
    "id": "6b58f93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tìm fold tốt nhất dựa trên metric: 'cosine_similarity' (cao hơn là tốt hơn: True)\n",
      "Fold 1: 'cosine_similarity' = 0.7108\n",
      "Fold 2: 'cosine_similarity' = 0.7008\n",
      "Fold 3: 'cosine_similarity' = 0.7117\n",
      "Fold 4: 'cosine_similarity' = 0.7075\n",
      "Fold 5: 'cosine_similarity' = 0.7089\n",
      "\n",
      "=> Fold tốt nhất được chọn: Fold 3 với cosine_similarity = 0.7117\n",
      "\n",
      "--- Bắt đầu quá trình merge model cho Fold 3 ---\n",
      "Đã giải phóng bộ nhớ GPU (nếu có)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đang tải tokenizer cho model: SeaLLMs/SeaLLMs-v3-7B-Chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer đã được tải.\n",
      "Đang tải base model 'SeaLLMs/SeaLLMs-v3-7B-Chat' ở định dạng BF16 (không lượng tử hóa BitsAndBytes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 131072,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/model.safetensors.index.json\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.33it/s]\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at SeaLLMs/SeaLLMs-v3-7B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--SeaLLMs--SeaLLMs-v3-7B-Chat/snapshots/a9900348910a6d7f611a6859270d6c28da1e0789/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model đã được tải thành công ở định dạng BF16.\n",
      "Đang tải adapter LoRA từ: /home/thanhnguyenvq2403/model/KLTN/finetuned_seaLLM_fold_3\n",
      "LỖI FILE: Lỗi: Thư mục adapter LoRA không tồn tại: /home/thanhnguyenvq2403/model/KLTN/finetuned_seaLLM_fold_3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer#, BitsAndBytesConfig # Không cần BitsAndBytesConfig cho bước này\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# --- Phần 1: Tìm fold có metric tốt nhất ---\n",
    "\n",
    "def load_metrics_from_file(file_path):\n",
    "    \"\"\"Đọc metrics từ file JSON.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Cảnh báo: Không tìm thấy file metric: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Cảnh báo: File metric không phải JSON hợp lệ: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def find_best_fold_by_metric(metrics_base_path, num_folds, metric_name_to_optimize, higher_is_better=True):\n",
    "    \"\"\"\n",
    "    Tìm fold có giá trị metric được chỉ định cao nhất (hoặc thấp nhất).\n",
    "\n",
    "    Args:\n",
    "        metrics_base_path (str): Đường dẫn cơ sở đến thư mục chứa các file metrics.\n",
    "                                 Ví dụ: \"./\" nếu các file ở thư mục hiện tại.\n",
    "        num_folds (int): Tổng số lượng folds.\n",
    "        metric_name_to_optimize (str): Tên của metric dùng để so sánh (ví dụ: 'rougeL', 'cosine_similarity').\n",
    "        higher_is_better (bool): True nếu giá trị metric cao hơn là tốt hơn, False nếu thấp hơn là tốt hơn.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_fold_number, best_metric_value, all_metrics_of_best_fold) hoặc (None, None, None) nếu lỗi.\n",
    "    \"\"\"\n",
    "    best_fold_so_far = None\n",
    "    best_metric_val = -float('inf') if higher_is_better else float('inf')\n",
    "    all_metrics_best_fold = None\n",
    "\n",
    "    print(f\"Đang tìm fold tốt nhất dựa trên metric: '{metric_name_to_optimize}' (cao hơn là tốt hơn: {higher_is_better})\")\n",
    "\n",
    "    for i in range(1, num_folds + 1):\n",
    "        # Giả sử tên file là \"fold_X_metrics\" hoặc \"fold_X_metrics.json\"\n",
    "        # Sửa lại mẫu tên file nếu cần\n",
    "        metric_file_candidate_1 = os.path.join(metrics_base_path, f\"fold_{i}_metrics.json\")\n",
    "        metric_file_candidate_2 = os.path.join(metrics_base_path, f\"fold_{i}_metrics\") # Không có đuôi .json\n",
    "\n",
    "        metrics_data = None\n",
    "        if os.path.exists(metric_file_candidate_1):\n",
    "            metrics_data = load_metrics_from_file(metric_file_candidate_1)\n",
    "        elif os.path.exists(metric_file_candidate_2):\n",
    "            metrics_data = load_metrics_from_file(metric_file_candidate_2)\n",
    "        else:\n",
    "            print(f\"Không tìm thấy file metric cho fold {i} tại: {metric_file_candidate_1} hoặc {metric_file_candidate_2}\")\n",
    "            continue\n",
    "\n",
    "        if metrics_data is None:\n",
    "            continue # Bỏ qua nếu không đọc được file\n",
    "\n",
    "        if metric_name_to_optimize not in metrics_data:\n",
    "            print(f\"Cảnh báo: Metric '{metric_name_to_optimize}' không có trong file của fold {i}. Bỏ qua fold này để so sánh.\")\n",
    "            continue\n",
    "\n",
    "        current_metric_val = metrics_data[metric_name_to_optimize]\n",
    "        print(f\"Fold {i}: '{metric_name_to_optimize}' = {current_metric_val:.4f}\")\n",
    "\n",
    "        if higher_is_better:\n",
    "            if current_metric_val > best_metric_val:\n",
    "                best_metric_val = current_metric_val\n",
    "                best_fold_so_far = i\n",
    "                all_metrics_best_fold = metrics_data\n",
    "        else: # lower is better\n",
    "            if current_metric_val < best_metric_val:\n",
    "                best_metric_val = current_metric_val\n",
    "                best_fold_so_far = i\n",
    "                all_metrics_best_fold = metrics_data\n",
    "\n",
    "    if best_fold_so_far is not None:\n",
    "        print(f\"\\n=> Fold tốt nhất được chọn: Fold {best_fold_so_far} với {metric_name_to_optimize} = {best_metric_val:.4f}\")\n",
    "        # print(f\"Toàn bộ metrics của fold {best_fold_so_far}: {all_metrics_best_fold}\")\n",
    "        return best_fold_so_far, best_metric_val, all_metrics_best_fold\n",
    "    else:\n",
    "        print(f\"\\n=> Không thể xác định fold tốt nhất dựa trên metric '{metric_name_to_optimize}'.\")\n",
    "        return None, None, None\n",
    "\n",
    "# --- Cấu hình cho việc tìm fold ---\n",
    "# Đặt đường dẫn đến thư mục chứa các file fold_X_metrics của bạn\n",
    "# Ví dụ: nếu các file fold_1_metrics, fold_2_metrics,... nằm cùng thư mục với script này:\n",
    "METRICS_FILES_DIRECTORY = \"./\" \n",
    "NUM_TOTAL_FOLDS = 5\n",
    "# Chọn metric bạn muốn sử dụng để quyết định fold nào tốt nhất\n",
    "# Ví dụ: 'rougeL', 'cosine_similarity', 'meteor', 'rouge1', 'rouge2'\n",
    "METRIC_TO_OPTIMIZE_FOR = \"cosine_similarity\" # THAY ĐỔI TÊN METRIC NÀY NẾU CẦN\n",
    "\n",
    "best_fold_id, _, _ = find_best_fold_by_metric(\n",
    "    METRICS_FILES_DIRECTORY,\n",
    "    NUM_TOTAL_FOLDS,\n",
    "    METRIC_TO_OPTIMIZE_FOR,\n",
    "    higher_is_better=True # Hầu hết các metric này, cao hơn là tốt hơn\n",
    ")\n",
    "\n",
    "if best_fold_id is None:\n",
    "    print(\"Không thể xác định fold tốt nhất. Sẽ sử dụng một fold mặc định hoặc dừng chương trình.\")\n",
    "    # Gán một fold mặc định nếu muốn tiếp tục\n",
    "    default_fold_if_not_found = 4 # Ví dụ, bạn có thể muốn mặc định là fold 4\n",
    "    print(f\"Sử dụng fold mặc định: {default_fold_if_not_found}\")\n",
    "    best_fold_id = default_fold_if_not_found\n",
    "    # Hoặc bạn có thể dừng chương trình:\n",
    "    # exit(\"Dừng chương trình do không tìm được fold tốt nhất.\")\n",
    "\n",
    "# --- Phần 2: Merge model sử dụng adapter từ fold tốt nhất ---\n",
    "# Sửa đổi để merge vào base model ở định dạng full/half precision\n",
    "\n",
    "print(f\"\\n--- Bắt đầu quá trình merge model cho Fold {best_fold_id} ---\")\n",
    "\n",
    "try:\n",
    "    # Giải phóng bộ nhớ trước khi tải model lớn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Đã giải phóng bộ nhớ GPU (nếu có).\")\n",
    "\n",
    "    # Cấu hình tải mô hình gốc\n",
    "    base_model_name = \"SeaLLMs/SeaLLMs-v3-7B-Chat\" # Thay bằng model base của bạn nếu khác\n",
    "    # Dựa trên lỗi trước đó, model Vinallama có tên khác (Viet-Mistral/Vinallama-7B-Chat?)\n",
    "    # Bạn cần chắc chắn base_model_name ở đây là model gốc bạn đã dùng để fine-tune\n",
    "    # Ví dụ: base_model_name = \"Viet-Mistral/Vinallama-7B-Chat\" # <-- KHẢ NĂNG CAO BẠN CẦN THAY ĐỔI Ở ĐÂY\n",
    "\n",
    "    print(f\"Đang tải tokenizer cho model: {base_model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    print(\"Tokenizer đã được tải.\")\n",
    "\n",
    "    # Tải base model ở định dạng Bfloat16 (hoặc Float16) - KHÔNG DÙNG BitsAndBytes\n",
    "    print(f\"Đang tải base model '{base_model_name}' ở định dạng BF16 (không lượng tử hóa BitsAndBytes)...\")\n",
    "    # Bỏ hoàn toàn quantization_config khi tải base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        # quantization_config=quantization_config, # <-- BỎ DÒNG NÀY!\n",
    "        torch_dtype=torch.bfloat16, # Tải ở bfloat16 (kích thước lớn)\n",
    "        device_map=\"cuda\" # Vẫn dùng auto device_map để phân bổ lên GPU nếu có đủ VRAM\n",
    "    )\n",
    "    print(\"Base model đã được tải thành công ở định dạng BF16.\")\n",
    "\n",
    "    # Đường dẫn tới adapter LoRA của fold tốt nhất\n",
    "    # Cần điều chỉnh đường dẫn này cho phù hợp với cấu trúc thư mục của bạn\n",
    "    # Đảm bảo bạn sử dụng tên thư mục fine-tuned adapter đúng với fold tốt nhất tìm được ở Phần 1\n",
    "    fine_tuned_adapters_base_dir = \"/home/thanhnguyenvq2403/model/KLTN/\" # Giả định thư mục chứa fine-tuned models\n",
    "    adapter_path_for_best_fold = os.path.join(fine_tuned_adapters_base_dir, f\"finetuned_seaLLM_fold_{best_fold_id}\") # <--- SỬA TÊN THƯ MỤC ADAPTER NẾU CẦN (ví dụ: vinallama thay vì seaLLM)\n",
    "\n",
    "    print(f\"Đang tải adapter LoRA từ: {adapter_path_for_best_fold}\")\n",
    "    if not os.path.isdir(adapter_path_for_best_fold):\n",
    "        raise FileNotFoundError(f\"Lỗi: Thư mục adapter LoRA không tồn tại: {adapter_path_for_best_fold}\")\n",
    "\n",
    "    lora_model = PeftModel.from_pretrained(base_model, adapter_path_for_best_fold, is_trainable=False)\n",
    "    print(\"Adapter LoRA đã được tải.\")\n",
    "\n",
    "    # Hợp nhất adapter vào base model (ở định dạng BF16)\n",
    "    # Kết quả merged_model sẽ là mô hình dense ở định dạng BF16\n",
    "    print(\"Đang hợp nhất adapter LoRA vào base model (BF16)...\")\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    print(\"Hợp nhất adapter thành công. Mô hình đã hợp nhất ở định dạng BF16.\")\n",
    "\n",
    "    # Lưu mô hình đã hợp nhất\n",
    "    # Đặt tên cho thư mục lưu model đã merge.\n",
    "    # Tên này nên phản ánh rằng nó đã merge và ở định dạng không lượng tử hóa BitsAndBytes\n",
    "    output_merged_model_dir = f\"/home/thanhnguyenvq2403/model/KLTN/merged_seaLLM_fold_{best_fold_id}_bf16\" # Đổi tên cho rõ định dạng\n",
    "    print(f\"Đang lưu mô hình đã hợp nhất vào: {output_merged_model_dir}\")\n",
    "\n",
    "    # Đảm bảo thư mục output tồn tại\n",
    "    os.makedirs(output_merged_model_dir, exist_ok=True)\n",
    "\n",
    "    merged_model.save_pretrained(output_merged_model_dir, safe_serialization=True) # Nên dùng safe_serialization\n",
    "    tokenizer.save_pretrained(output_merged_model_dir) # Lưu cả tokenizer\n",
    "\n",
    "    print(f\"Mô hình đã hợp nhất và tokenizer đã được lưu vào: {output_merged_model_dir}\")\n",
    "\n",
    "    # Dọn dẹp bộ nhớ\n",
    "    del base_model\n",
    "    del lora_model\n",
    "    del merged_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Đã dọn dẹp bộ nhớ.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     print(f\"LỖI FILE: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"ĐÃ CÓ LỖI XẢY RA TRONG QUÁ TRÌNH MERGE MODEL: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772727e2-9ceb-47f9-95cd-530d69ad0b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "046403be22d543dab3c5d9c3d3836b9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_788d9b117db34ff6b06e1fd6a942c39c",
       "IPY_MODEL_916b489caeac4041839f1ee2365360a3",
       "IPY_MODEL_c8d0620a47874f34b94681c71f35c3c8"
      ],
      "layout": "IPY_MODEL_57e0a819a11b42e0a35b75e072b2dbb1"
     }
    },
    "076e2be9ac414d6d92b90f203da87c38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d402147806d413590c8bbbcd89646de",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_edf820a16c044fe0a2d2f85b11b64683",
      "value": 1176
     }
    },
    "079359b31bb04296a232a84fc1b7db2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff5faacf70a04f19833d41589b7a78e5",
       "IPY_MODEL_e0d2e5d548e04e2ebcf6b5a79083f0dc",
       "IPY_MODEL_7ac0a7531ecc4823b33549eeff34a131"
      ],
      "layout": "IPY_MODEL_970af49c4f1940758130a5e334901a14"
     }
    },
    "0926abb88e2e4381b4b38839e6f652f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ac46097afa5445591d9a3fbad639ae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f4b1cc17a48461f8e2da6dc9b03c82a",
      "placeholder": "​",
      "style": "IPY_MODEL_4d0cb99e715847d0b05a031736ecd688",
      "value": " 1176/1176 [00:00&lt;00:00, 18224.04 examples/s]"
     }
    },
    "1201dfd96b7d4fcabf5b10b3fa769759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efc1ff78f5f546638b298419a4151695",
      "placeholder": "​",
      "style": "IPY_MODEL_0926abb88e2e4381b4b38839e6f652f5",
      "value": "Truncating train dataset: 100%"
     }
    },
    "1807262027f34e70b1e94883ac402b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a66367d4522a4b1eb6ce7b88394f8e30",
      "placeholder": "​",
      "style": "IPY_MODEL_a0041fc725964c50a3beff504ddeb9ab",
      "value": " 294/294 [00:00&lt;00:00, 2586.64 examples/s]"
     }
    },
    "187ce14699ae454c819ceacc050a955f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ffc5523dd084452a20ae6c913d39e47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e320bdd7a7c484183ed8b93e6705af6",
      "placeholder": "​",
      "style": "IPY_MODEL_cc5fad4a005e4aa9bcaecccb8217df1e",
      "value": " 1176/1176 [00:00&lt;00:00, 2971.18 examples/s]"
     }
    },
    "220a0b50b8d04e3dac9bbaf89de8a7c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1201dfd96b7d4fcabf5b10b3fa769759",
       "IPY_MODEL_98ae3ea018e24dabb90618b485a2b2cb",
       "IPY_MODEL_85f4e94e397f43a588a527fe41bc015a"
      ],
      "layout": "IPY_MODEL_686120f3641f43298071d6a5e32fe644"
     }
    },
    "23b95f393dd940039de764bd67d7709a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "246d63e4a14841c594db092689f2a64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb370caba2b844c486d30748e6caf9e0",
      "placeholder": "​",
      "style": "IPY_MODEL_73e04b4d775848a19e267669303d8266",
      "value": "Tokenizing train dataset: 100%"
     }
    },
    "2ae2a398a2a945d18ec5409bda99644a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d03ea85ed9e4262bfa21065ad73eae4",
       "IPY_MODEL_5f8edfd1f93f4001b51bde9e815d66af",
       "IPY_MODEL_f1a26a9520da4b888f8c54322ae29b82"
      ],
      "layout": "IPY_MODEL_ca3c86d9cf48410199515ce86fd68eaf"
     }
    },
    "2bd1218e781f4784be96419b838c45af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c01848e500843e39f326eeb3b0a8cce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d03ea85ed9e4262bfa21065ad73eae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fd2597c60874a4f9c920f8420b366f0",
      "placeholder": "​",
      "style": "IPY_MODEL_cd056a6bb14048bdbc7860ce2c06303f",
      "value": "Applying formatting function to train dataset: 100%"
     }
    },
    "2d402147806d413590c8bbbcd89646de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35fcfd38ee344fd0924e883972074784": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d7b45cd5bfa4ad9ba035ac29882fb8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80acce4d1d5c4459ab794ba6bbcc32cf",
      "placeholder": "​",
      "style": "IPY_MODEL_fa04a7d89df34dfcbd73d62f4dda799c",
      "value": " 294/294 [00:00&lt;00:00, 15282.07 examples/s]"
     }
    },
    "3fd2597c60874a4f9c920f8420b366f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4375f815d6724658a2cfaa6c608e478f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43b4ce75564943ca8b250e54a7ce1834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "442e0cfbeb554fd58e2bf0c2d77829c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4526ca6058b74e11ba4329d843d6a9d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "494b39f1fcf9434b88559980e35c006f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a974c0a2ec043219c538076bfffc87a",
      "placeholder": "​",
      "style": "IPY_MODEL_79f22a64d0fa4310b9ee63bbfb66b41a",
      "value": "Tokenizing eval dataset: 100%"
     }
    },
    "4a251249b63e4688ba357a8d412a3b69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1267d7977354e5aa3db8a28f511f2b8",
      "placeholder": "​",
      "style": "IPY_MODEL_9e526d55e8f84c66ab040536bc502d2b",
      "value": " 294/294 [00:00&lt;00:00, 7943.86 examples/s]"
     }
    },
    "4adcb8502fce4dd1af5b8a5b648fa263": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b47975658384af1a549c9f1acf1e5a5",
       "IPY_MODEL_56eff52cba7c4e06a6378805f2aa2707",
       "IPY_MODEL_4a251249b63e4688ba357a8d412a3b69"
      ],
      "layout": "IPY_MODEL_55fdfa966042462bb35ee44cfbfb841e"
     }
    },
    "4d0cb99e715847d0b05a031736ecd688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4eaab0584d164139b520cd15eaa3793f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_187ce14699ae454c819ceacc050a955f",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a08c13a1cfcb4fd295994b99f788d6c3",
      "value": 294
     }
    },
    "551f5413b209400fa23bedb10c4cde9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55b1ad5ece2c4be18a2acdb322b93c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fc18b5fb2e349afa2489a0adf1b21fc",
      "placeholder": "​",
      "style": "IPY_MODEL_2bd1218e781f4784be96419b838c45af",
      "value": "Truncating eval dataset: 100%"
     }
    },
    "55fdfa966042462bb35ee44cfbfb841e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56eff52cba7c4e06a6378805f2aa2707": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ea9a076bce947bab5096a028f7b3237",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c034029a3d5e48ebaf7ea25fa7cb96fd",
      "value": 294
     }
    },
    "57e0a819a11b42e0a35b75e072b2dbb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58850dfce0cc4811ba06ab1ea71d306d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b9cb618b32044518965997a96a7d930": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c5ac7432fae43f2a17870b83e19a17a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f8edfd1f93f4001b51bde9e815d66af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d98a8f43fc76406897ffbb66f7e136c0",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8f7022d52464b0eb74f965319229cef",
      "value": 1176
     }
    },
    "5fc18b5fb2e349afa2489a0adf1b21fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "634e691c8c8a4fb68c5af7704b9ce0fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65597da15fda4b51b7aa859b380c12db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "686120f3641f43298071d6a5e32fe644": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69fe2be7c9814f2ab93168f3d6ac45ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a974c0a2ec043219c538076bfffc87a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d044e9840e449c5b1e718fb06e99515": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f4b1cc17a48461f8e2da6dc9b03c82a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e04b4d775848a19e267669303d8266": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "788d9b117db34ff6b06e1fd6a942c39c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba961b0dab1a4ac6864646de1d46d9e6",
      "placeholder": "​",
      "style": "IPY_MODEL_442e0cfbeb554fd58e2bf0c2d77829c0",
      "value": "Converting train dataset to ChatML: 100%"
     }
    },
    "79f22a64d0fa4310b9ee63bbfb66b41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79f8dacb371b4fd987b8a4330520cc04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ac0a7531ecc4823b33549eeff34a131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d044e9840e449c5b1e718fb06e99515",
      "placeholder": "​",
      "style": "IPY_MODEL_d00e51ae50d144e392bf12aa8b54b303",
      "value": " 294/294 [00:00&lt;00:00, 5948.68 examples/s]"
     }
    },
    "80acce4d1d5c4459ab794ba6bbcc32cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83a4b7f740ec41389a6df6317af60615": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "841bc431816441a4bc79dde9600f9176": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85f4e94e397f43a588a527fe41bc015a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c5ac7432fae43f2a17870b83e19a17a",
      "placeholder": "​",
      "style": "IPY_MODEL_d224187c0f7340279949cddca91d3ecb",
      "value": " 1176/1176 [00:00&lt;00:00, 56603.68 examples/s]"
     }
    },
    "8983e348f59749bd86146b513c5acacf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cd9559471a9418d8a134b43c6dc1258": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fcd62139ad44297b2a950d97850468f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90fadb3fb3e14b1baaca9e77f571a80e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e2e6f823b0224c52a44e1e798e5c1985",
       "IPY_MODEL_4eaab0584d164139b520cd15eaa3793f",
       "IPY_MODEL_943be389e1a947ad99ec7d4d76de6db0"
      ],
      "layout": "IPY_MODEL_2c01848e500843e39f326eeb3b0a8cce"
     }
    },
    "916b489caeac4041839f1ee2365360a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0da77700eab4ab0bb06e1fe7b5184e9",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69fe2be7c9814f2ab93168f3d6ac45ae",
      "value": 1176
     }
    },
    "9419d62d8de24bce9697c0f60467ea99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "943be389e1a947ad99ec7d4d76de6db0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a81481e6777b4ab78442363553b73381",
      "placeholder": "​",
      "style": "IPY_MODEL_ad666ee0fe7e48de9c4e934212bd337c",
      "value": " 294/294 [00:00&lt;00:00, 8462.00 examples/s]"
     }
    },
    "970af49c4f1940758130a5e334901a14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98ae3ea018e24dabb90618b485a2b2cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58850dfce0cc4811ba06ab1ea71d306d",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_634e691c8c8a4fb68c5af7704b9ce0fe",
      "value": 1176
     }
    },
    "9b47975658384af1a549c9f1acf1e5a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b95f393dd940039de764bd67d7709a",
      "placeholder": "​",
      "style": "IPY_MODEL_c68779b0088e4f1c867ef1be26e7112b",
      "value": "Converting eval dataset to ChatML: 100%"
     }
    },
    "9c6f2000b243485db70d65e993621d3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c778736f27e349edb304160691210f4b",
       "IPY_MODEL_076e2be9ac414d6d92b90f203da87c38",
       "IPY_MODEL_0ac46097afa5445591d9a3fbad639ae5"
      ],
      "layout": "IPY_MODEL_4375f815d6724658a2cfaa6c608e478f"
     }
    },
    "9e320bdd7a7c484183ed8b93e6705af6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e526d55e8f84c66ab040536bc502d2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ea9a076bce947bab5096a028f7b3237": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0041fc725964c50a3beff504ddeb9ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a08c13a1cfcb4fd295994b99f788d6c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a0da77700eab4ab0bb06e1fe7b5184e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a66367d4522a4b1eb6ce7b88394f8e30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7ee09b72b034234b02194ab94471a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a81481e6777b4ab78442363553b73381": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad666ee0fe7e48de9c4e934212bd337c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3fbfe3f3f604b698e7d869437c0d5fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_246d63e4a14841c594db092689f2a64c",
       "IPY_MODEL_c6909db59ff04666b867097216de70ba",
       "IPY_MODEL_1ffc5523dd084452a20ae6c913d39e47"
      ],
      "layout": "IPY_MODEL_35fcfd38ee344fd0924e883972074784"
     }
    },
    "b8f7022d52464b0eb74f965319229cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba961b0dab1a4ac6864646de1d46d9e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba9f2bb92edf477d94d6cfa1e4950711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bb370caba2b844c486d30748e6caf9e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c034029a3d5e48ebaf7ea25fa7cb96fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1267d7977354e5aa3db8a28f511f2b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3d35975670546ab9998a6389e3ec707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c68779b0088e4f1c867ef1be26e7112b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6909db59ff04666b867097216de70ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4526ca6058b74e11ba4329d843d6a9d4",
      "max": 1176,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba9f2bb92edf477d94d6cfa1e4950711",
      "value": 1176
     }
    },
    "c778736f27e349edb304160691210f4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79f8dacb371b4fd987b8a4330520cc04",
      "placeholder": "​",
      "style": "IPY_MODEL_d990e34729bf43eca69270c1fd3dc8cb",
      "value": "Applying chat template to train dataset: 100%"
     }
    },
    "c8ad9a852eec438e972cadc8ea1f0fe2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8d0620a47874f34b94681c71f35c3c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_841bc431816441a4bc79dde9600f9176",
      "placeholder": "​",
      "style": "IPY_MODEL_5b9cb618b32044518965997a96a7d930",
      "value": " 1176/1176 [00:00&lt;00:00, 18682.87 examples/s]"
     }
    },
    "ca3c86d9cf48410199515ce86fd68eaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbbcc5aca91d46ba9b51859494da828b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_494b39f1fcf9434b88559980e35c006f",
       "IPY_MODEL_f849b4139b5d4ab3a2ae32ae83807160",
       "IPY_MODEL_1807262027f34e70b1e94883ac402b39"
      ],
      "layout": "IPY_MODEL_c3d35975670546ab9998a6389e3ec707"
     }
    },
    "cc5fad4a005e4aa9bcaecccb8217df1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd056a6bb14048bdbc7860ce2c06303f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d00e51ae50d144e392bf12aa8b54b303": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d224187c0f7340279949cddca91d3ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d43bfd1e65ae43af97b7fa6b44cf9023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d98a8f43fc76406897ffbb66f7e136c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d990e34729bf43eca69270c1fd3dc8cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc6578ccbf9740bfb315a07b6482d8bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0d2e5d548e04e2ebcf6b5a79083f0dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2283973a0de470a88d5bb8b78160982",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65597da15fda4b51b7aa859b380c12db",
      "value": 294
     }
    },
    "e2283973a0de470a88d5bb8b78160982": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2627486d9ba4848830b0687cbe8b3d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55b1ad5ece2c4be18a2acdb322b93c01",
       "IPY_MODEL_e502fdc27ded43fcbbc4d3fe3fac8030",
       "IPY_MODEL_3d7b45cd5bfa4ad9ba035ac29882fb8b"
      ],
      "layout": "IPY_MODEL_c8ad9a852eec438e972cadc8ea1f0fe2"
     }
    },
    "e2e6f823b0224c52a44e1e798e5c1985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cd9559471a9418d8a134b43c6dc1258",
      "placeholder": "​",
      "style": "IPY_MODEL_dc6578ccbf9740bfb315a07b6482d8bf",
      "value": "Applying chat template to eval dataset: 100%"
     }
    },
    "e502fdc27ded43fcbbc4d3fe3fac8030": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9419d62d8de24bce9697c0f60467ea99",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_43b4ce75564943ca8b250e54a7ce1834",
      "value": 294
     }
    },
    "edf820a16c044fe0a2d2f85b11b64683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "efc1ff78f5f546638b298419a4151695": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1a26a9520da4b888f8c54322ae29b82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8983e348f59749bd86146b513c5acacf",
      "placeholder": "​",
      "style": "IPY_MODEL_551f5413b209400fa23bedb10c4cde9f",
      "value": " 1176/1176 [00:00&lt;00:00, 11431.75 examples/s]"
     }
    },
    "f849b4139b5d4ab3a2ae32ae83807160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fcd62139ad44297b2a950d97850468f",
      "max": 294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a7ee09b72b034234b02194ab94471a2a",
      "value": 294
     }
    },
    "fa04a7d89df34dfcbd73d62f4dda799c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff5faacf70a04f19833d41589b7a78e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83a4b7f740ec41389a6df6317af60615",
      "placeholder": "​",
      "style": "IPY_MODEL_d43bfd1e65ae43af97b7fa6b44cf9023",
      "value": "Applying formatting function to eval dataset: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
