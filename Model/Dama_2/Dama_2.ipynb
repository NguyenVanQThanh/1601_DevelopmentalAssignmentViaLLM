{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1d461df2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d461df2",
        "outputId": "6dd982cb-7742-4e72-8a09-0ed9f7c4cb39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.16.1-py3-none-any.whl (336 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 KB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 KB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 KB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 KB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting underthesea\n",
            "  Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting thefuzz\n",
            "  Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 KB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-2.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting tqdm>=4.27\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Collecting tokenizers<0.22,>=0.21\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.4.3\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.30.0\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 KB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.34.0\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 KB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 KB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch>=1.13.0\n",
            "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from peft) (7.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess<0.70.17\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow>=15.0.0\n",
            "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 KB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m146.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in ./venv/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting underthesea-core==1.0.4\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 KB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Click>=6.0\n",
            "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.0.0\n",
            "  Downloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.5.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
            "Collecting Pillow\n",
            "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Collecting propcache>=0.2.0\n",
            "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 KB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=17.3.0\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 KB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
            "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 KB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 KB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Collecting async-timeout<6.0,>=4.0\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 KB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 KB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 KB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.2.0\n",
            "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sympy==1.13.1\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 KB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzdata>=2022.7\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich->trl) (2.19.1)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Using legacy 'setup.py install' for rouge_score, since package 'wheel' is not installed.\n",
            "Installing collected packages: underthesea-core, triton, pytz, nvidia-cusparselt-cu12, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, sympy, safetensors, regex, rapidfuzz, pyyaml, python-dotenv, python-crfsuite, pyarrow, propcache, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, mdurl, MarkupSafe, joblib, idna, fsspec, frozenlist, filelock, dill, Click, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, absl-py, yarl, thefuzz, scipy, requests, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, markdown-it-py, jinja2, dotenv, aiosignal, scikit-learn, rouge_score, rich, nvidia-cusolver-cu12, huggingface-hub, aiohttp, underthesea, torch, tokenizers, transformers, datasets, bitsandbytes, accelerate, trl, sentence-transformers, peft, evaluate\n",
            "  Running setup.py install for rouge_score ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed Click-8.1.8 MarkupSafe-3.0.2 Pillow-11.2.1 absl-py-2.2.2 accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 bitsandbytes-0.45.5 certifi-2025.1.31 charset-normalizer-3.4.1 datasets-3.5.0 dill-0.3.8 dotenv-0.9.9 evaluate-0.4.3 filelock-3.18.0 frozenlist-1.6.0 fsspec-2024.12.0 huggingface-hub-0.30.2 idna-3.10 jinja2-3.1.6 joblib-1.4.2 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.16 networkx-3.4.2 nltk-3.9.1 numpy-2.2.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 peft-0.15.2 propcache-0.3.1 pyarrow-19.0.1 python-crfsuite-0.9.11 python-dotenv-1.1.0 pytz-2025.2 pyyaml-6.0.2 rapidfuzz-3.13.0 regex-2024.11.6 requests-2.32.3 rich-14.0.0 rouge_score-0.1.2 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-4.1.0 sympy-1.13.1 thefuzz-0.22.1 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.51.3 triton-3.2.0 trl-0.16.1 tzdata-2025.2 underthesea-6.8.4 underthesea-core-1.0.4 urllib3-2.4.0 xxhash-3.5.0 yarl-1.20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -U transformers trl peft datasets evaluate rouge_score underthesea bitsandbytes thefuzz sentence-transformers numpy dotenv\n",
        "# Tải tài nguyên NLTK cho METEOR\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1LRADR_xiIjl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LRADR_xiIjl",
        "outputId": "6b563c1a-fda4-4f07-e200-3cef8da70ce4"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14ddc4b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ddc4b6",
        "outputId": "d490c573-2ce6-404d-c9c2-338e596f6b1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/model/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from underthesea import word_tokenize\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, EarlyStoppingCallback\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fc0bf13f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0bf13f",
        "outputId": "d33edc49-fd95-4125-9f46-769b9af0efcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 2 files: 100%|██████████| 2/2 [02:06<00:00, 63.48s/it] \n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(50261, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=50261, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Tải biến môi trường từ file .env\n",
        "load_dotenv()\n",
        "\n",
        "# Đọc access token từ biến môi trường\n",
        "hf_token = os.getenv(\"HF_DAMA2\")\n",
        "if not hf_token:\n",
        "    raise ValueError(\"Không tìm thấy HF_DAMA2 trong file .env. Vui lòng thêm token vào file .env với định dạng: HF_DAMA2=your_token\")\n",
        "\n",
        "def load_model_and_tokenizer(quantization=\"int8\"):  # Thêm tham số quantization\n",
        "    model_name = \"vietgpt/dama-2-7b-chat\"\n",
        "\n",
        "    # Cấu hình quantization với bitsandbytes\n",
        "    if quantization == \"int8\":\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,  # Quantize thành INT8\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
        "            bnb_8bit_use_double_quant=True,  # Double quantization để tăng hiệu quả\n",
        "        )\n",
        "    elif quantization == \"int4\":\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,  # Quantize thành INT4\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # Dùng bfloat16 để tính toán\n",
        "            bnb_4bit_use_double_quant=True,  # Double quantization\n",
        "            bnb_4bit_quant_type=\"nf4\",  # Dùng NF4 (Normalized Float 4-bit) để tối ưu\n",
        "        )\n",
        "    else:\n",
        "        quantization_config = None  # Không quantize\n",
        "\n",
        "    # Tải tokenizer và mô hình với token xác thực\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        token=hf_token  # Sử dụng token từ biến môi trường\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token  # Sử dụng token từ biến môi trường\n",
        "    )\n",
        "    print(model)\n",
        "    # Cấu hình LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.5,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Thử thay đổi ở đây\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    return model, tokenizer, peft_config\n",
        "\n",
        "# Tải mô hình với INT8 quantization\n",
        "model, tokenizer, peft_config = load_model_and_tokenizer(quantization=\"int8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e0fadab1",
      "metadata": {
        "id": "e0fadab1"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa formatting_func với apply_chat_template\n",
        "def formatting_func(example):\n",
        "    if not all(k in example for k in ['input', 'output']):\n",
        "        print('Thiếu key trong example:', example)\n",
        "        return ''\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example['input']},\n",
        "        {\"role\": \"assistant\", \"content\": example['output']}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cc6ee4ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc6ee4ec",
        "outputId": "2b63813f-cece-4a50-86b9-5f3e801bb8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng giá trị duy nhất trong 'input' (exact): 1599\n",
            "Số lượng giá trị duy nhất trong 'output' (exact): 1659\n",
            "Tổng số hàng: 1700\n",
            "Số cặp input tương tự (>90%): 320\n",
            "Số cặp output tương tự (>90%): 41\n",
            "Số hàng sau khi xóa record tương tự: 1472\n",
            "Số lượng giá trị duy nhất trong 'input' (sau xử lý): 1472\n",
            "Số lượng giá trị duy nhất trong 'output' (sau xử lý): 1472\n",
            "                                                  input  \\\n",
            "998   tự kỷ có phải do dùng thiết bị điện tử nhiều k...   \n",
            "254                     trẻ nói nhại lại lời người khác   \n",
            "1074              tự kỷ có nên dùng thuốc an thần không   \n",
            "643    trẻ không biết tự khen ngợi bản thân khi làm tốt   \n",
            "1451                    trẻ 2 tuổi chỉ hét mà không nói   \n",
            "\n",
            "                                                 output  \n",
            "998   hiện chưa có bằng chứng khoa học cho thấy thiế...  \n",
            "254   việc trẻ lặp lại lời nói người khác thường xuy...  \n",
            "1074  không dùng thuốc an thần bừa bãi cho trẻ tự kỷ...  \n",
            "643   không ghi nhận thành công khiến trẻ thiếu tự t...  \n",
            "1451  nếu trẻ 2 tuổi thường xuyên hét không dùng từ ...  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from underthesea import word_tokenize\n",
        "from thefuzz import fuzz\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    text = \" \".join(tokens)\n",
        "    return text\n",
        "\n",
        "def extract_json_from_folder(folder_path):\n",
        "    dataset = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".json\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    json_data = json.load(f)\n",
        "                    for item in json_data:\n",
        "                        if all(k in item for k in ['input', 'output']):\n",
        "                            item['input'] = preprocess_text(item['input'])\n",
        "                            item['output'] = preprocess_text(item['output'])\n",
        "                            dataset.append(item)\n",
        "                        else:\n",
        "                            print(f\"Thiếu trường trong {filename}: {item}\")\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Không thể parse JSON từ {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Lỗi khi đọc {filename}: {e}\")\n",
        "    return dataset\n",
        "\n",
        "folder_path = \"/root/model/data\"\n",
        "dataset = extract_json_from_folder(folder_path)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "print(\"Số lượng giá trị duy nhất trong 'input' (exact):\", df['input'].nunique())\n",
        "print(\"Số lượng giá trị duy nhất trong 'output' (exact):\", df['output'].nunique())\n",
        "print(\"Tổng số hàng:\", len(df))\n",
        "\n",
        "# Fuzzy matching để tìm các record tương tự\n",
        "similarity_threshold = 90  # Ngưỡng độ tương đồng (90%)\n",
        "input_pairs = []\n",
        "output_pairs = []\n",
        "\n",
        "# Tìm các cặp input và output tương tự\n",
        "for i in range(len(df)):\n",
        "    for j in range(i + 1, len(df)):\n",
        "        input_sim = fuzz.ratio(df['input'].iloc[i], df['input'].iloc[j])\n",
        "        if input_sim >= similarity_threshold:\n",
        "            input_pairs.append((i, j, input_sim))\n",
        "        output_sim = fuzz.ratio(df['output'].iloc[i], df['output'].iloc[j])\n",
        "        if output_sim >= similarity_threshold:\n",
        "            output_pairs.append((i, j, output_sim))\n",
        "\n",
        "print(f\"Số cặp input tương tự (>{similarity_threshold}%):\", len(input_pairs))\n",
        "print(f\"Số cặp output tương tự (>{similarity_threshold}%):\", len(output_pairs))\n",
        "\n",
        "# Loại bỏ các record có input hoặc output tương tự (giữ record đầu tiên)\n",
        "indices_to_keep = set(range(len(df)))\n",
        "for i, j, _ in input_pairs:\n",
        "    if j in indices_to_keep:\n",
        "        indices_to_keep.remove(j)\n",
        "for i, j, _ in output_pairs:\n",
        "    if j in indices_to_keep:\n",
        "        indices_to_keep.remove(j)\n",
        "\n",
        "df = df.iloc[list(indices_to_keep)].reset_index(drop=True)\n",
        "print(\"Số hàng sau khi xóa record tương tự:\", len(df))\n",
        "\n",
        "# Kiểm tra lại độ unique\n",
        "print(\"Số lượng giá trị duy nhất trong 'input' (sau xử lý):\", df['input'].nunique())\n",
        "print(\"Số lượng giá trị duy nhất trong 'output' (sau xử lý):\", df['output'].nunique())\n",
        "\n",
        "# Chia train/validation\n",
        "full_dataset = Dataset.from_pandas(df[['input', 'output']])\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(train_df[['input', 'output']])\n",
        "eval_dataset = Dataset.from_pandas(eval_df[['input', 'output']])\n",
        "print(train_df[['input', 'output']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d3d58ec",
      "metadata": {
        "id": "3d3d58ec"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "def evaluate_metrics(predictions, references):\n",
        "    # Tải các độ đo\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "    # Tính các độ đo\n",
        "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
        "    meteor_results = meteor.compute(predictions=predictions, references=references)\n",
        "\n",
        "    # Tải mô hình nhúng câu để tính Cosine Similarity\n",
        "    embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "    # Tính embeddings cho dự đoán và tham chiếu\n",
        "    pred_embeddings = embedder.encode(predictions, convert_to_tensor=True)\n",
        "    ref_embeddings = embedder.encode(references, convert_to_tensor=True)\n",
        "\n",
        "    # Tính Cosine Similarity giữa từng cặp dự đoán-tham chiếu\n",
        "    cosine_scores = util.cos_sim(pred_embeddings, ref_embeddings)\n",
        "    # Lấy trung bình Cosine Similarity (chỉ lấy đường chéo chính, vì mỗi dự đoán chỉ so với tham chiếu tương ứng)\n",
        "    avg_cosine_similarity = np.mean([cosine_scores[i][i].item() for i in range(len(predictions))])\n",
        "    \n",
        "    # Gộp kết quả\n",
        "    metrics = {\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "        \"meteor\": meteor_results[\"meteor\"],\n",
        "        \"cosine_similarity\": avg_cosine_similarity,\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def generate_predictions(model, tokenizer, inputs, max_length=200):\n",
        "    \"\"\"Tạo dự đoán từ mô hình cho các đầu vào.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    # Đảm bảo pad_token được thiết lập\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    for input_text in inputs:\n",
        "        # Tiền xử lý input để đồng bộ với huấn luyện\n",
        "        input_text = preprocess_text(input_text)\n",
        "        prompt = f\"<s>[INST] {input_text} [/INST]\"\n",
        "        inputs_encoded = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs_encoded,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        pred = generated_text.split(\"[/INST]\")[-1].strip()\n",
        "        # Tiền xử lý dự đoán để đồng bộ với tham chiếu\n",
        "        pred = preprocess_text(pred)\n",
        "        predictions.append(pred)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7b126a89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2ae2a398a2a945d18ec5409bda99644a",
            "2d03ea85ed9e4262bfa21065ad73eae4",
            "5f8edfd1f93f4001b51bde9e815d66af",
            "f1a26a9520da4b888f8c54322ae29b82",
            "ca3c86d9cf48410199515ce86fd68eaf",
            "3fd2597c60874a4f9c920f8420b366f0",
            "cd056a6bb14048bdbc7860ce2c06303f",
            "d98a8f43fc76406897ffbb66f7e136c0",
            "b8f7022d52464b0eb74f965319229cef",
            "8983e348f59749bd86146b513c5acacf",
            "551f5413b209400fa23bedb10c4cde9f",
            "046403be22d543dab3c5d9c3d3836b9e",
            "788d9b117db34ff6b06e1fd6a942c39c",
            "916b489caeac4041839f1ee2365360a3",
            "c8d0620a47874f34b94681c71f35c3c8",
            "57e0a819a11b42e0a35b75e072b2dbb1",
            "ba961b0dab1a4ac6864646de1d46d9e6",
            "442e0cfbeb554fd58e2bf0c2d77829c0",
            "a0da77700eab4ab0bb06e1fe7b5184e9",
            "69fe2be7c9814f2ab93168f3d6ac45ae",
            "841bc431816441a4bc79dde9600f9176",
            "5b9cb618b32044518965997a96a7d930",
            "9c6f2000b243485db70d65e993621d3c",
            "c778736f27e349edb304160691210f4b",
            "076e2be9ac414d6d92b90f203da87c38",
            "0ac46097afa5445591d9a3fbad639ae5",
            "4375f815d6724658a2cfaa6c608e478f",
            "79f8dacb371b4fd987b8a4330520cc04",
            "d990e34729bf43eca69270c1fd3dc8cb",
            "2d402147806d413590c8bbbcd89646de",
            "edf820a16c044fe0a2d2f85b11b64683",
            "6f4b1cc17a48461f8e2da6dc9b03c82a",
            "4d0cb99e715847d0b05a031736ecd688",
            "b3fbfe3f3f604b698e7d869437c0d5fe",
            "246d63e4a14841c594db092689f2a64c",
            "c6909db59ff04666b867097216de70ba",
            "1ffc5523dd084452a20ae6c913d39e47",
            "35fcfd38ee344fd0924e883972074784",
            "bb370caba2b844c486d30748e6caf9e0",
            "73e04b4d775848a19e267669303d8266",
            "4526ca6058b74e11ba4329d843d6a9d4",
            "ba9f2bb92edf477d94d6cfa1e4950711",
            "9e320bdd7a7c484183ed8b93e6705af6",
            "cc5fad4a005e4aa9bcaecccb8217df1e",
            "220a0b50b8d04e3dac9bbaf89de8a7c9",
            "1201dfd96b7d4fcabf5b10b3fa769759",
            "98ae3ea018e24dabb90618b485a2b2cb",
            "85f4e94e397f43a588a527fe41bc015a",
            "686120f3641f43298071d6a5e32fe644",
            "efc1ff78f5f546638b298419a4151695",
            "0926abb88e2e4381b4b38839e6f652f5",
            "58850dfce0cc4811ba06ab1ea71d306d",
            "634e691c8c8a4fb68c5af7704b9ce0fe",
            "5c5ac7432fae43f2a17870b83e19a17a",
            "d224187c0f7340279949cddca91d3ecb",
            "079359b31bb04296a232a84fc1b7db2d",
            "ff5faacf70a04f19833d41589b7a78e5",
            "e0d2e5d548e04e2ebcf6b5a79083f0dc",
            "7ac0a7531ecc4823b33549eeff34a131",
            "970af49c4f1940758130a5e334901a14",
            "83a4b7f740ec41389a6df6317af60615",
            "d43bfd1e65ae43af97b7fa6b44cf9023",
            "e2283973a0de470a88d5bb8b78160982",
            "65597da15fda4b51b7aa859b380c12db",
            "6d044e9840e449c5b1e718fb06e99515",
            "d00e51ae50d144e392bf12aa8b54b303",
            "4adcb8502fce4dd1af5b8a5b648fa263",
            "9b47975658384af1a549c9f1acf1e5a5",
            "56eff52cba7c4e06a6378805f2aa2707",
            "4a251249b63e4688ba357a8d412a3b69",
            "55fdfa966042462bb35ee44cfbfb841e",
            "23b95f393dd940039de764bd67d7709a",
            "c68779b0088e4f1c867ef1be26e7112b",
            "9ea9a076bce947bab5096a028f7b3237",
            "c034029a3d5e48ebaf7ea25fa7cb96fd",
            "c1267d7977354e5aa3db8a28f511f2b8",
            "9e526d55e8f84c66ab040536bc502d2b",
            "90fadb3fb3e14b1baaca9e77f571a80e",
            "e2e6f823b0224c52a44e1e798e5c1985",
            "4eaab0584d164139b520cd15eaa3793f",
            "943be389e1a947ad99ec7d4d76de6db0",
            "2c01848e500843e39f326eeb3b0a8cce",
            "8cd9559471a9418d8a134b43c6dc1258",
            "dc6578ccbf9740bfb315a07b6482d8bf",
            "187ce14699ae454c819ceacc050a955f",
            "a08c13a1cfcb4fd295994b99f788d6c3",
            "a81481e6777b4ab78442363553b73381",
            "ad666ee0fe7e48de9c4e934212bd337c",
            "cbbcc5aca91d46ba9b51859494da828b",
            "494b39f1fcf9434b88559980e35c006f",
            "f849b4139b5d4ab3a2ae32ae83807160",
            "1807262027f34e70b1e94883ac402b39",
            "c3d35975670546ab9998a6389e3ec707",
            "6a974c0a2ec043219c538076bfffc87a",
            "79f22a64d0fa4310b9ee63bbfb66b41a",
            "8fcd62139ad44297b2a950d97850468f",
            "a7ee09b72b034234b02194ab94471a2a",
            "a66367d4522a4b1eb6ce7b88394f8e30",
            "a0041fc725964c50a3beff504ddeb9ab",
            "e2627486d9ba4848830b0687cbe8b3d3",
            "55b1ad5ece2c4be18a2acdb322b93c01",
            "e502fdc27ded43fcbbc4d3fe3fac8030",
            "3d7b45cd5bfa4ad9ba035ac29882fb8b",
            "c8ad9a852eec438e972cadc8ea1f0fe2",
            "5fc18b5fb2e349afa2489a0adf1b21fc",
            "2bd1218e781f4784be96419b838c45af",
            "9419d62d8de24bce9697c0f60467ea99",
            "43b4ce75564943ca8b250e54a7ce1834",
            "80acce4d1d5c4459ab794ba6bbcc32cf",
            "fa04a7d89df34dfcbd73d62f4dda799c"
          ]
        },
        "id": "7b126a89",
        "outputId": "b0106b8a-df18-45c5-a8b4-bae598edea1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Bắt đầu chạy đơn ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 10558.04 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 30976.71 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 30332.81 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 3359.22 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 318784.44 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 9996.93 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 24154.61 examples/s]\n",
            "Applying chat template to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 23635.98 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 3290.95 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 115110.21 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,177\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 79,953,920\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 1177, Eval dataset size: 295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/model/venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/219 16:54 < 06:18, 0.16 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.374400</td>\n",
              "      <td>4.475065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.061000</td>\n",
              "      <td>3.462521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.039500</td>\n",
              "      <td>2.622163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.319200</td>\n",
              "      <td>2.050639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.962600</td>\n",
              "      <td>1.875167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.890300</td>\n",
              "      <td>1.809317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.725800</td>\n",
              "      <td>1.746339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.685000</td>\n",
              "      <td>1.713642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.644300</td>\n",
              "      <td>1.684539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.638000</td>\n",
              "      <td>1.663903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.566500</td>\n",
              "      <td>1.638024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.516200</td>\n",
              "      <td>1.622251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.544200</td>\n",
              "      <td>1.608509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.524700</td>\n",
              "      <td>1.599633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.516200</td>\n",
              "      <td>1.592216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.428800</td>\n",
              "      <td>1.587849</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_single_dama-2-7b-chat/checkpoint-100\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6806775d-4098f15062e0080f7e6954c0;ddaf9e41-c709-4644-9baa-cf17e79891f4)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_single_dama-2-7b-chat/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_single_dama-2-7b-chat/checkpoint-100/special_tokens_map.json\n",
            "/root/model/venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_single_dama-2-7b-chat/checkpoint-100 (score: 1.587849497795105).\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680678dc-7446042946b01f41517d5aa0;21ae69f9-ea39-4ee9-9213-8b7f0b36bd6a)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_dama-2-7b-chat_single/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_dama-2-7b-chat_single/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 5.11MB/s]\n",
            "Downloading builder script: 100%|██████████| 7.02k/7.02k [00:00<00:00, 8.43MB/s]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "loading file vocab.txt from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Single Run Metrics for vietgpt/dama-2-7b-chat: {'rouge1': np.float64(0.615354014833372), 'rouge2': np.float64(0.279524855165236), 'rougeL': np.float64(0.3940296014840703), 'meteor': np.float64(0.32146654101083755), 'cosine_similarity': np.float64(0.7181124540708833)}\n",
            "Đã lưu metrics chạy đơn vào: single_run_metrics_dama-2-7b-chat.json\n",
            "Thời gian chạy đơn: 2460.00 giây\n",
            "Input: trẻ không biết sử dụng hình ảnh minh họa kết quả hoạt động\n",
            "Prediction: trẻ không biết sử dụng hình ảnh minh họa kết quả hoạt động không dùng hình ảnh minh họa là thiếu kỹ năng trình bày – nên luyện qua tranh ảnh hoặc video ngắn\n",
            "Reference: không chụp hình hoặc chọn hình mờ nhạt – nên luyện chọn hình rõ người – rõ hành động – có cảm xúc\n",
            "\n",
            "Input: trẻ không dùng tay để chỉ vật\n",
            "Prediction: trẻ không dùng tay để chỉ vật không chỉ tay là thiếu kỹ năng chỉ – đây là dấu hiệu chậm phát triển ngôn ngữ\n",
            "Reference: việc trẻ không dùng tay để chỉ khi muốn thể hiện nhu cầu là dấu hiệu quan trọng để nhận biết trẻ có khó khăn trong giao tiếp phi ngôn ngữ\n",
            "\n",
            "Input: trẻ tự kỷ có học được cách tự kiểm tra sức khỏe không\n",
            "Prediction: trẻ tự kỷ có học được cách tự kiểm tra sức khỏe không có trẻ tự kỷ có thể học cách tự kiểm tra sức khỏe như đo nhiệt độ nhịp tim và nhịp thở nếu được hướng dẫn rõ ràng và lặp lại nhiều lần\n",
            "Reference: có trẻ có thể học nhận biết các dấu hiệu bất thường của cơ thể như sốt đau mệt dạy bằng hình ảnh mô hình và mô phỏng giúp trẻ biết khi nào cần báo cho người lớn hoặc đi khám\n",
            "\n",
            "Input: những dấu hiệu của trẻ chậm nói\n",
            "Prediction: những dấu hiệu của trẻ chậm nói trẻ chậm nói có thể có những biểu hiện như không nói được từ đơn khi 18 tháng không nói được câu đơn khi 24 tháng không nói được câu đơn khi 30 tháng không nói được câu đơn\n",
            "Reference: những trẻ không có tiến bộ về lời nói ít tương tác bằng mắt và không phát triển vốn từ theo tháng tuổi là dấu hiệu cần được kiểm tra sớm\n",
            "\n",
            "Input: tôi cần tư vấn\n",
            "Prediction: tôi cần tư vấn bạn có thể để lại tin nhắn hoặc gọi điện cho chúng tôi qua số điện thoại hoặc fanpage\n",
            "Reference: bạn có thể chia sẻ cụ thể hơn để mình hỗ trợ đúng nội dung bạn đang cần nhé\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Loại bỏ cột không cần thiết để tránh cảnh báo\n",
        "train_dataset = train_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_dataset.column_names else [])\n",
        "eval_dataset = eval_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_dataset.column_names else [])\n",
        "\n",
        "model_name = \"vietgpt/dama-2-7b-chat\"\n",
        "\n",
        "# Đo thời gian chạy đơn\n",
        "print(\"\\n=== Bắt đầu chạy đơn ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Cấu hình huấn luyện cho Single Run\n",
        "training_arguments_single = TrainingArguments(\n",
        "    output_dir=f\"./results_single_{model_name.split('/')[-1]}\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.1,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.1,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    log_level=\"info\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Huấn luyện Single Run\n",
        "trainer_single = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments_single,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_func,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.01,\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "print(f\"Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}\")\n",
        "trainer_single.train()\n",
        "\n",
        "# Lưu mô hình Single Run\n",
        "model.save_pretrained(f\"./finetuned_{model_name.split('/')[-1]}_single\")\n",
        "tokenizer.save_pretrained(f\"./finetuned_{model_name.split('/')[-1]}_single\")\n",
        "\n",
        "# Đánh giá Single Run\n",
        "test_inputs = eval_df['input'].tolist()\n",
        "test_references = eval_df['output'].tolist()\n",
        "predictions_single = generate_predictions(model, tokenizer, test_inputs)\n",
        "metrics_single = evaluate_metrics(predictions_single, test_references)\n",
        "print(f\"Single Run Metrics for {model_name}:\", metrics_single)\n",
        "\n",
        "# Lưu metrics vào file\n",
        "metrics_file = f\"single_run_metrics_{model_name.split('/')[-1]}.json\"\n",
        "try:\n",
        "    with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics_single, f, indent=4, ensure_ascii=False)\n",
        "    print(f\"Đã lưu metrics chạy đơn vào: {metrics_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi lưu metrics chạy đơn: {e}\")\n",
        "\n",
        "# In thời gian chạy\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "print(f\"Thời gian chạy đơn: {duration:.2f} giây\")\n",
        "\n",
        "# Kiểm tra mẫu dự đoán\n",
        "for i in range(5):\n",
        "    print(f\"Input: {test_inputs[i]}\")\n",
        "    print(f\"Prediction: {predictions_single[i]}\")\n",
        "    print(f\"Reference: {test_references[i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "18c38bce",
      "metadata": {
        "id": "18c38bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50261\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/pytorch_model.bin.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Attempting to create safetensors variant\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.73s/it]\n",
            "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vietgpt/dama-2-7b-chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(50261, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=50261, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 - Train size: 1177, Eval size: 295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 9423.06 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 25279.18 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23874.03 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 2771.47 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 307256.85 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 9266.23 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 22517.60 examples/s]\n",
            "Applying chat template to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 22492.22 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 2830.88 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 104090.16 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,177\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 79,953,920\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [170/219 15:12 < 04:26, 0.18 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.580200</td>\n",
              "      <td>3.740267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.563000</td>\n",
              "      <td>3.149174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.756900</td>\n",
              "      <td>2.333185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.137000</td>\n",
              "      <td>2.020281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.902800</td>\n",
              "      <td>1.868671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.818000</td>\n",
              "      <td>1.810463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.777700</td>\n",
              "      <td>1.754960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.672300</td>\n",
              "      <td>1.726559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.653300</td>\n",
              "      <td>1.695835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.612500</td>\n",
              "      <td>1.668896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.544700</td>\n",
              "      <td>1.653354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.604200</td>\n",
              "      <td>1.635903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.537200</td>\n",
              "      <td>1.618640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.536600</td>\n",
              "      <td>1.607549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.508400</td>\n",
              "      <td>1.600648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.397700</td>\n",
              "      <td>1.600758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.422200</td>\n",
              "      <td>1.595713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_dama-2-7b-chatfold_1/checkpoint-100\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680682be-4a0d73f725cb33ef64c01438;ab703cfb-8c72-4ad3-9d2a-15d06ab6176d)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_dama-2-7b-chatfold_1/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_dama-2-7b-chatfold_1/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_dama-2-7b-chatfold_1/checkpoint-100 (score: 1.5957129001617432).\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6806843c-5d59610a6db403b121683c23;0e317a51-bb76-4ade-a629-3329ae122ade)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_1/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_1/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Metrics: {'rouge1': np.float64(0.613427265402745), 'rouge2': np.float64(0.27563172873584285), 'rougeL': np.float64(0.394659465311162), 'meteor': np.float64(0.32624372139481167), 'cosine_similarity': np.float64(0.7175719186411066)}\n",
            "Thời gian chạy Fold 1: 1978.48 giây\n",
            "\n",
            "Training Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50261\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/pytorch_model.bin.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Attempting to create safetensors variant\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.93s/it]\n",
            "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vietgpt/dama-2-7b-chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(50261, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=50261, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 - Train size: 1177, Eval size: 295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 8743.22 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1177/1177 [00:00<00:00, 21321.23 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 23464.61 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 2543.07 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1177/1177 [00:00<00:00, 243510.87 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 8508.83 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 295/295 [00:00<00:00, 18531.90 examples/s]\n",
            "Applying chat template to eval dataset: 100%|██████████| 295/295 [00:00<00:00, 18814.83 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 295/295 [00:00<00:00, 2252.98 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 295/295 [00:00<00:00, 89941.10 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,177\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 79,953,920\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [170/219 14:57 < 04:21, 0.19 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.519200</td>\n",
              "      <td>3.756752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.545500</td>\n",
              "      <td>3.138627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.728700</td>\n",
              "      <td>2.311746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.140400</td>\n",
              "      <td>2.011232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.971800</td>\n",
              "      <td>1.872389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.775700</td>\n",
              "      <td>1.807349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.754400</td>\n",
              "      <td>1.763428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.728900</td>\n",
              "      <td>1.728989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.583600</td>\n",
              "      <td>1.697502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.562400</td>\n",
              "      <td>1.669949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.584600</td>\n",
              "      <td>1.646991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.552600</td>\n",
              "      <td>1.636358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.559900</td>\n",
              "      <td>1.620477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.543100</td>\n",
              "      <td>1.607703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.579800</td>\n",
              "      <td>1.601414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.426000</td>\n",
              "      <td>1.596829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.401600</td>\n",
              "      <td>1.594669</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_dama-2-7b-chatfold_2/checkpoint-100\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68068a7c-637d2414361690577fe8fc64;f2764dfa-56e6-4702-8d5b-a29574622906)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_dama-2-7b-chatfold_2/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_dama-2-7b-chatfold_2/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 295\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_dama-2-7b-chatfold_2/checkpoint-100 (score: 1.5946685075759888).\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68068beb-44405d1364d00c1e23fdedde;efb105d7-7f4c-41e0-82bc-6a50dc5cd97a)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_2/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_2/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Metrics: {'rouge1': np.float64(0.6141435779495137), 'rouge2': np.float64(0.28554587349212524), 'rougeL': np.float64(0.39785513077999046), 'meteor': np.float64(0.33962357426447026), 'cosine_similarity': np.float64(0.7299524540618315)}\n",
            "Thời gian chạy Fold 2: 1925.74 giây\n",
            "\n",
            "Training Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50261\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/pytorch_model.bin.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Attempting to create safetensors variant\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.48s/it]\n",
            "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vietgpt/dama-2-7b-chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(50261, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=50261, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 - Train size: 1178, Eval size: 294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 10668.57 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1178/1178 [00:00<00:00, 31203.32 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 31369.74 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 3352.22 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 307059.23 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 10260.40 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 27916.45 examples/s]\n",
            "Applying chat template to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 23961.86 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 3335.21 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 130835.58 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,178\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 79,953,920\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/219 14:02 < 05:14, 0.19 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.621900</td>\n",
              "      <td>3.747000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.565000</td>\n",
              "      <td>3.146334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.727700</td>\n",
              "      <td>2.382592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.172900</td>\n",
              "      <td>2.047302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.894500</td>\n",
              "      <td>1.905340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.791200</td>\n",
              "      <td>1.854760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.778900</td>\n",
              "      <td>1.807153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.667600</td>\n",
              "      <td>1.761982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.618600</td>\n",
              "      <td>1.720284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.650100</td>\n",
              "      <td>1.692319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.605900</td>\n",
              "      <td>1.673787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.521600</td>\n",
              "      <td>1.653455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.555500</td>\n",
              "      <td>1.640982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.514800</td>\n",
              "      <td>1.631497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.504900</td>\n",
              "      <td>1.621621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.393700</td>\n",
              "      <td>1.619655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_dama-2-7b-chatfold_3/checkpoint-100\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-680691ed-45f5f7825a4c1fb20a3067c3;867edd5f-4db5-4f07-a790-e6aa6fb463d5)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_dama-2-7b-chatfold_3/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_dama-2-7b-chatfold_3/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_dama-2-7b-chatfold_3/checkpoint-100 (score: 1.6196552515029907).\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68069333-7abc7e432747a94a2247f40e;8253503c-08b6-4a01-b7e5-ac50de11a20b)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_3/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_3/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Metrics: {'rouge1': np.float64(0.6040820324205936), 'rouge2': np.float64(0.27843056198650673), 'rougeL': np.float64(0.392487013080095), 'meteor': np.float64(0.31949058737054353), 'cosine_similarity': np.float64(0.7218702684352998)}\n",
            "Thời gian chạy Fold 3: 1807.01 giây\n",
            "\n",
            "Training Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50261\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/pytorch_model.bin.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Attempting to create safetensors variant\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]\n",
            "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vietgpt/dama-2-7b-chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(50261, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=50261, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 - Train size: 1178, Eval size: 294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 10579.95 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1178/1178 [00:00<00:00, 31750.94 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 31016.65 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 3381.11 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 300303.29 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 9670.59 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 28055.55 examples/s]\n",
            "Applying chat template to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 24599.04 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 3317.16 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 99566.04 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,178\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 79,953,920\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='170' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [170/219 14:54 < 04:20, 0.19 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.632500</td>\n",
              "      <td>3.735242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.494700</td>\n",
              "      <td>3.136047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.710700</td>\n",
              "      <td>2.367109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.171800</td>\n",
              "      <td>2.029018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.934200</td>\n",
              "      <td>1.899366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.801200</td>\n",
              "      <td>1.843770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.827800</td>\n",
              "      <td>1.784296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.666500</td>\n",
              "      <td>1.739207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.602700</td>\n",
              "      <td>1.707266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.616400</td>\n",
              "      <td>1.683488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.604700</td>\n",
              "      <td>1.653214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>1.640912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.573600</td>\n",
              "      <td>1.624200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.503300</td>\n",
              "      <td>1.613176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.520200</td>\n",
              "      <td>1.604225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.382800</td>\n",
              "      <td>1.600288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.424400</td>\n",
              "      <td>1.599025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_dama-2-7b-chatfold_4/checkpoint-100\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68069907-6f8ea07a4e9a02c06223b7f1;10cff8da-2aff-4622-881b-97c59e42721c)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_dama-2-7b-chatfold_4/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_dama-2-7b-chatfold_4/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_dama-2-7b-chatfold_4/checkpoint-100 (score: 1.5990251302719116).\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-68069a76-68e06cb20cb926e639b9497b;a58a4dfb-cf7c-413e-9f18-58b3d54095ef)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_4/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_4/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Metrics: {'rouge1': np.float64(0.6027767146836853), 'rouge2': np.float64(0.28422386868121075), 'rougeL': np.float64(0.39384037330077515), 'meteor': np.float64(0.3274491233442434), 'cosine_similarity': np.float64(0.7114187382719144)}\n",
            "Thời gian chạy Fold 4: 1939.22 giây\n",
            "\n",
            "Training Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50261\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/pytorch_model.bin.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Attempting to create safetensors variant\n",
            "Attempting to convert .bin model on the fly to safetensors.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.84s/it]\n",
            "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "All the weights of LlamaForCausalLM were initialized from the model checkpoint at vietgpt/dama-2-7b-chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(50261, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=50261, bias=False)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 - Train size: 1178, Eval size: 294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Applying formatting function to train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 9305.88 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 1178/1178 [00:00<00:00, 22971.27 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 23242.39 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 2887.05 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 1178/1178 [00:00<00:00, 301752.17 examples/s]\n",
            "Applying formatting function to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 9121.49 examples/s]\n",
            "Converting eval dataset to ChatML: 100%|██████████| 294/294 [00:00<00:00, 22802.72 examples/s]\n",
            "Applying chat template to eval dataset: 100%|██████████| 294/294 [00:00<00:00, 21699.26 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 294/294 [00:00<00:00, 2978.00 examples/s]\n",
            "Truncating eval dataset: 100%|██████████| 294/294 [00:00<00:00, 80067.88 examples/s]\n",
            "Using auto half precision backend\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 1,178\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 219\n",
            "  Number of trainable parameters = 79,953,920\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/219 15:36 < 05:49, 0.17 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.674500</td>\n",
              "      <td>3.779777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.481100</td>\n",
              "      <td>3.175684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.705400</td>\n",
              "      <td>2.360362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.168600</td>\n",
              "      <td>2.058716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.954200</td>\n",
              "      <td>1.908956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.816400</td>\n",
              "      <td>1.846899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.734700</td>\n",
              "      <td>1.782158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.737400</td>\n",
              "      <td>1.737031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.610600</td>\n",
              "      <td>1.712836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.577400</td>\n",
              "      <td>1.685128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.605100</td>\n",
              "      <td>1.666394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.524700</td>\n",
              "      <td>1.644840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.576500</td>\n",
              "      <td>1.631921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.499800</td>\n",
              "      <td>1.622378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.464900</td>\n",
              "      <td>1.612833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.609152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results_dama-2-7b-chatfold_5/checkpoint-100\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6806a0fb-315308364895fefb732ba900;cd24f209-80e0-4fd1-9e5d-9b9769172634)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./results_dama-2-7b-chatfold_5/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_dama-2-7b-chatfold_5/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, output, text. If input, output, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 294\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_dama-2-7b-chatfold_5/checkpoint-100 (score: 1.6091524362564087).\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6806a23a-67fd115a357b8d6551ca815c;5bd5d4bc-4ae9-4451-b62d-5c94a45ab73a)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/vietgpt/dama-2-7b-chat/resolve/main/config.json.\n",
            "Access to model vietgpt/dama-2-7b-chat is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in vietgpt/dama-2-7b-chat.\n",
            "  warnings.warn(\n",
            "/root/model/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in vietgpt/dama-2-7b-chat - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in ./finetuned_vistral_fold_5/tokenizer_config.json\n",
            "Special tokens file saved in ./finetuned_vistral_fold_5/special_tokens_map.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at None\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Metrics: {'rouge1': np.float64(0.6186252424333942), 'rouge2': np.float64(0.2930516278453371), 'rougeL': np.float64(0.3991195690132021), 'meteor': np.float64(0.3510191560313527), 'cosine_similarity': np.float64(0.7293402220116181)}\n",
            "Thời gian chạy Fold 5: 2067.52 giây\n",
            "\n",
            "=== Kết thúc huấn luyện K-Fold ===\n",
            "Tổng thời gian chạy: 9721.49 giây\n",
            "Thời gian trung bình mỗi fold: 1943.59 giây\n",
            "\n",
            "Average Cross-Validation Metrics: {'rouge1': np.float64(0.6106109665779863), 'rouge2': np.float64(0.2833767321482045), 'rougeL': np.float64(0.39559231029704495), 'meteor': np.float64(0.3327652324810843), 'cosine_similarity': np.float64(0.7220307202843541)}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Cấu hình KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_metrics = []\n",
        "fold_models = []\n",
        "fold_times = []\n",
        "total_start_time = time.time()\n",
        "\n",
        "# Lặp qua từng fold\n",
        "for fold, (train_idx, eval_idx) in enumerate(kf.split(df)):\n",
        "    print(f\"\\nTraining Fold {fold + 1}...\")\n",
        "    fold_start_time = time.time()\n",
        "\n",
        "    # Tạo tập train và eval cho fold hiện tại\n",
        "    train_fold = df.iloc[train_idx][['input', 'output']]\n",
        "    eval_fold = df.iloc[eval_idx][['input', 'output']]\n",
        "    train_fold_dataset = Dataset.from_pandas(train_fold)\n",
        "    eval_fold_dataset = Dataset.from_pandas(eval_fold)\n",
        "\n",
        "    # Loại bỏ cột không cần thiết\n",
        "    train_fold_dataset = train_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in train_fold_dataset.column_names else [])\n",
        "    eval_fold_dataset = eval_fold_dataset.remove_columns(['__index_level_0__'] if '__index_level_0__' in eval_fold_dataset.column_names else [])\n",
        "\n",
        "    # Tải lại mô hình gốc với INT4 quantization\n",
        "    model, tokenizer, peft_config = load_model_and_tokenizer(quantization=\"int4\")\n",
        "    print(f\"Fold {fold + 1} - Train size: {len(train_fold_dataset)}, Eval size: {len(eval_fold_dataset)}\")\n",
        "\n",
        "    # Cấu hình huấn luyện cho fold\n",
        "    training_arguments_fold = TrainingArguments(\n",
        "        output_dir=f\"./results_{model_name.split('/')[-1]}fold_{fold + 1}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        save_steps=100,\n",
        "        logging_steps=10,\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.1,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        max_grad_norm=0.3,\n",
        "        warmup_ratio=0.1,\n",
        "        group_by_length=True,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        logging_strategy=\"steps\",\n",
        "        log_level=\"info\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "    )\n",
        "\n",
        "    # Huấn luyện fold\n",
        "    trainer_fold = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_arguments_fold,\n",
        "        train_dataset=train_fold_dataset,\n",
        "        eval_dataset=eval_fold_dataset,\n",
        "        peft_config=peft_config,\n",
        "        formatting_func=formatting_func,\n",
        "        callbacks=[\n",
        "            EarlyStoppingCallback(\n",
        "                early_stopping_patience=3,\n",
        "                early_stopping_threshold=0.01,\n",
        "            )\n",
        "        ],\n",
        "    )\n",
        "    trainer_fold.train()\n",
        "\n",
        "    # Lưu mô hình fold\n",
        "    fold_path = f\"./finetuned_vistral_fold_{fold + 1}\"\n",
        "#pragma warning disable format\n",
        "    model.save_pretrained(fold_path)\n",
        "    tokenizer.save_pretrained(fold_path)\n",
        "    fold_models.append(fold_path)\n",
        "\n",
        "    # Đánh giá fold\n",
        "    test_inputs_fold = eval_fold['input'].tolist()\n",
        "    test_references_fold = eval_fold['output'].tolist()\n",
        "    predictions_fold = generate_predictions(model, tokenizer, test_inputs_fold)\n",
        "    metrics_fold = evaluate_metrics(predictions_fold, test_references_fold)\n",
        "    print(f\"Fold {fold + 1} Metrics:\", metrics_fold)\n",
        "    fold_metrics.append(metrics_fold)\n",
        "\n",
        "    # Lưu metrics của fold\n",
        "    with open(f\"fold_{fold + 1}_metrics_vistral.json\", \"w\") as f:\n",
        "        json.dump(metrics_fold, f, indent=4)\n",
        "\n",
        "    # Đo thời gian chạy của fold\n",
        "    fold_end_time = time.time()\n",
        "    fold_duration = fold_end_time - fold_start_time\n",
        "    fold_times.append(fold_duration)\n",
        "    print(f\"Thời gian chạy Fold {fold + 1}: {fold_duration:.2f} giây\")\n",
        "\n",
        "    # Dọn dẹp bộ nhớ\n",
        "    del model, trainer_fold\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Tính tổng thời gian và thời gian trung bình\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "print(f\"\\n=== Kết thúc huấn luyện K-Fold ===\")\n",
        "print(f\"Tổng thời gian chạy: {total_duration:.2f} giây\")\n",
        "print(f\"Thời gian trung bình mỗi fold: {np.mean(fold_times):.2f} giây\")\n",
        "\n",
        "# Tính trung bình metrics qua các fold\n",
        "avg_metrics = {\n",
        "    \"rouge1\": np.mean([m[\"rouge1\"] for m in fold_metrics]),\n",
        "    \"rouge2\": np.mean([m[\"rouge2\"] for m in fold_metrics]),\n",
        "    \"rougeL\": np.mean([m[\"rougeL\"] for m in fold_metrics]),\n",
        "    \"meteor\": np.mean([m[\"meteor\"] for m in fold_metrics]),\n",
        "    \"cosine_similarity\": np.mean([m[\"cosine_similarity\"] for m in fold_metrics]),\n",
        "}\n",
        "print(\"\\nAverage Cross-Validation Metrics:\", avg_metrics)\n",
        "\n",
        "# Lưu metrics trung bình\n",
        "with open(\"cross_validation_metrics.json\", \"w\") as f:\n",
        "    json.dump(avg_metrics, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b58f93a",
      "metadata": {
        "id": "6b58f93a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer.json\n",
            "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/added_tokens.json\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50261\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vietgpt--dama-2-7b-chat/snapshots/efacdb053b58e26fe1de9e6b6faf0224367f78a1/pytorch_model.bin.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\n",
            "Attempting to create safetensors variant\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Tải mô hình gốc với INT8 quantization\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m base_model, tokenizer, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Kiểm tra xem có mô hình fold nào để gộp không\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fold_models:\n",
            "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(quantization)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Tải tokenizer và mô hình với token xác thực\u001b[39;00m\n\u001b[1;32m     36\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     37\u001b[0m     model_name,\n\u001b[1;32m     38\u001b[0m     token\u001b[38;5;241m=\u001b[39mhf_token  \u001b[38;5;66;03m# Sử dụng token từ biến môi trường\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_token\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Sử dụng token từ biến môi trường\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Cấu hình LoRA\u001b[39;00m\n",
            "File \u001b[0;32m~/model/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
            "File \u001b[0;32m~/model/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
            "File \u001b[0;32m~/model/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4380\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[1;32m   4379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4380\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4382\u001b[0m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[1;32m   4383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
            "File \u001b[0;32m~/model/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1304\u001b[0m, in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     tied_params \u001b[38;5;241m=\u001b[39m find_tied_parameters(model)\n",
            "File \u001b[0;32m~/model/venv/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:101\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.37.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 8bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attempting to convert .bin model on the fly to safetensors.\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import copy\n",
        "\n",
        "# Tải mô hình gốc với INT8 quantization\n",
        "base_model, tokenizer, _ = load_model_and_tokenizer(quantization=\"int8\")\n",
        "\n",
        "# Kiểm tra xem có mô hình fold nào để gộp không\n",
        "if not fold_models:\n",
        "    raise ValueError(\"No fold models found to merge.\")\n",
        "\n",
        "# Thu thập và trung bình trọng số LoRA từ các fold\n",
        "state_dicts = []\n",
        "for fold_path in fold_models:\n",
        "    fold_model = PeftModel.from_pretrained(base_model, fold_path, is_trainable=False)\n",
        "    state_dicts.append(copy.deepcopy(fold_model.state_dict()))\n",
        "    del fold_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Trung bình trọng số\n",
        "averaged_state_dict = state_dicts[0]\n",
        "for key in averaged_state_dict:\n",
        "    for i in range(1, len(state_dicts)):\n",
        "        averaged_state_dict[key] += state_dicts[i][key]\n",
        "    averaged_state_dict[key] /= len(state_dicts)\n",
        "\n",
        "# Tải trọng số trung bình vào mô hình gốc\n",
        "base_model.load_state_dict(averaged_state_dict, strict=False)\n",
        "\n",
        "# Lưu mô hình gộp\n",
        "merged_model_path = \"./finetuned_dama_2_merged\"\n",
        "base_model.save_pretrained(merged_model_path)\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "# Dọn dẹp bộ nhớ\n",
        "del base_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Tải mô hình gộp để đánh giá với INT8 quantization\n",
        "merged_model = AutoModelForCausalLM.from_pretrained(\n",
        "    merged_model_path,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_8bit_use_double_quant=True\n",
        "    ),\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Đánh giá mô hình gộp trên tập validation\n",
        "test_inputs = eval_df['input'].tolist()\n",
        "test_references = eval_df['output'].tolist()\n",
        "predictions_merged = generate_predictions(merged_model, tokenizer, test_inputs)\n",
        "metrics_merged = evaluate_metrics(predictions_merged, test_references)\n",
        "print(\"Merged Model Metrics:\", metrics_merged)\n",
        "\n",
        "# Lưu metrics của mô hình gộp\n",
        "with open(\"merged_model_metrics_dama_2.json\", \"w\") as f:\n",
        "    json.dump(metrics_merged, f, indent=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "046403be22d543dab3c5d9c3d3836b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_788d9b117db34ff6b06e1fd6a942c39c",
              "IPY_MODEL_916b489caeac4041839f1ee2365360a3",
              "IPY_MODEL_c8d0620a47874f34b94681c71f35c3c8"
            ],
            "layout": "IPY_MODEL_57e0a819a11b42e0a35b75e072b2dbb1"
          }
        },
        "076e2be9ac414d6d92b90f203da87c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d402147806d413590c8bbbcd89646de",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edf820a16c044fe0a2d2f85b11b64683",
            "value": 1176
          }
        },
        "079359b31bb04296a232a84fc1b7db2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff5faacf70a04f19833d41589b7a78e5",
              "IPY_MODEL_e0d2e5d548e04e2ebcf6b5a79083f0dc",
              "IPY_MODEL_7ac0a7531ecc4823b33549eeff34a131"
            ],
            "layout": "IPY_MODEL_970af49c4f1940758130a5e334901a14"
          }
        },
        "0926abb88e2e4381b4b38839e6f652f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ac46097afa5445591d9a3fbad639ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4b1cc17a48461f8e2da6dc9b03c82a",
            "placeholder": "​",
            "style": "IPY_MODEL_4d0cb99e715847d0b05a031736ecd688",
            "value": " 1176/1176 [00:00&lt;00:00, 18224.04 examples/s]"
          }
        },
        "1201dfd96b7d4fcabf5b10b3fa769759": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efc1ff78f5f546638b298419a4151695",
            "placeholder": "​",
            "style": "IPY_MODEL_0926abb88e2e4381b4b38839e6f652f5",
            "value": "Truncating train dataset: 100%"
          }
        },
        "1807262027f34e70b1e94883ac402b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a66367d4522a4b1eb6ce7b88394f8e30",
            "placeholder": "​",
            "style": "IPY_MODEL_a0041fc725964c50a3beff504ddeb9ab",
            "value": " 294/294 [00:00&lt;00:00, 2586.64 examples/s]"
          }
        },
        "187ce14699ae454c819ceacc050a955f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffc5523dd084452a20ae6c913d39e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e320bdd7a7c484183ed8b93e6705af6",
            "placeholder": "​",
            "style": "IPY_MODEL_cc5fad4a005e4aa9bcaecccb8217df1e",
            "value": " 1176/1176 [00:00&lt;00:00, 2971.18 examples/s]"
          }
        },
        "220a0b50b8d04e3dac9bbaf89de8a7c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1201dfd96b7d4fcabf5b10b3fa769759",
              "IPY_MODEL_98ae3ea018e24dabb90618b485a2b2cb",
              "IPY_MODEL_85f4e94e397f43a588a527fe41bc015a"
            ],
            "layout": "IPY_MODEL_686120f3641f43298071d6a5e32fe644"
          }
        },
        "23b95f393dd940039de764bd67d7709a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246d63e4a14841c594db092689f2a64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb370caba2b844c486d30748e6caf9e0",
            "placeholder": "​",
            "style": "IPY_MODEL_73e04b4d775848a19e267669303d8266",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "2ae2a398a2a945d18ec5409bda99644a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d03ea85ed9e4262bfa21065ad73eae4",
              "IPY_MODEL_5f8edfd1f93f4001b51bde9e815d66af",
              "IPY_MODEL_f1a26a9520da4b888f8c54322ae29b82"
            ],
            "layout": "IPY_MODEL_ca3c86d9cf48410199515ce86fd68eaf"
          }
        },
        "2bd1218e781f4784be96419b838c45af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c01848e500843e39f326eeb3b0a8cce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d03ea85ed9e4262bfa21065ad73eae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd2597c60874a4f9c920f8420b366f0",
            "placeholder": "​",
            "style": "IPY_MODEL_cd056a6bb14048bdbc7860ce2c06303f",
            "value": "Applying formatting function to train dataset: 100%"
          }
        },
        "2d402147806d413590c8bbbcd89646de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35fcfd38ee344fd0924e883972074784": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d7b45cd5bfa4ad9ba035ac29882fb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80acce4d1d5c4459ab794ba6bbcc32cf",
            "placeholder": "​",
            "style": "IPY_MODEL_fa04a7d89df34dfcbd73d62f4dda799c",
            "value": " 294/294 [00:00&lt;00:00, 15282.07 examples/s]"
          }
        },
        "3fd2597c60874a4f9c920f8420b366f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4375f815d6724658a2cfaa6c608e478f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b4ce75564943ca8b250e54a7ce1834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "442e0cfbeb554fd58e2bf0c2d77829c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4526ca6058b74e11ba4329d843d6a9d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494b39f1fcf9434b88559980e35c006f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a974c0a2ec043219c538076bfffc87a",
            "placeholder": "​",
            "style": "IPY_MODEL_79f22a64d0fa4310b9ee63bbfb66b41a",
            "value": "Tokenizing eval dataset: 100%"
          }
        },
        "4a251249b63e4688ba357a8d412a3b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1267d7977354e5aa3db8a28f511f2b8",
            "placeholder": "​",
            "style": "IPY_MODEL_9e526d55e8f84c66ab040536bc502d2b",
            "value": " 294/294 [00:00&lt;00:00, 7943.86 examples/s]"
          }
        },
        "4adcb8502fce4dd1af5b8a5b648fa263": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b47975658384af1a549c9f1acf1e5a5",
              "IPY_MODEL_56eff52cba7c4e06a6378805f2aa2707",
              "IPY_MODEL_4a251249b63e4688ba357a8d412a3b69"
            ],
            "layout": "IPY_MODEL_55fdfa966042462bb35ee44cfbfb841e"
          }
        },
        "4d0cb99e715847d0b05a031736ecd688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eaab0584d164139b520cd15eaa3793f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187ce14699ae454c819ceacc050a955f",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a08c13a1cfcb4fd295994b99f788d6c3",
            "value": 294
          }
        },
        "551f5413b209400fa23bedb10c4cde9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b1ad5ece2c4be18a2acdb322b93c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc18b5fb2e349afa2489a0adf1b21fc",
            "placeholder": "​",
            "style": "IPY_MODEL_2bd1218e781f4784be96419b838c45af",
            "value": "Truncating eval dataset: 100%"
          }
        },
        "55fdfa966042462bb35ee44cfbfb841e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56eff52cba7c4e06a6378805f2aa2707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea9a076bce947bab5096a028f7b3237",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c034029a3d5e48ebaf7ea25fa7cb96fd",
            "value": 294
          }
        },
        "57e0a819a11b42e0a35b75e072b2dbb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58850dfce0cc4811ba06ab1ea71d306d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9cb618b32044518965997a96a7d930": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c5ac7432fae43f2a17870b83e19a17a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f8edfd1f93f4001b51bde9e815d66af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d98a8f43fc76406897ffbb66f7e136c0",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8f7022d52464b0eb74f965319229cef",
            "value": 1176
          }
        },
        "5fc18b5fb2e349afa2489a0adf1b21fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "634e691c8c8a4fb68c5af7704b9ce0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65597da15fda4b51b7aa859b380c12db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "686120f3641f43298071d6a5e32fe644": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69fe2be7c9814f2ab93168f3d6ac45ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a974c0a2ec043219c538076bfffc87a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d044e9840e449c5b1e718fb06e99515": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4b1cc17a48461f8e2da6dc9b03c82a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73e04b4d775848a19e267669303d8266": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "788d9b117db34ff6b06e1fd6a942c39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba961b0dab1a4ac6864646de1d46d9e6",
            "placeholder": "​",
            "style": "IPY_MODEL_442e0cfbeb554fd58e2bf0c2d77829c0",
            "value": "Converting train dataset to ChatML: 100%"
          }
        },
        "79f22a64d0fa4310b9ee63bbfb66b41a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79f8dacb371b4fd987b8a4330520cc04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac0a7531ecc4823b33549eeff34a131": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d044e9840e449c5b1e718fb06e99515",
            "placeholder": "​",
            "style": "IPY_MODEL_d00e51ae50d144e392bf12aa8b54b303",
            "value": " 294/294 [00:00&lt;00:00, 5948.68 examples/s]"
          }
        },
        "80acce4d1d5c4459ab794ba6bbcc32cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a4b7f740ec41389a6df6317af60615": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841bc431816441a4bc79dde9600f9176": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f4e94e397f43a588a527fe41bc015a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c5ac7432fae43f2a17870b83e19a17a",
            "placeholder": "​",
            "style": "IPY_MODEL_d224187c0f7340279949cddca91d3ecb",
            "value": " 1176/1176 [00:00&lt;00:00, 56603.68 examples/s]"
          }
        },
        "8983e348f59749bd86146b513c5acacf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd9559471a9418d8a134b43c6dc1258": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcd62139ad44297b2a950d97850468f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90fadb3fb3e14b1baaca9e77f571a80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2e6f823b0224c52a44e1e798e5c1985",
              "IPY_MODEL_4eaab0584d164139b520cd15eaa3793f",
              "IPY_MODEL_943be389e1a947ad99ec7d4d76de6db0"
            ],
            "layout": "IPY_MODEL_2c01848e500843e39f326eeb3b0a8cce"
          }
        },
        "916b489caeac4041839f1ee2365360a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0da77700eab4ab0bb06e1fe7b5184e9",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69fe2be7c9814f2ab93168f3d6ac45ae",
            "value": 1176
          }
        },
        "9419d62d8de24bce9697c0f60467ea99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943be389e1a947ad99ec7d4d76de6db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a81481e6777b4ab78442363553b73381",
            "placeholder": "​",
            "style": "IPY_MODEL_ad666ee0fe7e48de9c4e934212bd337c",
            "value": " 294/294 [00:00&lt;00:00, 8462.00 examples/s]"
          }
        },
        "970af49c4f1940758130a5e334901a14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ae3ea018e24dabb90618b485a2b2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58850dfce0cc4811ba06ab1ea71d306d",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_634e691c8c8a4fb68c5af7704b9ce0fe",
            "value": 1176
          }
        },
        "9b47975658384af1a549c9f1acf1e5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23b95f393dd940039de764bd67d7709a",
            "placeholder": "​",
            "style": "IPY_MODEL_c68779b0088e4f1c867ef1be26e7112b",
            "value": "Converting eval dataset to ChatML: 100%"
          }
        },
        "9c6f2000b243485db70d65e993621d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c778736f27e349edb304160691210f4b",
              "IPY_MODEL_076e2be9ac414d6d92b90f203da87c38",
              "IPY_MODEL_0ac46097afa5445591d9a3fbad639ae5"
            ],
            "layout": "IPY_MODEL_4375f815d6724658a2cfaa6c608e478f"
          }
        },
        "9e320bdd7a7c484183ed8b93e6705af6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e526d55e8f84c66ab040536bc502d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ea9a076bce947bab5096a028f7b3237": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0041fc725964c50a3beff504ddeb9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a08c13a1cfcb4fd295994b99f788d6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0da77700eab4ab0bb06e1fe7b5184e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a66367d4522a4b1eb6ce7b88394f8e30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ee09b72b034234b02194ab94471a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a81481e6777b4ab78442363553b73381": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad666ee0fe7e48de9c4e934212bd337c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3fbfe3f3f604b698e7d869437c0d5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_246d63e4a14841c594db092689f2a64c",
              "IPY_MODEL_c6909db59ff04666b867097216de70ba",
              "IPY_MODEL_1ffc5523dd084452a20ae6c913d39e47"
            ],
            "layout": "IPY_MODEL_35fcfd38ee344fd0924e883972074784"
          }
        },
        "b8f7022d52464b0eb74f965319229cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba961b0dab1a4ac6864646de1d46d9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba9f2bb92edf477d94d6cfa1e4950711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb370caba2b844c486d30748e6caf9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c034029a3d5e48ebaf7ea25fa7cb96fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1267d7977354e5aa3db8a28f511f2b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d35975670546ab9998a6389e3ec707": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c68779b0088e4f1c867ef1be26e7112b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6909db59ff04666b867097216de70ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4526ca6058b74e11ba4329d843d6a9d4",
            "max": 1176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba9f2bb92edf477d94d6cfa1e4950711",
            "value": 1176
          }
        },
        "c778736f27e349edb304160691210f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79f8dacb371b4fd987b8a4330520cc04",
            "placeholder": "​",
            "style": "IPY_MODEL_d990e34729bf43eca69270c1fd3dc8cb",
            "value": "Applying chat template to train dataset: 100%"
          }
        },
        "c8ad9a852eec438e972cadc8ea1f0fe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d0620a47874f34b94681c71f35c3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841bc431816441a4bc79dde9600f9176",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9cb618b32044518965997a96a7d930",
            "value": " 1176/1176 [00:00&lt;00:00, 18682.87 examples/s]"
          }
        },
        "ca3c86d9cf48410199515ce86fd68eaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbcc5aca91d46ba9b51859494da828b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_494b39f1fcf9434b88559980e35c006f",
              "IPY_MODEL_f849b4139b5d4ab3a2ae32ae83807160",
              "IPY_MODEL_1807262027f34e70b1e94883ac402b39"
            ],
            "layout": "IPY_MODEL_c3d35975670546ab9998a6389e3ec707"
          }
        },
        "cc5fad4a005e4aa9bcaecccb8217df1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd056a6bb14048bdbc7860ce2c06303f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d00e51ae50d144e392bf12aa8b54b303": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d224187c0f7340279949cddca91d3ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d43bfd1e65ae43af97b7fa6b44cf9023": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d98a8f43fc76406897ffbb66f7e136c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d990e34729bf43eca69270c1fd3dc8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc6578ccbf9740bfb315a07b6482d8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0d2e5d548e04e2ebcf6b5a79083f0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2283973a0de470a88d5bb8b78160982",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65597da15fda4b51b7aa859b380c12db",
            "value": 294
          }
        },
        "e2283973a0de470a88d5bb8b78160982": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2627486d9ba4848830b0687cbe8b3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55b1ad5ece2c4be18a2acdb322b93c01",
              "IPY_MODEL_e502fdc27ded43fcbbc4d3fe3fac8030",
              "IPY_MODEL_3d7b45cd5bfa4ad9ba035ac29882fb8b"
            ],
            "layout": "IPY_MODEL_c8ad9a852eec438e972cadc8ea1f0fe2"
          }
        },
        "e2e6f823b0224c52a44e1e798e5c1985": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cd9559471a9418d8a134b43c6dc1258",
            "placeholder": "​",
            "style": "IPY_MODEL_dc6578ccbf9740bfb315a07b6482d8bf",
            "value": "Applying chat template to eval dataset: 100%"
          }
        },
        "e502fdc27ded43fcbbc4d3fe3fac8030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9419d62d8de24bce9697c0f60467ea99",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43b4ce75564943ca8b250e54a7ce1834",
            "value": 294
          }
        },
        "edf820a16c044fe0a2d2f85b11b64683": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efc1ff78f5f546638b298419a4151695": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a26a9520da4b888f8c54322ae29b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8983e348f59749bd86146b513c5acacf",
            "placeholder": "​",
            "style": "IPY_MODEL_551f5413b209400fa23bedb10c4cde9f",
            "value": " 1176/1176 [00:00&lt;00:00, 11431.75 examples/s]"
          }
        },
        "f849b4139b5d4ab3a2ae32ae83807160": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fcd62139ad44297b2a950d97850468f",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7ee09b72b034234b02194ab94471a2a",
            "value": 294
          }
        },
        "fa04a7d89df34dfcbd73d62f4dda799c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff5faacf70a04f19833d41589b7a78e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83a4b7f740ec41389a6df6317af60615",
            "placeholder": "​",
            "style": "IPY_MODEL_d43bfd1e65ae43af97b7fa6b44cf9023",
            "value": "Applying formatting function to eval dataset: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
